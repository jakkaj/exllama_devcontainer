{"docstore/metadata": {"1c6114a4-afbf-45a4-83f6-cd2d70a83c3a": {"doc_hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "a573b002-0d92-428d-a48d-2cce0a286c36": {"doc_hash": "ecb39b218aebc7786ab4655b3e25d7c7676fdd6aa549cee4db1979af12ee683a", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "0803fb57-4e48-4e17-ab80-ce1ba08f94ac": {"doc_hash": "8590ce61ed59196bc0657dd4a073512a294d5a944305c93e9f2c12c9497d4ed5", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "4a9a202f-f246-4466-ae3a-aba01475afc5": {"doc_hash": "7232fbbbb38a3372236b904d326948c4ccbd9d4848a8e451406ecb88877dfbb7", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "2bca0dad-497b-4a01-9d6f-a90ee16f6a0e": {"doc_hash": "e93ee1dcb9f233be50f4c645ea6caf82429f685a43e9909ab0d72311740a367e", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "04c3ce37-44a5-4a97-8a43-c303ff8f28c2": {"doc_hash": "f020bdb166f401fd9a1be91975095304b9af536f66afe883dc9bdce41e060f39", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "b15b964a-7605-4b6e-a088-f0b21b60908c": {"doc_hash": "18fbb7a8f29a58f623035b88fabd5fcf21c51744394a6831b02ac21758e1f15c", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "c5583c66-ac98-4a56-a9e7-ab57435872a5": {"doc_hash": "8f24939669b7da708449b5fc42804187acc200d8c201f732a3363a63982427e4", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "9c4c5d61-ad46-4022-b9c7-51f0721020bf": {"doc_hash": "6dc3fe15e1f7ef9d7a74916ed18332127968249671be88d0487047cf1acbc5a9", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "af0618cd-8378-476a-9f0b-0fac1971e397": {"doc_hash": "ad377370dbf197d076cc8f057f74cad45583fdc869f2226cf75678dac5ed570f", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "50c65e40-4f4e-4c8b-9ac0-72c47fd724c7": {"doc_hash": "1bdfdb211667e2923a5e4b6649c9e28720507cad7ef6eae8496298c7028b4f16", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "421707ef-f3a5-4b5e-a7e5-238e600906f8": {"doc_hash": "078458cf6ab3bec53be24f358001dcddf8c5261693a5dfe22df74e26fcc45473", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "b01d032b-fe92-4a01-be44-488dac655c47": {"doc_hash": "e851ef16996b4ea7bb36c992715ad5e7f19ae976b166923a07f56e0e7b7892dd", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "761861c5-296f-439a-a225-b986fdd74fc1": {"doc_hash": "06b3b4215c516161f9840fd68dac85c3d905ec97d8e5a1e485e483d5d39cca3d", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "46e6d9bb-340f-4ee3-8d21-edbaa5348e1e": {"doc_hash": "8b28e24e81893497d9f38662c3f1ced9261a4fa4325f1ef096ffa23e0b547957", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "dc2e61d6-cb40-4f23-b2f6-dc8d41aef858": {"doc_hash": "9f25e45c596e11cce807e6fb171ec04f945bd109a9a6b9b2d9c93ce4725c88c6", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "c7c3abef-9148-4b08-badd-b31eb06ce1fe": {"doc_hash": "7971cac1db8b883ca4fcd033e2e6d660b6772766158d773d726fac61a1a6a195", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "65dd82f8-de16-480f-ab50-9b75a01a6314": {"doc_hash": "96d9bf94cd035e91b99417a8e8b5b53a1c72b5a6a01b4c2266c0826f0543943a", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "e4bf0c50-cd16-4a42-80d1-508a739cd3b2": {"doc_hash": "f4c3d7ce6360366973eeb399a813b0a58dba69206135ff9b2b18cf08cbf22bb0", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "6107db41-ff01-42eb-a3f6-f486a16a4f2c": {"doc_hash": "80db8fede1bde1e6a1d1eb62524138761c4e20ccb1d11f37533032bd4fd93555", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "57e65d6e-31c8-4ff2-b498-157573256f9f": {"doc_hash": "84da4a991f1e0541cb3210d76e8237dba85566a878e9d247774d2a448b621d89", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "ef4c8c43-2e1d-45a8-b380-b47b75395f42": {"doc_hash": "6bcd0ab1fe52c8c6244bf11f081fed0a09d6650fbc1eb36e879f222db67e3ff3", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "640a9b0b-a083-4364-90e2-2f5543b56315": {"doc_hash": "abf15c98c5c8653cd5f102fbba312087d82b6416cab8e8ceaee93b95aa0c9730", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "34b32204-2ef5-4f6b-9793-44bb1dd96711": {"doc_hash": "99de19ad3d4f29861063de6c0669b54c3ab1d4a997fb727804543587b2daccda", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "872c14ae-0416-476d-97fa-38a37827f66b": {"doc_hash": "7e3069971080c23face66dae5c80bbdba566a2a0395f656787ad255c3112656b", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "37bed96a-f95a-47dd-a1d9-a2ed0e58b845": {"doc_hash": "41c2c7c4d4c22400d0076daf9b7c0296eade9e468b89d53ddfb0b20fde9309de", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "de020396-b2cb-41d4-ba1f-4108e5a557b3": {"doc_hash": "d746e5189900fbc62a1c94b6844956fae00c520294649e7f3b90242d8d7788af", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "caea057f-c4ca-4b03-acde-d84deb48413f": {"doc_hash": "44b8c215e4ad0bc91fe71878bb4297781a2e0aee8e0fc06517a09d425ca84f31", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "e48db89d-394f-4da1-9e88-5c1eab40747c": {"doc_hash": "0a1981d5799a0ed5ff0dab2360daea6d50d3d0c0c5fff2802fe933fab06c5846", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "5b414108-ce4d-40f6-a572-2975e369348c": {"doc_hash": "0df0570fa1840b9ad31bafbcfce4f175e61e71b3d5dff1b912c0bcae415ff527", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "27232c73-14cd-4802-ae69-4433455e8b32": {"doc_hash": "a1c1f49808a0be3bfa997e3bf2a096a43a386cec5f0b8a2b663861db9936a7ef", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "0c9c177e-0989-4c36-b6e1-ed23a7276819": {"doc_hash": "9e64a2d89f11fd757e3494f6233404a56eeb106ab23c4f29ae32b547092122d4", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "a02df946-1b52-436a-87ea-58340691d278": {"doc_hash": "dfe6fd956e0d1d3d71b507e160efdb4531b29c7c08f6b3e430cee6ba66cc5f73", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "4f102465-74bd-4e58-a28a-46526f522bea": {"doc_hash": "0db1d0bab095d189a627907d944b76104a150819021bd519d7cfde7ce0b86aa8", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "39cbc489-014e-406a-82a9-b31b11f722ea": {"doc_hash": "a267afda5441b33f6c6b5317df2cd8fa2f612920f7e057105bb8235daa71a084", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "98242268-27d0-4a33-809e-b4dc476681ab": {"doc_hash": "50f469ab374e5e9b34b613c15a21846330702a365684d1dcc878cc9c66275d91", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "c9ba681f-d8b7-4f8b-be47-d369fa644e67": {"doc_hash": "321b40c326ae9e09bf67ab5de1dccaf9df42b524c89083aae8c3e4275b577935", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "bb426f0a-bd9f-46cd-bfc2-4717564d1170": {"doc_hash": "7b52a49ecc42cf827ecdc457828bd55a08d924b7dccd03777fb80a0ed7a83c53", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "b4c4cd88-7cea-4fd2-a854-f1187d6f3c6a": {"doc_hash": "2c3a63bed8ab63cb57563a1fb41ac1b4f2100c2756d3ce00354d395c758a8c2e", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "cadb9d45-9843-4531-a2cd-2567a287210a": {"doc_hash": "25fd99d180137db3f0ef1a1bc9e60450de973c5b650e4536d384fcc07fa496fe", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "232c6788-a8b7-437d-84ad-fa0273d1901c": {"doc_hash": "6d0b4324e82bf1138da46acfc35eed48f29abe7d2dae2d7397f964d8b91bd275", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "f848e9ac-4c21-4afa-8fd7-ca37e3c61c3e": {"doc_hash": "9ec1175c62d69ba798c3b910a2956c87295da5e69ead1bb49a5e168d6b74e41f", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "277bc300-2c2e-4b92-9eea-fa96e785ae98": {"doc_hash": "b398594fede85dedd2f483f7a5b205f5409432c49a4648f5d5599e529f36e1ae", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "f60c039b-347d-41d8-8334-99c8389902a7": {"doc_hash": "e189f89cf771de51ac75762cff571e7c9a868399ae18391e4d3877e09aede3b0", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "a7a1ac86-b86e-4f9a-a2fc-2ecc2d885007": {"doc_hash": "3627640734cc812d31fc79c9213fe95f485da8e3dc0a94bb7e396a7ff6411a3c", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "d54f85af-d0fc-4636-a72f-ad4613bcf6e6": {"doc_hash": "4190acdda5d9c109ac9f29d4ce0251da21f58a8bdebaf1450309d7655518fa42", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "be8c7dcb-1b14-4850-a82b-5f9e9de74c4d": {"doc_hash": "2c27b0b303dff65e222a4e7913331bbf62366df1d9f9659c1ce7c1dbbf47b919", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "b15d6d5e-0064-4818-9503-39754929969d": {"doc_hash": "19abd5c91ebc45d95080efbdbe59bb6de18c731bf62438f026b8a80240c1145d", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "dafcacc4-d8f9-4bff-bfcb-f6f74de7dc7c": {"doc_hash": "964c46907a3a329fdde56b32f4b5a0a22973e4b9190ca5b87db6bf5a1eb20462", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "946c1d16-56bb-4d8c-ba0b-1555df851642": {"doc_hash": "8e1566a84f37c1b15434423a5df8882e61970e685bda3847a33377573bd597c6", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "26ee11bc-2381-4340-9a52-20b53563588d": {"doc_hash": "b69adc0ac748e4f60284909ef5301879516ef5422eb7682e743880ff7d977174", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "b494e4b9-ee9b-4794-a807-98e144fbea89": {"doc_hash": "588f29a92f872a40ca4a93f430d670286164379c63785530e93ec0408436df4c", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "ff18a060-413a-4c56-a41a-faad54d5eecf": {"doc_hash": "fee97cff4fcc7f1adbcc9775aa3f012689e5bddb91d25567587b8c4e5b5256a2", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "28d4b0e1-a30c-44d2-b8c7-9ecd5f79254b": {"doc_hash": "99519410ebaa8a1c5eca4775eed0f689baaae49b75babc2ffc0e802db52c2f05", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "77424ee9-defc-4b65-9027-d7d6536a3e51": {"doc_hash": "9b606cd30099392a5374bacf6e870ae7fbc8306c201f3997885a9364a03b1f9b", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "8bed7003-1d02-43f9-8e87-77c5d10231c6": {"doc_hash": "d7a704882b5df7e54b360b358ad58b6210d66e783bf1db1724e1f433326fa1c9", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "beb32c2b-70f0-4d38-8ce5-6b52d9f59e21": {"doc_hash": "5e2a2ea7fd43f2c58ae01994f591e5b72af077edd7b7acf3e37fa49e38b6ec3f", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "1a7b5a50-c21f-4b70-8cfc-e3a5d7b28483": {"doc_hash": "59d28cccbe13a90de46fa51984c0805e19c59e98020e1d13bb2eca9aff36171f", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "5f0613c8-76e7-40f9-83a3-e676b05ba8be": {"doc_hash": "42d7616ac3c9d2556fe401b2d26b83617e3b225f4c33e926ecfb191fc5cd6b0e", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "58bb2cae-413d-4131-988e-33201961e8dd": {"doc_hash": "e5b3d8b9c8429a2de7b783ec8270f9184e267616e88e4775dfe95058fc13f58f", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "cbd8b3af-0e1a-455c-b6d5-703c93ca6812": {"doc_hash": "5b3091ef6e6c6465346d77d07efd070a08d06fa9dafbcdd2fedc9968ecf528d4", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "25bf4a5e-0bc6-4592-824f-5321d0f7d5b1": {"doc_hash": "738dc3fea6325fd16fae181514bb259f62a7b6c3aa067eb3e322dd27b7b9295b", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "b9413688-685f-4087-a3e1-4d4d48ec9b74": {"doc_hash": "27f9240336b78c5d2e16bfceaa3e3dcfb5cc09a130459ad83f9f5fb5909d39ec", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "d38b97a2-714a-4cd0-a997-d2a5b94622fa": {"doc_hash": "5d86bb6307e0c5352d38491f5ab10069182e72057b1c424dcffcf0fdf0e3ee98", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "30d68ce3-7d65-4e47-852a-fee9953253eb": {"doc_hash": "5b3c65f33719420cc9459d2040f16fda11a04c4a2ad83c2f32951a785e4a6419", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "1623a297-9d4f-42a8-9a55-693bc937f6a5": {"doc_hash": "0bd2d587573ebe1d1e32b9c4bdfee943ec9bcd00fd278c8341fcde7d23c2a7bf", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "719fdc44-7038-4b3f-9522-2c8183d46807": {"doc_hash": "dde661b775a21f74a1afa0c10942d57233ce1415e0fb35845c87cbb42b66fe2b", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "358581b8-6620-45e7-8042-8e52fe7a11a9": {"doc_hash": "d85c99f120be5e4bc762415c90515d897eefb77087962aae3806013ea211251e", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "a0e5b11e-e971-4d29-a7f0-f33cff4efa25": {"doc_hash": "ecfe956906cccb7db92dd0b6508bc23df5d2f2a9e3aa52b2c8d4d07732e18392", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "637b6b93-f153-4944-a725-b29bbf9a662f": {"doc_hash": "78f9b140f997e9add640206a054d782080055d8a96ddfce301b7540000dc1edb", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "c7416539-0cbd-42bc-9d7c-2fb129c412d9": {"doc_hash": "39828217e97230cb73d40116d5084ecb45e1c13831eb219e02ee8afead1c0411", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "1d347c22-2461-4a69-8245-aa2be7bf901a": {"doc_hash": "7269c8b5e7de74ee14f0e54db1475ee057085c244f30038b5ad24e4605d37d28", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "61536b64-d213-4ede-90b6-128097fb380b": {"doc_hash": "b548ac333243714a87d4609d837c529d8df0527ac615c5753f858ad7109d8aa5", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "7d656dc9-e1cd-4dba-abdb-9bbd1862b21c": {"doc_hash": "c9fd8d6158b1e2d17ad2cc391eb756d5c7aee6025dfdb3e3181a7468cd52e16e", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "3d3e0ddf-acde-4444-aa24-511ce8050c22": {"doc_hash": "b9bfeeae72305f7d347b250f2d3fd3eb12ba9b1f001c4a1c2f13415f2bd3603a", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "8fd5a7d8-ea80-4b04-99fb-f39a48c7c9b3": {"doc_hash": "9fee71c674992af7f9825fb3e204ec09e3a27d3894ad92180102f1eee9962815", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "c7f873bf-ddfb-4c71-bc1f-e7cf3026c452": {"doc_hash": "8a25bd54e04a39f86ee25b1d4370c65204c09266d400f9c29fa22af618c20f90", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "0c71ba38-076d-4d2c-8a40-b50c3f85b5c1": {"doc_hash": "26295d881f81243e88e20586a4b76819eb036de60d3755c98c08a7ddf1d4be59", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "cef44be1-ee48-4ff0-8872-08d74530e3c1": {"doc_hash": "a0e7c05f36bb7bb53182228d50da0c145ae38a6e0ecf2c78d83a03467545a97b", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "5e521b21-0ead-494e-8690-68b4cc7096b9": {"doc_hash": "59259d1f377ee1970baa30a18899d95743f3172b4de2665b78b365cd949befb0", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "9666c334-2245-4f6a-8a07-6c02c944702f": {"doc_hash": "23fe0526d86ffb37837c0087f8724b40f455cf774d5ff3dd05f88d6bfe693727", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "3ea99bc3-8f16-41ae-9ba1-18c87cbd849b": {"doc_hash": "158356b5ca7c5947eb6bcfbc0f953dd63372fe8dc279c6cb5da7d43faf6124d5", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "cf094076-a52c-4980-83bb-f2d5a25b643f": {"doc_hash": "cd491caa5c65a5785e9f1499ae46afc114cc400297dbce4ab4d829b5b2b53fe3", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "f925dced-f69c-449f-9c37-629d941e110b": {"doc_hash": "a1d5e7d715eb8c17afdedb745c79d3063e00bf7ce107582c709dce51d093c218", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "d0adb495-064a-4f39-86fa-1e904b74dd2e": {"doc_hash": "5c28b2161a65271d41d382a48ba14b16e4e51146523d1c5435ae4d5cc35cf000", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "c1188a9c-8ab9-4c44-81cc-916ccd3342d8": {"doc_hash": "a82161531733c56f40e33a58238084fab0a9932de05a32a3fcd5e28b9b48ec21", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "1d792d69-87d6-424a-a1e4-df9b63ec7fe1": {"doc_hash": "6d8b0961bdf928b53d020a5a2a3e41692e8cb91dac5414c235f69ce1048593f9", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "51ef6659-d4b2-4344-ba0f-5fd65487a9b4": {"doc_hash": "3d3715f3577528a2c5a7cb57e9e98b1f6accfd7dd2fec9cbe76a8fa1d29d6b18", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "9abbd0e8-de67-45fa-a73c-f7e31dbe2ef9": {"doc_hash": "d050e091f08993235db786f71052994c72c8418c632adc8c483f3b6bed1f1c2c", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "ff3bb99f-c4f1-4a41-b5d1-30dc85973069": {"doc_hash": "09b522b6bde4537a4f23b4549d69cbe20acf293b65a5bcc88a90564665dd89a5", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "7e2bcfc5-a822-45f2-905a-26ddcf11f482": {"doc_hash": "e1b9887b2fb75cad8c3da7a2abde6550cb10d291fed8fa65f097d4fafbe3daa3", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "9058e6b6-d1e1-46d5-9105-02ce96404880": {"doc_hash": "5bc1d4386451113e4b96b323fd1b7e78eaddafcfed6dd39bf7a319d47b8c9631", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "1693798f-391b-4324-a5bb-8869f6e40ec9": {"doc_hash": "3cca8815b82ae6bec7d5fe34472aa773f3052d07a14adf63c0d84f7f054009b0", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "8d618149-8d33-496e-9607-dba2df1cae92": {"doc_hash": "52c2b428f6e54adc38eccb5f2236e459c4f4dcf835b2bb4451841c30442ead2b", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "8b904a93-070e-4a67-b9e1-9c8890ec0ccc": {"doc_hash": "e3853e94a816560d6dfc43a2427bb8b9e5af913514c9510ab882da043fe9fbf0", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "3cbac886-bcaa-41a2-8e3a-88ecdf72785a": {"doc_hash": "fec7cfa610d57909305e101ea354cb4085f110f0fb56ed456c9fdcee16bdcb13", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "588afb2f-817f-4a6a-b61c-abb4a30e7712": {"doc_hash": "35328a0b741f83e2029738c8ea8a0102a95ed1def557adb58f105dcedea9951d", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "09eeb4db-79c8-40b0-b222-85a06d9b8f07": {"doc_hash": "70af66c509fdf843628412519017fa0862cd63957006be42ed1009b578f2c071", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "4dc467e5-ed33-4e3a-bfb9-8376a49133b7": {"doc_hash": "dcb41f90159fb16b5c899854d73c1307a060db46bc254ba2927ea93b9f0cfc25", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "68a17f27-2e13-4c99-93c0-16bb371d914e": {"doc_hash": "b87a6046d2bd7b5177e33d44475501f34fd9f75d6b5d45c119d05fcacda9380e", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "366863b6-673d-4f6d-9dec-abff2d07ed26": {"doc_hash": "e9103660e8e1667ffdac2aaa9e1d5245cc80a896d37b736d09c77cfc0d7a7bef", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "62da66d8-fa5a-4e6c-811f-82f66cc9adb1": {"doc_hash": "5d9025ef55cc10e0638ccc6017ff0da1662309592e0f0ce65058a7098a87be4a", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "d421e349-dd36-413e-b132-703b86887d7b": {"doc_hash": "a81aba0e5c3079c0a3132cda85e1e5e2433519b734cefe02c11b6004fdb733ed", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "00b870a3-a4d2-4d20-a770-ff54321633e6": {"doc_hash": "474dc0aec1a069022d6a63bdfc8117054721bcd265db9e9fc55e114608437522", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "6537c040-bd70-464d-8c4f-1138f1000fa9": {"doc_hash": "a49275ba8dfc3d7d48af153dcdb732d7d09c9e90c50ccabdd8ead025bf580c0c", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "7fa02926-68cd-40fe-9152-781d08ab1183": {"doc_hash": "e0c9035255c5d9c86842790b63175863d8dac33bd112eec6e656069bd0f48d96", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "405158b9-9b8d-4624-9de0-5e1c55bf07d3": {"doc_hash": "6c97927c6b6fd8604f8ad6b21008474046c0dd8b146febe58943ae96463091fb", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "79736362-4040-4c34-8c9a-a73211271402": {"doc_hash": "f36d945f1b9a9502b68c115974c5bfaecea9abae0c9dff6f3da5907124caf473", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "87ec85ba-f93f-4b3e-a58b-19bded79bf60": {"doc_hash": "19b5d2f26b1565725d5f9d0dfb13e0fa1abce0b27bfee83da28a69f3de13d1a9", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "958656c5-73d4-4f7e-890c-74aeb1e141f8": {"doc_hash": "4a225b9fbdaf4e850e6f1ffe03044a56399b7c17336c012b22e2051ac4962f0f", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "045b94f1-1612-4002-9276-c6236c6f87d7": {"doc_hash": "c6e6b3191a5e174e6d56bb5018c8775efed5c0c7ca5363494354b38655af3ee7", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "8db43dbe-6391-4294-a2ed-c6cfe42236b1": {"doc_hash": "c2c0b567353976c3ba97c21c9237978e4852b4408d27b197f57ecc9a423b83c9", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "92d5c780-a559-437e-aefe-2fb3702c45d1": {"doc_hash": "28a5c308d3fa2011bb5e8039532c21ccd5ee9f3ac0824eb2677987f57c6d940d", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "556dbab9-5a90-48c0-926d-20eeffb941f3": {"doc_hash": "b4b2668affc8d6f4d077da9c65611fb970aa9917567f7e45bcc45dbf633c731b", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "42ee3e86-a475-4eaf-b526-55f7920b4013": {"doc_hash": "17fc789e6fcb50b92e0887ff207c15280a1531f759789268f4a89f05d11236c8", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "a7b678be-36d6-4517-be75-b47a6ea5615e": {"doc_hash": "79bba5e82f34c2a90cceee35c0c08e0b82882ea05bec12e6c94aea4af66a4631", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "fb4c9479-22d2-47eb-b6a3-6bd0d76d7702": {"doc_hash": "8bdc263cf7e2bfb0a9302373a521dd100347ab6d87f13a4c1de2c4bcbe3cb610", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "383d7d6b-7777-4601-8256-c0eacbc2c8ad": {"doc_hash": "7fc3bfbc4be2dcb02368ebb53687b07a3c02cc34f0029e9e0c286ca369b439bb", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "86f88c87-3188-492d-945d-5d1d161276e4": {"doc_hash": "22eedbc49257f6fd966f56c0c8a4c70b3666d832aea2280fe0b53d69092f6fc1", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "076a55c4-0d46-4d69-bab7-1b8e89a02590": {"doc_hash": "e831f6a99e62036d5207c372b5f5b5d343be8f05d70416f4b638076f13e07388", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "8ce5d6e6-002d-4d5f-8f05-55b993161338": {"doc_hash": "7f87e676e537030dd93c8ef04403374cc81b66bb3966a14e8b66d27a905493ed", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "c7103c14-4077-4a6a-8434-3d1972431f37": {"doc_hash": "b24416d7a5dbc2e590ce2141ac7c409403f8e5ed2ba8876b03bb7b391870e20e", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}, "5db410a5-82e3-4d1f-ba5c-674f825be945": {"doc_hash": "4be946aa293da6464502be76ba4f55cbfdd10007803d741d09ca7e271241c2ad", "ref_doc_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a"}}, "docstore/data": {"a573b002-0d92-428d-a48d-2cce0a286c36": {"__data__": {"id_": "a573b002-0d92-428d-a48d-2cce0a286c36", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "3": {"node_id": "0803fb57-4e48-4e17-ab80-ce1ba08f94ac", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8590ce61ed59196bc0657dd4a073512a294d5a944305c93e9f2c12c9497d4ed5"}}, "hash": "ecb39b218aebc7786ab4655b3e25d7c7676fdd6aa549cee4db1979af12ee683a", "text": "LightGlue: Local Feature Matching at Light Speed\n\nPhilipp Lindenberger1 1 ETH Zurich\n\nPaul-Edouard Sarlin1 Marc Pollefeys1,2 2 Microsoft Mixed Reality & AI Lab\n\n65\n\nSuperGlueSGMNetLoFTRMatchFormerL=3L=5L=7L=9fixed-depthadaptiveoptimizedLightGlue\n\n66\n\n10\n\n0\n\n30\n\n50Image Pairs Per Second\n\n64\n\n67Relative Pose Accuracy [%]\n\n40\n\n20\n\nAbstract\n\n3 2 0 2\n\nWe introduce LightGlue, a deep neural network that learns to match local features across images. We revisit multiple design decisions of SuperGlue, the state of the art in sparse matching, and derive simple but effective improve- ments. Cumulatively, they make LightGlue more efficient \u2013 in terms of both memory and computation, more accurate, and much easier to train. One key property is that LightGlue is adaptive to", "start_char_idx": 0, "end_char_idx": 768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0803fb57-4e48-4e17-ab80-ce1ba08f94ac": {"__data__": {"id_": "0803fb57-4e48-4e17-ab80-ce1ba08f94ac", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "a573b002-0d92-428d-a48d-2cce0a286c36", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "ecb39b218aebc7786ab4655b3e25d7c7676fdd6aa549cee4db1979af12ee683a"}, "3": {"node_id": "4a9a202f-f246-4466-ae3a-aba01475afc5", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7232fbbbb38a3372236b904d326948c4ccbd9d4848a8e451406ecb88877dfbb7"}}, "hash": "8590ce61ed59196bc0657dd4a073512a294d5a944305c93e9f2c12c9497d4ed5", "text": "and much easier to train. One key property is that LightGlue is adaptive to the difficulty of the problem: the inference is much faster on image pairs that are intuitively easy to match, for example because of a larger visual overlap or limited appearance change. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like 3D reconstruction. The code and trained models are publicly available at github.com/cvg/LightGlue.\n\nn u J\n\n3 2\n\n]\n\nV C . s c [\n\nFigure 1. LightGlue matches sparse features faster and better than existing approaches like SuperGlue. Its adaptive stopping mechanism gives a fine-grained control over the speed vs. accuracy trade-off. Our final, optimized model \u22c6 delivers an accuracy closer to the dense matcher LoFTR at an 8\u00d7 higher speed, here in typical outdoor conditions.\n\n1 v 3 4 6 3 1 . 6 0 3 2 : v i", "start_char_idx": 707, "end_char_idx": 1575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4a9a202f-f246-4466-ae3a-aba01475afc5": {"__data__": {"id_": "4a9a202f-f246-4466-ae3a-aba01475afc5", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "0803fb57-4e48-4e17-ab80-ce1ba08f94ac", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8590ce61ed59196bc0657dd4a073512a294d5a944305c93e9f2c12c9497d4ed5"}, "3": {"node_id": "2bca0dad-497b-4a01-9d6f-a90ee16f6a0e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e93ee1dcb9f233be50f4c645ea6caf82429f685a43e9909ab0d72311740a367e"}}, "hash": "7232fbbbb38a3372236b904d326948c4ccbd9d4848a8e451406ecb88877dfbb7", "text": "conditions.\n\n1 v 3 4 6 3 1 . 6 0 3 2 : v i X r a\n\n1. Introduction\n\nFinding correspondences between two images is a funda- mental building block of many computer vision applications like camera tracking and 3D mapping. The most common approach to image matching relies on sparse interest points that are matched using high-dimensional representations en- coding their local visual appearance. Reliably describing each point is challenging in conditions that exhibit symme- tries, weak texture, or appearance changes due to varying viewpoint and lighting. To reject outliers that arise from occlusion and missing points, such representations should also be discriminative. This yields two conflicting objectives, robustness and uniqueness, that are hard to satisfy.\n\npensive, while the efficiency of image matching is critical for tasks that require a low latency, like tracking, or a high processing volume, like large-scale mapping. Additionally, SuperGlue, as with other Transformer-based models,", "start_char_idx": 1610, "end_char_idx": 2607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2bca0dad-497b-4a01-9d6f-a90ee16f6a0e": {"__data__": {"id_": "2bca0dad-497b-4a01-9d6f-a90ee16f6a0e", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "4a9a202f-f246-4466-ae3a-aba01475afc5", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7232fbbbb38a3372236b904d326948c4ccbd9d4848a8e451406ecb88877dfbb7"}, "3": {"node_id": "04c3ce37-44a5-4a97-8a43-c303ff8f28c2", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "f020bdb166f401fd9a1be91975095304b9af536f66afe883dc9bdce41e060f39"}}, "hash": "e93ee1dcb9f233be50f4c645ea6caf82429f685a43e9909ab0d72311740a367e", "text": "mapping. Additionally, SuperGlue, as with other Transformer-based models, is noto- riously hard to train, requiring computing resources that are inaccessible to many practitioners. Follow-up works [8, 65] have thus failed to reach the performance of the original Su- perGlue model. Yet, since its initial publication, Transform- ers have been extensively studied, improved, and applied to numerous language [17, 51, 13] and vision [18, 6, 29] tasks. In this paper, we draw on these insights to design Light- Glue, a deep network that is more accurate, more efficient, and easier to train than SuperGlue. We revisit its design decisions and combine numerous simple, yet effective, ar- chitecture modifications. We distill a recipe to train high- performance deep matchers with limited resources, reach- ing state-of-the-art accuracy within just a few GPU-days. As shown in Figure 1, LightGlue is Pareto-optimal on the efficiency-accuracy", "start_char_idx": 2570, "end_char_idx": 3506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "04c3ce37-44a5-4a97-8a43-c303ff8f28c2": {"__data__": {"id_": "04c3ce37-44a5-4a97-8a43-c303ff8f28c2", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "2bca0dad-497b-4a01-9d6f-a90ee16f6a0e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e93ee1dcb9f233be50f4c645ea6caf82429f685a43e9909ab0d72311740a367e"}, "3": {"node_id": "b15b964a-7605-4b6e-a088-f0b21b60908c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "18fbb7a8f29a58f623035b88fabd5fcf21c51744394a6831b02ac21758e1f15c"}}, "hash": "f020bdb166f401fd9a1be91975095304b9af536f66afe883dc9bdce41e060f39", "text": "in Figure 1, LightGlue is Pareto-optimal on the efficiency-accuracy trade-off when compared to existing sparse and dense matchers.\n\nTo address these limitations, SuperGlue [56] introduced a new paradigm \u2013 a deep network that considers both images at the same time to jointly match sparse points and reject outliers. It leverages the powerful Transformer model [74] to learn to match challenging image pairs from large datasets. This yields robust image matching in both indoor and out- door environments. SuperGlue is highly effective for visual localization in challenging conditions [59, 55, 58, 57] and generalizes well to other tasks like aerial matching [83], ob- ject pose estimation [69], and even fish re-identification [47]. These improvements are however computationally ex-\n\nUnlike previous approaches, LightGlue is adaptive to the difficulty of each image pair, which varies based on the\n\n1\n\nEasyRuntime:", "start_char_idx": 3514, "end_char_idx": 4430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b15b964a-7605-4b6e-a088-f0b21b60908c": {"__data__": {"id_": "b15b964a-7605-4b6e-a088-f0b21b60908c", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "04c3ce37-44a5-4a97-8a43-c303ff8f28c2", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "f020bdb166f401fd9a1be91975095304b9af536f66afe883dc9bdce41e060f39"}, "3": {"node_id": "c5583c66-ac98-4a56-a9e7-ab57435872a5", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8f24939669b7da708449b5fc42804187acc200d8c201f732a3363a63982427e4"}}, "hash": "18fbb7a8f29a58f623035b88fabd5fcf21c51744394a6831b02ac21758e1f15c", "text": "of each image pair, which varies based on the\n\n1\n\nEasyRuntime: 16.9msStop after 3 layers\n\npoints and imperfect descriptors, some correspondences are incorrect. Those are filtered out by heuristics, like Lowe\u2019s ratio test [41] or the mutual check, inlier classifiers [44, 82], and by robustly fitting geometric models [22, 7]. This pro- cess requires extensive domain expertise and tuning and is prone to failure when conditions are too challenging. These limitations are largely solved by deep matchers.\n\nDifficultRuntime: 32.3msStop after 8 layers\n\nDeep matchers are deep networks trained to jointly match local features and reject outliers given an input image pair. The first of its kind, SuperGlue [56] combines the expres- sive representations of Transformers [74] with optimal trans- port [48] to solve a partial assignment problem. It learns powerful priors about scene geometry and camera motion and", "start_char_idx": 4436, "end_char_idx": 5343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c5583c66-ac98-4a56-a9e7-ab57435872a5": {"__data__": {"id_": "c5583c66-ac98-4a56-a9e7-ab57435872a5", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "b15b964a-7605-4b6e-a088-f0b21b60908c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "18fbb7a8f29a58f623035b88fabd5fcf21c51744394a6831b02ac21758e1f15c"}, "3": {"node_id": "9c4c5d61-ad46-4022-b9c7-51f0721020bf", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6dc3fe15e1f7ef9d7a74916ed18332127968249671be88d0487047cf1acbc5a9"}}, "hash": "8f24939669b7da708449b5fc42804187acc200d8c201f732a3363a63982427e4", "text": "a partial assignment problem. It learns powerful priors about scene geometry and camera motion and is thus robust to extreme changes and generalizes well across data domains. Inheriting the limitations of early Trans- formers, SuperGlue is hard to train and its complexity grows quadratically with the number of keypoints.\n\nFigure 2. Depth adaptivity. LigthGlue is faster at matching easy image pairs (top) than difficult ones (bottom) because it can stop at earlier layers when its predictions are confident.\n\nSubsequent works make it more efficient by reducing the size of the attention mechanism. They restrict it to a small set of seed matches [8] or within clusters of similar keypoints [65]. This largely reduces the run time for large numbers of keypoints but yields no gains for smaller, stan- dard input sizes. This also impairs the robustness in the most challenging conditions, failing to reach the performance of the original SuperGlue model. LightGlue instead brings", "start_char_idx": 5314, "end_char_idx": 6293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9c4c5d61-ad46-4022-b9c7-51f0721020bf": {"__data__": {"id_": "9c4c5d61-ad46-4022-b9c7-51f0721020bf", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "c5583c66-ac98-4a56-a9e7-ab57435872a5", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8f24939669b7da708449b5fc42804187acc200d8c201f732a3363a63982427e4"}, "3": {"node_id": "af0618cd-8378-476a-9f0b-0fac1971e397", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "ad377370dbf197d076cc8f057f74cad45583fdc869f2226cf75678dac5ed570f"}}, "hash": "6dc3fe15e1f7ef9d7a74916ed18332127968249671be88d0487047cf1acbc5a9", "text": "failing to reach the performance of the original SuperGlue model. LightGlue instead brings large improvements for typical operating conditions, like in SLAM, without compromising on performance for any level of difficulty. This is achieved by dynamically adapting the network size instead of reducing its overall capacity.\n\namount of visual overlap, appearance changes, or discrimi- native information. Figure 2 shows that the inference is thus much faster on pairs that are intuitively easy to match than on challenging ones, a behavior that is reminiscent of how humans process visual information. This is achieved by 1) predicting a set of correspondences after each computational blocks, and 2) enabling the model to introspect them and predict whether further computation is required. LigthGlue also discards at an early stage points that are not matchable, thus focusing its attention on the covisible area.\n\nOur experiments show that LightGlue is a plug-and-play replacement to SuperGlue: it predicts strong matches", "start_char_idx": 6300, "end_char_idx": 7322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "af0618cd-8378-476a-9f0b-0fac1971e397": {"__data__": {"id_": "af0618cd-8378-476a-9f0b-0fac1971e397", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "9c4c5d61-ad46-4022-b9c7-51f0721020bf", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6dc3fe15e1f7ef9d7a74916ed18332127968249671be88d0487047cf1acbc5a9"}, "3": {"node_id": "50c65e40-4f4e-4c8b-9ac0-72c47fd724c7", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "1bdfdb211667e2923a5e4b6649c9e28720507cad7ef6eae8496298c7028b4f16"}}, "hash": "ad377370dbf197d076cc8f057f74cad45583fdc869f2226cf75678dac5ed570f", "text": "is a plug-and-play replacement to SuperGlue: it predicts strong matches from two sets of local features, at a fraction of the run time. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like SLAM [45, 5] or re- constructing larger scenes from crowd-sourced data [25, 60, 39, 57]. The LightGlue model and its training code will be released publicly with a permissive license.\n\nConversely, dense matchers like LoFTR [68] and follow- ups [9, 78] match points distributed on dense grids rather than sparse locations. This boosts the robustness to impres- sive levels but is generally much slower because it processes many more elements. This limits the resolution of the input images and, in turn, the spatial accuracy of the correspon- dences. While LightGlue operates on sparse inputs, we show that fair tuning and evaluation makes it competitive with dense matchers, for a fraction", "start_char_idx": 7339, "end_char_idx": 8264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "50c65e40-4f4e-4c8b-9ac0-72c47fd724c7": {"__data__": {"id_": "50c65e40-4f4e-4c8b-9ac0-72c47fd724c7", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "af0618cd-8378-476a-9f0b-0fac1971e397", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "ad377370dbf197d076cc8f057f74cad45583fdc869f2226cf75678dac5ed570f"}, "3": {"node_id": "421707ef-f3a5-4b5e-a7e5-238e600906f8", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "078458cf6ab3bec53be24f358001dcddf8c5261693a5dfe22df74e26fcc45473"}}, "hash": "1bdfdb211667e2923a5e4b6649c9e28720507cad7ef6eae8496298c7028b4f16", "text": "that fair tuning and evaluation makes it competitive with dense matchers, for a fraction of the run time.\n\n2. Related work\n\nMaking Transformers efficient has received significant attention following their success in language processing. As the memory footprint of attention is a major limitation to handling long sequences, many works reduce it using linear formulations [79, 32, 33] or bottleneck latent tokens [35, 30]. This enables long-range context but can impair the perfor- mance for small input sizes. Selective checkpointing [49] reduces the memory footprint of attention and optimizing the memory access also drastically speeds it up [14].\n\nMatching images that depict the same scene or object typi- cally relies on local features, which are sparse keypoints each associated with a descriptor of its local appearance. While classical algorithms rely on hand-crafted criteria and gradient statistics [41, 23, 4, 53], much of the recent research has fo- cused on designing Convolutional", "start_char_idx": 8252, "end_char_idx": 9246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "421707ef-f3a5-4b5e-a7e5-238e600906f8": {"__data__": {"id_": "421707ef-f3a5-4b5e-a7e5-238e600906f8", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "50c65e40-4f4e-4c8b-9ac0-72c47fd724c7", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "1bdfdb211667e2923a5e4b6649c9e28720507cad7ef6eae8496298c7028b4f16"}, "3": {"node_id": "b01d032b-fe92-4a01-be44-488dac655c47", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e851ef16996b4ea7bb36c992715ad5e7f19ae976b166923a07f56e0e7b7892dd"}}, "hash": "078458cf6ab3bec53be24f358001dcddf8c5261693a5dfe22df74e26fcc45473", "text": "53], much of the recent research has fo- cused on designing Convolutional Neural Networks (CNNs) for both detection [81, 16, 19, 52, 73] and description [42, 72]. Trained with challenging data, CNNs largely improve the accuracy and robustness of matching. Local features now come in many flavors: some are better localized [41], highly repeatable [16], cheap to store and match [54], invariant to specific changes [46], or ignore unreliable objects [73].\n\nOther, orthogonal works instead adaptively modulate the network depth by predicting whether the prediction of a token at a given layer is final or requires further com- putations [15, 20, 62] . This is mostly inspired by adap- tive schemes developed for CNNs by the vision commu-\n\nLocal features are then matched with a nearest neighbor search in descriptor space. Because of non-matchable key-\n\n2\n\nexit?Layer", "start_char_idx": 9260, "end_char_idx": 10125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b01d032b-fe92-4a01-be44-488dac655c47": {"__data__": {"id_": "b01d032b-fe92-4a01-be44-488dac655c47", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "421707ef-f3a5-4b5e-a7e5-238e600906f8", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "078458cf6ab3bec53be24f358001dcddf8c5261693a5dfe22df74e26fcc45473"}, "3": {"node_id": "761861c5-296f-439a-a225-b986fdd74fc1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "06b3b4215c516161f9840fd68dac85c3d905ec97d8e5a1e485e483d5d39cca3d"}}, "hash": "e851ef16996b4ea7bb36c992715ad5e7f19ae976b166923a07f56e0e7b7892dd", "text": "space. Because of non-matchable key-\n\n2\n\nexit?Layer #1attention\n\nPruning\n\nself\n\nself\n\nMatching\u2026no\n\n<latexit", "start_char_idx": 10141, "end_char_idx": 10248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "761861c5-296f-439a-a225-b986fdd74fc1": {"__data__": {"id_": "761861c5-296f-439a-a225-b986fdd74fc1", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "b01d032b-fe92-4a01-be44-488dac655c47", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e851ef16996b4ea7bb36c992715ad5e7f19ae976b166923a07f56e0e7b7892dd"}, "3": {"node_id": "46e6d9bb-340f-4ee3-8d21-edbaa5348e1e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8b28e24e81893497d9f38662c3f1ced9261a4fa4325f1ef096ffa23e0b547957"}}, "hash": "06b3b4215c516161f9840fd68dac85c3d905ec97d8e5a1e485e483d5d39cca3d", "text": "sha1_base64=\"+R8ETE7Hij8x8HNVdpggh7Ao4p8=\">AAACQHicbVC7TsMwFHV4lvJqYWSJqJCAoUpQBYwVLIytRB9SE1WOc0ut2k5kO4Uqyhewwg/xF/wBG2Jlwm0zUMqVLB2fc+/18QliRpV2nHdrZXVtfWOzsFXc3tnd2y+VD9oqSiSBFolYJLsBVsCogJammkE3loB5wKATjG6nemcMUtFI3OtJDD7HD4IOKMHaUE3SL1WcqjMrexm4OaigvBr9snXmhRFJOAhNGFaq5zqx9lMsNSUMsqKXKIgxGeEH6BkoMAflpzOnmX1imNAeRNIcoe0Z+3sixV", "start_char_idx": 10296, "end_char_idx": 10629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46e6d9bb-340f-4ee3-8d21-edbaa5348e1e": {"__data__": {"id_": "46e6d9bb-340f-4ee3-8d21-edbaa5348e1e", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "761861c5-296f-439a-a225-b986fdd74fc1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "06b3b4215c516161f9840fd68dac85c3d905ec97d8e5a1e485e483d5d39cca3d"}, "3": {"node_id": "dc2e61d6-cb40-4f23-b2f6-dc8d41aef858", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9f25e45c596e11cce807e6fb171ec04f945bd109a9a6b9b2d9c93ce4725c88c6"}}, "hash": "8b28e24e81893497d9f38662c3f1ced9261a4fa4325f1ef096ffa23e0b547957", "text": "ypCQ9MJ8d6qP5qU/I/rZfowbWfUhEnGgSZPzRImK0je/ptO6QSiGYTAzCR1Hi1yRBLTLQJZ2FTOKaxyl0/zW0vuAi4uUsQ8EgizrEIU+8867l+6gU89abOJE8rbpZlRROu+zfKZdC+qLqX1VqzVqnf5DEX0BE6RqfIRVeoju5QA7UQQYCe0Qt6td6sD+vT+pq3rlj5zCFaKOv7ByUusGA=</latexit>cconfidence  Layer #N\n\n<latexit", "start_char_idx": 10630, "end_char_idx": 10885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dc2e61d6-cb40-4f23-b2f6-dc8d41aef858": {"__data__": {"id_": "dc2e61d6-cb40-4f23-b2f6-dc8d41aef858", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "46e6d9bb-340f-4ee3-8d21-edbaa5348e1e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8b28e24e81893497d9f38662c3f1ced9261a4fa4325f1ef096ffa23e0b547957"}, "3": {"node_id": "c7c3abef-9148-4b08-badd-b31eb06ce1fe", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7971cac1db8b883ca4fcd033e2e6d660b6772766158d773d726fac61a1a6a195"}}, "hash": "9f25e45c596e11cce807e6fb171ec04f945bd109a9a6b9b2d9c93ce4725c88c6", "text": " Layer #N\n\n<latexit sha1_base64=\"sGScxVPZ5yVzivKihlbkx5X+bfM=\">AAACRHicbVDLTsMwEHR4lvIqcOQSUSFBD1WCKuDI48KxSBQqNaFynG1r1XYi2ylUUb6BK/wQ/8A/cENcEW6bA21ZydJ4Znc9niBmVGnH+bAWFpeWV1YLa8X1jc2t7dLO7r2KEkmgQSIWyWaAFTAqoKGpZtCMJWAeMHgI+tcj/WEAUtFI3OlhDD7HXUE7lGBtqIZXiR8v26WyU3XGZc8DNwdllFe9vWMde2FEEg5CE4aVarlOrP0US00Jg6zoJQpiTPq4Cy0DBeag/HTsNrMPDRPanUiaI7Q9Zv9", "start_char_idx": 10869, "end_char_idx": 11223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7c3abef-9148-4b08-badd-b31eb06ce1fe": {"__data__": {"id_": "c7c3abef-9148-4b08-badd-b31eb06ce1fe", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "dc2e61d6-cb40-4f23-b2f6-dc8d41aef858", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9f25e45c596e11cce807e6fb171ec04f945bd109a9a6b9b2d9c93ce4725c88c6"}, "3": {"node_id": "65dd82f8-de16-480f-ab50-9b75a01a6314", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "96d9bf94cd035e91b99417a8e8b5b53a1c72b5a6a01b4c2266c0826f0543943a"}}, "hash": "7971cac1db8b883ca4fcd033e2e6d660b6772766158d773d726fac61a1a6a195", "text": "OpJgrNeSB6eRY99SsNiL/01qJ7pz7KRVxokGQyUOdhNk6skdft0MqgWg2NAATSY1Xm/SwxESbgKY2hQMaq9z188T2lIuAm7sEAU8k4hyLMPUqWcv1Uy/gqTdyJnladrMsK5pw3dko58H9SdU9rdZua+WLqzzmAtpHB+gIuegMXaAbVEcNRBBFL+gVvVnv1qf1ZX1PWhesfGYPTZX18wv2bLG6</latexit>pA<latexit", "start_char_idx": 11241, "end_char_idx": 11478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "65dd82f8-de16-480f-ab50-9b75a01a6314": {"__data__": {"id_": "65dd82f8-de16-480f-ab50-9b75a01a6314", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "c7c3abef-9148-4b08-badd-b31eb06ce1fe", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7971cac1db8b883ca4fcd033e2e6d660b6772766158d773d726fac61a1a6a195"}, "3": {"node_id": "e4bf0c50-cd16-4a42-80d1-508a739cd3b2", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "f4c3d7ce6360366973eeb399a813b0a58dba69206135ff9b2b18cf08cbf22bb0"}}, "hash": "96d9bf94cd035e91b99417a8e8b5b53a1c72b5a6a01b4c2266c0826f0543943a", "text": "sha1_base64=\"AT4FWDS3vmt4CLG/ezI148tR5AQ=\">AAACRHicbVDLTsMwEHR4U56FI5eICAl6qBKEgGNVLhxBolCpCZXjbIqF7US2U6iifANX+CH+gX/ghrgi3DYH2rKSpfHM7no8Ycqo0q77Yc3NLywuLa+sVtbWNza3tqs7tyrJJIEWSVgi2yFWwKiAlqaaQTuVgHnI4C58vBjqd32QiibiRg9SCDjuCRpTgrWhWn4tvW92tx237o7KngVeCRxU1lW3ah35UUIyDkIThpXqeG6qgxxLTQmDouJnClJMHnEPOgYKzEEF+chtYR8YJrLjRJojtD1i/07", "start_char_idx": 11479, "end_char_idx": 11813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e4bf0c50-cd16-4a42-80d1-508a739cd3b2": {"__data__": {"id_": "e4bf0c50-cd16-4a42-80d1-508a739cd3b2", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "65dd82f8-de16-480f-ab50-9b75a01a6314", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "96d9bf94cd035e91b99417a8e8b5b53a1c72b5a6a01b4c2266c0826f0543943a"}, "3": {"node_id": "6107db41-ff01-42eb-a3f6-f486a16a4f2c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "80db8fede1bde1e6a1d1eb62524138761c4e20ccb1d11f37533032bd4fd93555"}}, "hash": "f4c3d7ce6360366973eeb399a813b0a58dba69206135ff9b2b18cf08cbf22bb0", "text": "kmCs14KHp5Fg/qGltSP6ndTIdnwc5FWmmQZDxQ3HGbJ3Yw6/bEZVANBsYgImkxqtNHrDERJuAJjZFfZqq0vXz2PaEi5CbuwQBTyThHIso92tFxwtyP+S5P3Qmee54RVFUTLjedJSz4Pa47p3WT65PnEazjHkF7aF9dIg8dIYa6BJdoRYiiKIX9IrerHfr0/qyvsetc1Y5s4smyvr5BfhIsbs=</latexit>pB<latexit", "start_char_idx": 11814, "end_char_idx": 12051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6107db41-ff01-42eb-a3f6-f486a16a4f2c": {"__data__": {"id_": "6107db41-ff01-42eb-a3f6-f486a16a4f2c", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "e4bf0c50-cd16-4a42-80d1-508a739cd3b2", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "f4c3d7ce6360366973eeb399a813b0a58dba69206135ff9b2b18cf08cbf22bb0"}, "3": {"node_id": "57e65d6e-31c8-4ff2-b498-157573256f9f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "84da4a991f1e0541cb3210d76e8237dba85566a878e9d247774d2a448b621d89"}}, "hash": "80db8fede1bde1e6a1d1eb62524138761c4e20ccb1d11f37533032bd4fd93555", "text": "sha1_base64=\"ENdw5w7DzFyMTcYa4zs53AdJrQA=\">AAACRHicbVDLTsMwEHR4U56FI5eICAl6qBKEgGNVLhxBolCpCZXjbIqF7US2U6iifANX+CH+gX/ghrgi3DYH2rKSpfHM7no8Ycqo0q77Yc3NLywuLa+sVtbWNza3tqs7tyrJJIEWSVgi2yFWwKiAlqaaQTuVgHnI4C58vBjqd32QiibiRg9SCDjuCRpTgrWhWn4tum92tx237o7KngVeCRxU1lW3ah35UUIyDkIThpXqeG6qgxxLTQmDouJnClJMHnEPOgYKzEEF+chtYR8YJrLjRJojtD1", "start_char_idx": 12052, "end_char_idx": 12382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "57e65d6e-31c8-4ff2-b498-157573256f9f": {"__data__": {"id_": "57e65d6e-31c8-4ff2-b498-157573256f9f", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "6107db41-ff01-42eb-a3f6-f486a16a4f2c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "80db8fede1bde1e6a1d1eb62524138761c4e20ccb1d11f37533032bd4fd93555"}, "3": {"node_id": "ef4c8c43-2e1d-45a8-b380-b47b75395f42", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6bcd0ab1fe52c8c6244bf11f081fed0a09d6650fbc1eb36e879f222db67e3ff3"}}, "hash": "84da4a991f1e0541cb3210d76e8237dba85566a878e9d247774d2a448b621d89", "text": "i/07kmCs14KHp5Fg/qGltSP6ndTIdnwc5FWmmQZDxQ3HGbJ3Yw6/bEZVANBsYgImkxqtNHrDERJuAJjZFfZqq0vXz2PaEi5CbuwQBTyThHIso92tFxwtyP+S5P3Qmee54RVFUTLjedJSz4Pa47p3WT65PnEazjHkF7aF9dIg8dIYa6BJdoRYiiKIX9IrerHfr0/qyvsetc1Y5s4smyvr5BeHgsa8=</latexit>dB<latexit", "start_char_idx": 12383, "end_char_idx": 12624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ef4c8c43-2e1d-45a8-b380-b47b75395f42": {"__data__": {"id_": "ef4c8c43-2e1d-45a8-b380-b47b75395f42", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "57e65d6e-31c8-4ff2-b498-157573256f9f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "84da4a991f1e0541cb3210d76e8237dba85566a878e9d247774d2a448b621d89"}, "3": {"node_id": "640a9b0b-a083-4364-90e2-2f5543b56315", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "abf15c98c5c8653cd5f102fbba312087d82b6416cab8e8ceaee93b95aa0c9730"}}, "hash": "6bcd0ab1fe52c8c6244bf11f081fed0a09d6650fbc1eb36e879f222db67e3ff3", "text": "sha1_base64=\"PsaLD2Mv4BuDKV4DY8PhlB54U48=\">AAACRHicbVDLTsMwEHR4lvIqcOQSUSFBD1WCKuDI48KxSBQqNaFynG1r1XYi2ylUUb6BK/wQ/8A/cENcEW6bA21ZydJ4Znc9niBmVGnH+bAWFpeWV1YLa8X1jc2t7dLO7r2KEkmgQSIWyWaAFTAqoKGpZtCMJWAeMHgI+tcj/WEAUtFI3OlhDD7HXUE7lGBtqIZXCR8v26WyU3XGZc8DNwdllFe9vWMde2FEEg5CE4aVarlOrP0US00Jg6zoJQpiTPq4Cy0DBeag/HTsNrMPDRPanUiaI7Q9Zv9Op", "start_char_idx": 12625, "end_char_idx": 12961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "640a9b0b-a083-4364-90e2-2f5543b56315": {"__data__": {"id_": "640a9b0b-a083-4364-90e2-2f5543b56315", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "ef4c8c43-2e1d-45a8-b380-b47b75395f42", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6bcd0ab1fe52c8c6244bf11f081fed0a09d6650fbc1eb36e879f222db67e3ff3"}, "3": {"node_id": "34b32204-2ef5-4f6b-9793-44bb1dd96711", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "99de19ad3d4f29861063de6c0669b54c3ab1d4a997fb727804543587b2daccda"}}, "hash": "abf15c98c5c8653cd5f102fbba312087d82b6416cab8e8ceaee93b95aa0c9730", "text": "JgrNeSB6eRY99SsNiL/01qJ7pz7KRVxokGQyUOdhNk6skdft0MqgWg2NAATSY1Xm/SwxESbgKY2hQMaq9z188T2lIuAm7sEAU8k4hyLMPUqWcv1Uy/gqTdyJnladrMsK5pw3dko58H9SdU9rdZua+WLqzzmAtpHB+gIuegMXaAbVEcNRBBFL+gVvVnv1qf1ZX1PWhesfGYPTZX18wvgBLGu</latexit>dA  <latexit", "start_char_idx": 12962, "end_char_idx": 13199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "34b32204-2ef5-4f6b-9793-44bb1dd96711": {"__data__": {"id_": "34b32204-2ef5-4f6b-9793-44bb1dd96711", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "640a9b0b-a083-4364-90e2-2f5543b56315", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "abf15c98c5c8653cd5f102fbba312087d82b6416cab8e8ceaee93b95aa0c9730"}, "3": {"node_id": "872c14ae-0416-476d-97fa-38a37827f66b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7e3069971080c23face66dae5c80bbdba566a2a0395f656787ad255c3112656b"}}, "hash": "99de19ad3d4f29861063de6c0669b54c3ab1d4a997fb727804543587b2daccda", "text": " <latexit sha1_base64=\"+R8ETE7Hij8x8HNVdpggh7Ao4p8=\">AAACQHicbVC7TsMwFHV4lvJqYWSJqJCAoUpQBYwVLIytRB9SE1WOc0ut2k5kO4Uqyhewwg/xF/wBG2Jlwm0zUMqVLB2fc+/18QliRpV2nHdrZXVtfWOzsFXc3tnd2y+VD9oqSiSBFolYJLsBVsCogJammkE3loB5wKATjG6nemcMUtFI3OtJDD7HD4IOKMHaUE3SL1WcqjMrexm4OaigvBr9snXmhRFJOAhNGFaq5zqx9lMsNSUMsqKXKIgxGeEH6BkoMAflpzOnmX1imNAeRNIcoe0Z+3sixV", "start_char_idx": 13192, "end_char_idx": 13535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "872c14ae-0416-476d-97fa-38a37827f66b": {"__data__": {"id_": "872c14ae-0416-476d-97fa-38a37827f66b", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "34b32204-2ef5-4f6b-9793-44bb1dd96711", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "99de19ad3d4f29861063de6c0669b54c3ab1d4a997fb727804543587b2daccda"}, "3": {"node_id": "37bed96a-f95a-47dd-a1d9-a2ed0e58b845", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "41c2c7c4d4c22400d0076daf9b7c0296eade9e468b89d53ddfb0b20fde9309de"}}, "hash": "7e3069971080c23face66dae5c80bbdba566a2a0395f656787ad255c3112656b", "text": "ypCQ9MJ8d6qP5qU/I/rZfowbWfUhEnGgSZPzRImK0je/ptO6QSiGYTAzCR1Hi1yRBLTLQJZ2FTOKaxyl0/zW0vuAi4uUsQ8EgizrEIU+8867l+6gU89abOJE8rbpZlRROu+zfKZdC+qLqX1VqzVqnf5DEX0BE6RqfIRVeoju5QA7UQQYCe0Qt6td6sD+vT+pq3rlj5zCFaKOv7ByUusGA=</latexit>c  matchabilitysimilarity  crossassignment  B  imagesAlocal features\n\nexit?yes!\n\nFigure 3. The LightGlue architecture. Given a pair of input local features (d, p), each layer augments the visual descriptors (\u2022,\u2022) with context based on", "start_char_idx": 13544, "end_char_idx": 14002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "37bed96a-f95a-47dd-a1d9-a2ed0e58b845": {"__data__": {"id_": "37bed96a-f95a-47dd-a1d9-a2ed0e58b845", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "872c14ae-0416-476d-97fa-38a37827f66b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7e3069971080c23face66dae5c80bbdba566a2a0395f656787ad255c3112656b"}, "3": {"node_id": "de020396-b2cb-41d4-ba1f-4108e5a557b3", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "d746e5189900fbc62a1c94b6844956fae00c520294649e7f3b90242d8d7788af"}}, "hash": "41c2c7c4d4c22400d0076daf9b7c0296eade9e468b89d53ddfb0b20fde9309de", "text": "p), each layer augments the visual descriptors (\u2022,\u2022) with context based on self- and cross-attention units with positional encoding \u2299. A confidence classifier c helps decide whether to stop the inference. If few points are confident, the inference proceeds to the next layer but we prune points that are confidently unmatchable. Once a confident state if reached, LightGlue predicts an assignment between points based on their pariwise similarity and unary matchability.\n\ni \u2190 dI\n\nresponding visual descriptor xI i and subsequently updated by each layer. We define a layer as a succession of one self-attention and one cross-attention units.\n\nnity [71, 80, 40, 21, 36, 76]. In Transformers, the type of po- sitional encoding has a large impact on the accuracy. While absolute sinusoidal [74] or learned encodings [17, 51] were initially prevalent, recent works have studied relative en- codings [63, 67]", "start_char_idx": 13940, "end_char_idx": 14842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "de020396-b2cb-41d4-ba1f-4108e5a557b3": {"__data__": {"id_": "de020396-b2cb-41d4-ba1f-4108e5a557b3", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "37bed96a-f95a-47dd-a1d9-a2ed0e58b845", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "41c2c7c4d4c22400d0076daf9b7c0296eade9e468b89d53ddfb0b20fde9309de"}, "3": {"node_id": "caea057f-c4ca-4b03-acde-d84deb48413f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "44b8c215e4ad0bc91fe71878bb4297781a2e0aee8e0fc06517a09d425ca84f31"}}, "hash": "d746e5189900fbc62a1c94b6844956fae00c520294649e7f3b90242d8d7788af", "text": "prevalent, recent works have studied relative en- codings [63, 67] to stabilize the training and better capture long-range dependencies.\n\nAttention unit: (MLP) updates the state given a message mI\u2190S from a source image S \u2208 {A, B}:\n\nIn each unit, a Multi-Layer Perceptron aggregated\n\ni\n\nLightGlue adapts some of these innovations to 2D feature matching and shows gains in both efficiency and accuracy.\n\ni + MLP (cid:0)(cid:2)xI\n\n(cid:3)(cid:1) ,\n\ni | mI\u2190S i\n\ni \u2190 xI xI\n\n(1)\n\n3. Fast feature matching\n\nwhere [\u00b7 | \u00b7] stacks two vectors. This is computed for all points in both images in parallel. In a self-attention unit, each image I pulls information from points of the same image and thus S = I. In a cross-attention", "start_char_idx": 14849, "end_char_idx": 15566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "caea057f-c4ca-4b03-acde-d84deb48413f": {"__data__": {"id_": "caea057f-c4ca-4b03-acde-d84deb48413f", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "de020396-b2cb-41d4-ba1f-4108e5a557b3", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "d746e5189900fbc62a1c94b6844956fae00c520294649e7f3b90242d8d7788af"}, "3": {"node_id": "e48db89d-394f-4da1-9e88-5c1eab40747c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "0a1981d5799a0ed5ff0dab2360daea6d50d3d0c0c5fff2802fe933fab06c5846"}}, "hash": "44b8c215e4ad0bc91fe71878bb4297781a2e0aee8e0fc06517a09d425ca84f31", "text": "information from points of the same image and thus S = I. In a cross-attention unit, each image pulls informa- tion from the other image and S = {A, B}\\I.\n\nProblem formulation: LightGlue predicts a partial assign- ment between two sets of local features extracted from im- ages A and B, following SuperGlue. Each local feature i is composed of a 2D point position pi := (x, y)i \u2208 [0, 1]2, nor- malized by the image size, and a visual descriptor di \u2208 Rd. Images A and B have M and N local features, indexed by A := {1, ..., M } and B := {1, ..., N }, respectively.\n\nThe message is computed by an attention mechanism as\n\nthe weighted average of all states j of image S:\n\n(cid:88)\n\n(cid:0)aIS\n\n(cid:1)\n\nmI\u2190S i\n\nj", "start_char_idx": 15560, "end_char_idx": 16269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e48db89d-394f-4da1-9e88-5c1eab40747c": {"__data__": {"id_": "e48db89d-394f-4da1-9e88-5c1eab40747c", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "caea057f-c4ca-4b03-acde-d84deb48413f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "44b8c215e4ad0bc91fe71878bb4297781a2e0aee8e0fc06517a09d425ca84f31"}, "3": {"node_id": "5b414108-ce4d-40f6-a572-2975e369348c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "0df0570fa1840b9ad31bafbcfce4f175e61e71b3d5dff1b912c0bcae415ff527"}}, "hash": "0a1981d5799a0ed5ff0dab2360daea6d50d3d0c0c5fff2802fe933fab06c5846", "text": "i\n\nj WxS\n\n=\n\nSoftmax k\u2208S\n\n(2)\n\n,\n\nik\n\nj\n\nWe design LightGlue to output a set of correspondences M = {(i, j)} \u2282 A \u00d7 B. Each point is matchable at least once, as it stems from a unique 3D point, and some keypoints are unmatchable, due to occlusion or non-repeatability. As in previous works, we thus seek a soft partial assignment matrix P \u2208 [0, 1]M \u00d7N between local features in A and B, from which we can extract correspondences.\n\nj\u2208S\n\nwhere W is a projection matrix and aIS ij is an attention score between points i and j of images I and S. How this score is computed differs for self- and cross-attention units.\n\nSelf-attention: Each point attends to all points of the same image. We perform the same following steps for each im- age I and thus drop the superscript I for", "start_char_idx": 16330, "end_char_idx": 17102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5b414108-ce4d-40f6-a572-2975e369348c": {"__data__": {"id_": "5b414108-ce4d-40f6-a572-2975e369348c", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "e48db89d-394f-4da1-9e88-5c1eab40747c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "0a1981d5799a0ed5ff0dab2360daea6d50d3d0c0c5fff2802fe933fab06c5846"}, "3": {"node_id": "27232c73-14cd-4802-ae69-4433455e8b32", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a1c1f49808a0be3bfa997e3bf2a096a43a386cec5f0b8a2b663861db9936a7ef"}}, "hash": "0df0570fa1840b9ad31bafbcfce4f175e61e71b3d5dff1b912c0bcae415ff527", "text": "the same following steps for each im- age I and thus drop the superscript I for clarity. For each point i, the current state xi is first decomposed into key and query vectors ki and qi via different linear transformations. We then define the attention score between points i and j as\n\nOverview \u2013 Figure 3: LightGlue is made of a stack of L identical layers that process the two sets jointly. Each layer is composed of self- and cross-attention units that update the representation of each point. A classifier then decides, at each layer, whether to halt the inference, thus avoiding unnecessary computations. A lightweight head finally com- putes a partial assignment from the set of representations.\n\ni R(cid:0)pj \u2212 pi\n\n(cid:1) kj ,\n\naij = q\u22a4\n\n(3)\n\nwhere R(\u00b7) \u2208 Rd\u00d7d is a rotary encoding [67] of the relative position between the points. We partition the space into", "start_char_idx": 17043, "end_char_idx": 17909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "27232c73-14cd-4802-ae69-4433455e8b32": {"__data__": {"id_": "27232c73-14cd-4802-ae69-4433455e8b32", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "5b414108-ce4d-40f6-a572-2975e369348c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "0df0570fa1840b9ad31bafbcfce4f175e61e71b3d5dff1b912c0bcae415ff527"}, "3": {"node_id": "0c9c177e-0989-4c36-b6e1-ed23a7276819", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9e64a2d89f11fd757e3494f6233404a56eeb106ab23c4f29ae32b547092122d4"}}, "hash": "a1c1f49808a0be3bfa997e3bf2a096a43a386cec5f0b8a2b663861db9936a7ef", "text": "encoding [67] of the relative position between the points. We partition the space into d/2 2D subspaces and rotate each of them by an angle corre- sponding, following Fourier Features [37], to the projection\n\n3.1. Transformer backbone\n\nWe associate each local feature i in image I \u2208 {A, B} i \u2208 Rd. The state is initialized with the cor-\n\nwith a state xI\n\n3\n\nonto a learned basis bk \u2208 R2: \u02c6R(b\u22a4\n\n\uf8eb\n\n\uf8f6 \uf8f8, \u02c6R(\u03b8) = (cid:0) cos \u03b8 \u2212 sin \u03b8\n\n1 p)\n\n0\n\n(cid:1) .\n\n...\n\nR(p) =\n\n\uf8ed\n\nsin \u03b8 cos \u03b8\n\n\u02c6R(b\u22a4\n\n0\n\nd/2p)\n\n(4) Positional encoding is a critical part of attention as", "start_char_idx": 17901, "end_char_idx": 18459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c9c177e-0989-4c36-b6e1-ed23a7276819": {"__data__": {"id_": "0c9c177e-0989-4c36-b6e1-ed23a7276819", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "27232c73-14cd-4802-ae69-4433455e8b32", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a1c1f49808a0be3bfa997e3bf2a096a43a386cec5f0b8a2b663861db9936a7ef"}, "3": {"node_id": "a02df946-1b52-436a-87ea-58340691d278", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "dfe6fd956e0d1d3d71b507e160efdb4531b29c7c08f6b3e430cee6ba66cc5f73"}}, "hash": "9e64a2d89f11fd757e3494f6233404a56eeb106ab23c4f29ae32b547092122d4", "text": "Positional encoding is a critical part of attention as it allows addressing different elements based on their position. We note that, in projective camera geometry, the position of visual observations is equivariant w.r.t. a translation of the camera within the image plane: 2D points that stem from 3D points on the same fronto-parallel plane are translated in an identical way and their relative distance remains constant. This calls for an encoding that only captures the relative but not the absolute position of points.\n\nFigure 4. Point pruning. As LigthGlue aggregates context, it can find out early that some points (\u2022) are unmatchable and thus exclude them from subsequent layers. Other, non-repeatable points are excluded in later layers: \u2022 \u2192 \u2022 \u2192 \u2022. This reduces the inference time and the search space (\u2022) to ultimately find good matches fast.\n\nThe rotary encoding [67] enables the model to retrieve points j that are located at a learned relative position from i. The positional", "start_char_idx": 18487, "end_char_idx": 19476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a02df946-1b52-436a-87ea-58340691d278": {"__data__": {"id_": "a02df946-1b52-436a-87ea-58340691d278", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "0c9c177e-0989-4c36-b6e1-ed23a7276819", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9e64a2d89f11fd757e3494f6233404a56eeb106ab23c4f29ae32b547092122d4"}, "3": {"node_id": "4f102465-74bd-4e58-a28a-46526f522bea", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "0db1d0bab095d189a627907d944b76104a150819021bd519d7cfde7ce0b86aa8"}}, "hash": "dfe6fd956e0d1d3d71b507e160efdb4531b29c7c08f6b3e430cee6ba66cc5f73", "text": "retrieve points j that are located at a learned relative position from i. The positional encoding is not applied to the value vj and thus does not spill into the state xi. The encoding is identical for all layers and is thus computed once and cached.\n\nA pair of points (i, j) yields a correspondence when both points are predicted as matchable and when their similarity is higher than any other point in both images. We select pairs for which Pij is larger than a threshold \u03c4 and than any other element along both its row and column.\n\nCross-attention: Each point in I attends to all points of the other image S. We compute a key ki for each element but no query. This allows to express the score as\n\n3.3. Adaptive depth and width\n\nWe add two mechanisms that avoid unnecessary compu- tations and save inference time: i) we reduce the number of layers depending on the difficulty of the input image pair; ii) we prune out points that are confidently rejected", "start_char_idx": 19449, "end_char_idx": 20405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4f102465-74bd-4e58-a28a-46526f522bea": {"__data__": {"id_": "4f102465-74bd-4e58-a28a-46526f522bea", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "a02df946-1b52-436a-87ea-58340691d278", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "dfe6fd956e0d1d3d71b507e160efdb4531b29c7c08f6b3e430cee6ba66cc5f73"}, "3": {"node_id": "39cbc489-014e-406a-82a9-b31b11f722ea", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a267afda5441b33f6c6b5317df2cd8fa2f612920f7e057105bb8235daa71a084"}}, "hash": "0db1d0bab095d189a627907d944b76104a150819021bd519d7cfde7ce0b86aa8", "text": "of the input image pair; ii) we prune out points that are confidently rejected early.\n\n!= aSI ji\n\n\u22a4kS j\n\nij = kI aIS i\n\n(5)\n\n.\n\nWe thus need to compute the similarity only once for both I \u2190 S and S \u2190 I messages. This trick has been previously referred to as bidirectional attention [77]. Since this step is expensive, with a complexity of O(NMd), it saves a signifi- cant factor of 2. We do not add any positional information as relative positions are not meaningful across images.\n\nConfidence classifier: The backbone of LightGlue aug- ments input visual descriptors with context. These are often reliable if the image pair is easy, i.e. has high visual overlap and little appearance changes. In such case, predictions from early layers are confident and identical to those of late layers. We can then output these predictions and halt the inference. At the end of each", "start_char_idx": 20415, "end_char_idx": 21285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "39cbc489-014e-406a-82a9-b31b11f722ea": {"__data__": {"id_": "39cbc489-014e-406a-82a9-b31b11f722ea", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "4f102465-74bd-4e58-a28a-46526f522bea", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "0db1d0bab095d189a627907d944b76104a150819021bd519d7cfde7ce0b86aa8"}, "3": {"node_id": "98242268-27d0-4a33-809e-b4dc476681ab", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "50f469ab374e5e9b34b613c15a21846330702a365684d1dcc878cc9c66275d91"}}, "hash": "a267afda5441b33f6c6b5317df2cd8fa2f612920f7e057105bb8235daa71a084", "text": "We can then output these predictions and halt the inference. At the end of each layer, LightGlue infers the confidence\n\n3.2. Correspondence prediction\n\nof the predicted assignment of each point:\n\nWe design a lightweight head that predicts an assignment\n\ngiven the updated state at any layer.\n\nci = Sigmoid (MLP(xi)) \u2208 [0, 1] .\n\n(9)\n\nAssignment scores: We first compute a pairwise score matrix S \u2208 RM \u00d7N between the points of both images: (cid:1) \u22a4 Linear (cid:0)xB Sij = Linear (cid:0)xA\n\nA higher value indicates that the representation of i is reliable and final \u2013 it is confidently either matched or unmatchable. This is inspired by multiple works that successfully apply this strategy to language and vision tasks [62, 20, 71, 80, 40]. The compact MLP adds only 2% of inference time in the worst case but most often", "start_char_idx": 21286, "end_char_idx": 22105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "98242268-27d0-4a33-809e-b4dc476681ab": {"__data__": {"id_": "98242268-27d0-4a33-809e-b4dc476681ab", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "39cbc489-014e-406a-82a9-b31b11f722ea", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a267afda5441b33f6c6b5317df2cd8fa2f612920f7e057105bb8235daa71a084"}, "3": {"node_id": "c9ba681f-d8b7-4f8b-be47-d369fa644e67", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "321b40c326ae9e09bf67ab5de1dccaf9df42b524c89083aae8c3e4275b577935"}}, "hash": "50f469ab374e5e9b34b613c15a21846330702a365684d1dcc878cc9c66275d91", "text": "compact MLP adds only 2% of inference time in the worst case but most often saves much more.\n\n(cid:1)\n\n\u2200(i, j) \u2208 A\u00d7B, (6)\n\ni\n\nj\n\nwhere Linear(\u00b7) is a learned linear transformation with bias. This score encodes the affinity of each pair of points to be in correspondence, i.e. 2D projections of the same 3D point. We also compute, for each point, a matchability score as\n\nExit criterion: For a given layer \u2113, a point is deemed confi- dent if ci > \u03bb\u2113. We halt the inference if a sufficient ratio \u03b1 of all points is confident:\n\n\u03c3i = Sigmoid (Linear(xi)) \u2208 [0, 1] .\n\n(7)\n\nThis score encodes the likelihood of i to have a correspond- ing point. A point that is not detected in the other image, e.g. when occluded, is", "start_char_idx": 22110, "end_char_idx": 22821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c9ba681f-d8b7-4f8b-be47-d369fa644e67": {"__data__": {"id_": "c9ba681f-d8b7-4f8b-be47-d369fa644e67", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "98242268-27d0-4a33-809e-b4dc476681ab", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "50f469ab374e5e9b34b613c15a21846330702a365684d1dcc878cc9c66275d91"}, "3": {"node_id": "bb426f0a-bd9f-46cd-bfc2-4717564d1170", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7b52a49ecc42cf827ecdc457828bd55a08d924b7dccd03777fb80a0ed7a83c53"}}, "hash": "321b40c326ae9e09bf67ab5de1dccaf9df42b524c89083aae8c3e4275b577935", "text": "point that is not detected in the other image, e.g. when occluded, is not matchable and thus has \u03c3i \u2192 0. Correspondences: We combine both similarity and match- ability scores into a soft partial assignment matrix P as\n\n\uf8eb\n\n\uf8f6\n\n1 N +M\n\n(cid:88)\n\n(cid:88) i\u2208I (cid:74)\n\ncI i > \u03bb\u2113\n\nexit =\n\n(10)\n\n\uf8f8 > \u03b1 .\n\n\uf8ed\n\n(cid:75)\n\nI\u2208{A,B}\n\nWe observe, as in [62], that the classifier itself is less confi- dent in early layers. We thus decay \u03bb\u2113 throughout the layers based on the validation accuracy of each classifier. The exit\n\nPij = \u03c3A\n\ni \u03c3B\n\nj Softmax k\u2208A\n\n(Skj)i Softmax\n\n(Sik)j", "start_char_idx": 22826, "end_char_idx": 23391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bb426f0a-bd9f-46cd-bfc2-4717564d1170": {"__data__": {"id_": "bb426f0a-bd9f-46cd-bfc2-4717564d1170", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "c9ba681f-d8b7-4f8b-be47-d369fa644e67", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "321b40c326ae9e09bf67ab5de1dccaf9df42b524c89083aae8c3e4275b577935"}, "3": {"node_id": "b4c4cd88-7cea-4fd2-a854-f1187d6f3c6a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "2c3a63bed8ab63cb57563a1fb41ac1b4f2100c2756d3ce00354d395c758a8c2e"}}, "hash": "7b52a49ecc42cf827ecdc457828bd55a08d924b7dccd03777fb80a0ed7a83c53", "text": "k\u2208A\n\n(Skj)i Softmax\n\n(Sik)j .\n\n(8)\n\nk\u2208B\n\n4\n\n01M2M3M4M5M\n\n01M2M3M4M5M\n\n0.10.20.51\n\nLightGlue\n\n# Pairs# PairsLossRecall\n\n80859095100\n\nSuperGlue\n\nthreshold \u03b1 directly controls the trade-off between accuracy and inference time.\n\nPoint pruning: When the exit criterion is not met, points that are predicted as both confident and unmatchable are unlikely to aid the matching of other points in subsequent layers. Such points are for example in areas that are clearly not covisible across the images. We therefore discard them at each layer and feed only the remaining points to the next one. This significantly reduces computation, given the quadratic complexity of attention, and does not impact the accuracy.\n\nFigure 5. Ease of training.", "start_char_idx": 23423, "end_char_idx": 24156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b4c4cd88-7cea-4fd2-a854-f1187d6f3c6a": {"__data__": {"id_": "b4c4cd88-7cea-4fd2-a854-f1187d6f3c6a", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "bb426f0a-bd9f-46cd-bfc2-4717564d1170", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7b52a49ecc42cf827ecdc457828bd55a08d924b7dccd03777fb80a0ed7a83c53"}, "3": {"node_id": "cadb9d45-9843-4531-a2cd-2567a287210a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "25fd99d180137db3f0ef1a1bc9e60450de973c5b650e4536d384fcc07fa496fe"}}, "hash": "2c3a63bed8ab63cb57563a1fb41ac1b4f2100c2756d3ce00354d395c758a8c2e", "text": "and does not impact the accuracy.\n\nFigure 5. Ease of training. The LightGlue architecture vastly im- proves the speed of convergence of the pre-training on synthetic homographies. After 5M image pairs (only 2 GPU-days), LighGlue achieves -33% loss at the final layer and +4% match recall. Super- Glue requires over 7 days of training to reach a similar accuracy.\n\n3.4. Supervision\n\nWe train LightGlue in two stages: we first train it to pre- dict correspondences and only after train the confidence classifier. The latter thus does not impact the accuracy at the final layer or the convergence of the training.\n\npositional information throughout the layers. LightGlue in- stead relies on a relative encoding that is better comparable across images and is added in each self-attention unit. This makes it easier to leverage the positions and improves the accuracy of deeper layers.\n\nCorrespondences: We supervise the", "start_char_idx": 24130, "end_char_idx": 25045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cadb9d45-9843-4531-a2cd-2567a287210a": {"__data__": {"id_": "cadb9d45-9843-4531-a2cd-2567a287210a", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "b4c4cd88-7cea-4fd2-a854-f1187d6f3c6a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "2c3a63bed8ab63cb57563a1fb41ac1b4f2100c2756d3ce00354d395c758a8c2e"}, "3": {"node_id": "232c6788-a8b7-437d-84ad-fa0273d1901c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6d0b4324e82bf1138da46acfc35eed48f29abe7d2dae2d7397f964d8b91bd275"}}, "hash": "25fd99d180137db3f0ef1a1bc9e60450de973c5b650e4536d384fcc07fa496fe", "text": "the accuracy of deeper layers.\n\nCorrespondences: We supervise the assignment matrix P with ground truth labels estimated from two-view transfor- mations. Given a homography or pixel-wise depth and a relative pose, we wrap points from A to B and conversely. Ground truth matches M are pairs of points with a low re- projection error in both images and a consistent depth. Some points \u00afA \u2286 A and \u00afB \u2286 B are labeled as unmatchable when their reprojection or depth errors are sufficiently large with all other points. We then minimize the log-likelihood of the assignment predicted at each layer \u2113, pushing LightGlue to predict correct correspondences early:\n\nPrediction head: SuperGlue predicts an assignment by solving a differentiable optimal transport problem using the Sinkhorn algorithm [66, 48]. It consists in many iterations of row-wise and column-wise normalization, which is expen- sive in terms of both compute and memory. SuperGlue", "start_char_idx": 25041, "end_char_idx": 25981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "232c6788-a8b7-437d-84ad-fa0273d1901c": {"__data__": {"id_": "232c6788-a8b7-437d-84ad-fa0273d1901c", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "cadb9d45-9843-4531-a2cd-2567a287210a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "25fd99d180137db3f0ef1a1bc9e60450de973c5b650e4536d384fcc07fa496fe"}, "3": {"node_id": "f848e9ac-4c21-4afa-8fd7-ca37e3c61c3e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9ec1175c62d69ba798c3b910a2956c87295da5e69ead1bb49a5e168d6b74e41f"}}, "hash": "6d0b4324e82bf1138da46acfc35eed48f29abe7d2dae2d7397f964d8b91bd275", "text": "which is expen- sive in terms of both compute and memory. SuperGlue adds a dustbin to reject unmatchable points. We found that the dustbin entangles the similarity score of all points and thus yields suboptimal training dynamics. LightGlue disentangles similarity and matchability, which are much more efficient to predict. This also yields cleaner gradients.\n\n(cid:32)\n\n1 L\n\n1 |M|\n\n(cid:88)\n\n(cid:88)\n\nDeep supervision: Because of how expensive Sinkhorn is, SuperGlue cannot make predictions after each layer and is supervised only at the last one. The lighter head of LightGlue makes it possible to predict an assignment at each layer and to supervise it. This speeds up the convergence and enables exiting the inference after any layer, which is key to the efficiency gains of LightGlue.\n\nlog \u2113Pij\n\nloss =", "start_char_idx": 25984, "end_char_idx": 26792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f848e9ac-4c21-4afa-8fd7-ca37e3c61c3e": {"__data__": {"id_": "f848e9ac-4c21-4afa-8fd7-ca37e3c61c3e", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "232c6788-a8b7-437d-84ad-fa0273d1901c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6d0b4324e82bf1138da46acfc35eed48f29abe7d2dae2d7397f964d8b91bd275"}, "3": {"node_id": "277bc300-2c2e-4b92-9eea-fa96e785ae98", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b398594fede85dedd2f483f7a5b205f5409432c49a4648f5d5599e529f36e1ae"}}, "hash": "9ec1175c62d69ba798c3b910a2956c87295da5e69ead1bb49a5e168d6b74e41f", "text": "efficiency gains of LightGlue.\n\nlog \u2113Pij\n\nloss = \u2212\n\n(i,j)\u2208M 1 (cid:88) 2| \u00afA|\n\n\u2113\n\nlog (cid:0)1 \u2212 \u2113\u03c3A\n\n(cid:1)\n\n+\n\n(11)\n\ni\n\ni\u2208 \u00afA\n\n(cid:33)\n\n1 2| \u00afB|\n\n(cid:88)\n\nlog (cid:0)1 \u2212 \u2113\u03c3B\n\n(cid:1)\n\n+\n\n.\n\nj\n\nj\u2208 \u00afB\n\n4. Details that matter\n\nThe loss is balanced between positive and negative labels.\n\nConfidence classifier: We then train the MLP of Eq. (9) to predict whether the prediction of each layer is identical to the final one. Let \u2113mA i \u2208 B \u222a {\u2022} be the index of the point in B matched to i at layer \u2113,", "start_char_idx": 26806, "end_char_idx": 27305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "277bc300-2c2e-4b92-9eea-fa96e785ae98": {"__data__": {"id_": "277bc300-2c2e-4b92-9eea-fa96e785ae98", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "f848e9ac-4c21-4afa-8fd7-ca37e3c61c3e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9ec1175c62d69ba798c3b910a2956c87295da5e69ead1bb49a5e168d6b74e41f"}, "3": {"node_id": "f60c039b-347d-41d8-8334-99c8389902a7", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e189f89cf771de51ac75762cff571e7c9a868399ae18391e4d3877e09aede3b0"}}, "hash": "b398594fede85dedd2f483f7a5b205f5409432c49a4648f5d5599e529f36e1ae", "text": "{\u2022} be the index of the point in B matched to i at layer \u2113, with \u2113mA i = \u2022 if i is unmatchable. The ground truth binary label of each point is and identically for B. We then minimize the binary cross-entropy of the classifiers of layers \u2113 \u2208 {1, ..., L\u22121}.\n\nRecipe: LightGlue follows the supervised training setup of SuperGlue. We first pre-train the model with synthetic homo- graphies sampled from 1M images [50]. Such augmentations provide full and noise-free supervision but require careful tuning. LightGlue is then fine-tuned with the MegaDepth dataset [38], which includes 1M crowd-sourced images de- picting 196 tourism landmarks, with camera calibration and poses recovered by SfM and dense depth by multi-view stereo. Because large models easily overfit to such distinc- tive scenes, the pre-training is critical to the generalization of the model but was omitted", "start_char_idx": 27304, "end_char_idx": 28176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f60c039b-347d-41d8-8334-99c8389902a7": {"__data__": {"id_": "f60c039b-347d-41d8-8334-99c8389902a7", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "277bc300-2c2e-4b92-9eea-fa96e785ae98", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b398594fede85dedd2f483f7a5b205f5409432c49a4648f5d5599e529f36e1ae"}, "3": {"node_id": "a7a1ac86-b86e-4f9a-a2fc-2ecc2d885007", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "3627640734cc812d31fc79c9213fe95f485da8e3dc0a94bb7e396a7ff6411a3c"}}, "hash": "e189f89cf771de51ac75762cff571e7c9a868399ae18391e4d3877e09aede3b0", "text": "scenes, the pre-training is critical to the generalization of the model but was omitted in recent follow-ups [8, 65].\n\n\u2113mA\n\ni = LmA i\n\n(cid:74)\n\n(cid:75)\n\n3.5. Comparison with SuperGlue\n\nLightGlue is inspired by SuperGlue but differs in aspects\n\ncritical to its accuracy, efficiency, and ease of training.\n\nPositional encoding: SuperGlue encodes the absolute point positions with an MLP and fuses them early with the de- scriptors. We observed that the model tends to forget this\n\nTraining tricks: While the LightGlue architecture improves the training speed, stability, and accuracy, we found that some details have a large impact too. Figure 5 shows that\n\n5\n\nthis reduces the resources required to train a model compared to SuperGlue. This lowers the cost of training and makes deep matchers more accessible to the broader community.\n\nAUC - RANSAC AUC -", "start_char_idx": 28148, "end_char_idx": 29003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a7a1ac86-b86e-4f9a-a2fc-2ecc2d885007": {"__data__": {"id_": "a7a1ac86-b86e-4f9a-a2fc-2ecc2d885007", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "f60c039b-347d-41d8-8334-99c8389902a7", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e189f89cf771de51ac75762cff571e7c9a868399ae18391e4d3877e09aede3b0"}, "3": {"node_id": "d54f85af-d0fc-4636-a72f-ad4613bcf6e6", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "4190acdda5d9c109ac9f29d4ce0251da21f58a8bdebaf1450309d7655518fa42"}}, "hash": "3627640734cc812d31fc79c9213fe95f485da8e3dc0a94bb7e396a7ff6411a3c", "text": "more accessible to the broader community.\n\nAUC - RANSAC AUC - DLT\n\nfeatures + matcher\n\nR\n\nP\n\n@1px @5px @1px @5px\n\ndense LoFTR\n\n\n\n92.7\n\n41.5\n\n78.8\n\n38.5\n\n70.6\n\nSince the depth maps of MegaDepth are often incomplete, we also label points with a large epipolar error as unmatch- able. Carefully tuning and annealing the learning rate boosts the accuracy. Training with more points also does: we use 2k per image instead of 1k. The batch size matters: we use gradient checkpointing [10] and mixed-precision to fit 32 image pairs on a single GPU with 24GB VRAM.\n\nt NN+mutual 72.7 67.2 n i o 94.9 87.4 P r 95.5 83.0 e p u 94.3 88.9 S\n\n35.0 38.3 38.6 38.3\n\n75.3 79.3", "start_char_idx": 29026, "end_char_idx": 29685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d54f85af-d0fc-4636-a72f-ad4613bcf6e6": {"__data__": {"id_": "d54f85af-d0fc-4636-a72f-ad4613bcf6e6", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "a7a1ac86-b86e-4f9a-a2fc-2ecc2d885007", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "3627640734cc812d31fc79c9213fe95f485da8e3dc0a94bb7e396a7ff6411a3c"}, "3": {"node_id": "be8c7dcb-1b14-4850-a82b-5f9e9de74c4d", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "2c27b0b303dff65e222a4e7913331bbf62366df1d9f9659c1ce7c1dbbf47b919"}}, "hash": "4190acdda5d9c109ac9f29d4ce0251da21f58a8bdebaf1450309d7655518fa42", "text": "38.3 38.6 38.3\n\n75.3 79.3 79.0 79.6\n\n0.0 33.8 31.7 35.9\n\n2.0 76.7 76.0 78.6\n\nSuperGlue SGMNet LightGlue\n\nTable 1. Homography estimation on HPatches. LightGlue yields better correspondences than sparse matchers, with the highest preci- sion (P) and a high recall (R). This results in accurate homographies when estimated by RANSAC or even a faster least-squares solver (DLT). LightGlue is competitive with dense matchers like LoFTR.\n\nImplementation details: LighGlue has L=9 layers. Each attention unit has 4 heads. All representations have dimen- sion d=256. Throughout the paper, run-time numbers la- beled as optimized use an efficient implementation of self- attention [14]. More details are given in the Appendix.\n\nResults: Table 1 shows that LightGlue", "start_char_idx": 29716, "end_char_idx": 30472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "be8c7dcb-1b14-4850-a82b-5f9e9de74c4d": {"__data__": {"id_": "be8c7dcb-1b14-4850-a82b-5f9e9de74c4d", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "d54f85af-d0fc-4636-a72f-ad4613bcf6e6", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "4190acdda5d9c109ac9f29d4ce0251da21f58a8bdebaf1450309d7655518fa42"}, "3": {"node_id": "b15d6d5e-0064-4818-9503-39754929969d", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "19abd5c91ebc45d95080efbdbe59bb6de18c731bf62438f026b8a80240c1145d"}}, "hash": "2c27b0b303dff65e222a4e7913331bbf62366df1d9f9659c1ce7c1dbbf47b919", "text": "More details are given in the Appendix.\n\nResults: Table 1 shows that LightGlue yields correspon- dences with higher precision than and similar recall to Su- perGlue and SGMNet. When estimating homographies with DLT, this results in much more accurate estimates than with other matchers. LightGlue thus makes DLT, a simple solver, competitive with the expensive and slower MAGSAC [3]. At a coarse threshold of 5px, LightGlue is also more accurate than LoFTR despite being constrained by sparse keypoints.\n\nWe train LightGlue with both SuperPoint [16] and SIFT [41] local features but it is compatible with any other type. When fine-tuning the model on MegaDepth [38], we use the data splits of Sun et al. [68] to avoid training on scenes included in the Image Matching Challenge [31].\n\n5. Experiments\n\n5.2. Relative pose estimation\n\nWe evaluate LightGlue for the tasks of homography esti-", "start_char_idx": 30428, "end_char_idx": 31315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b15d6d5e-0064-4818-9503-39754929969d": {"__data__": {"id_": "b15d6d5e-0064-4818-9503-39754929969d", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "be8c7dcb-1b14-4850-a82b-5f9e9de74c4d", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "2c27b0b303dff65e222a4e7913331bbf62366df1d9f9659c1ce7c1dbbf47b919"}, "3": {"node_id": "dafcacc4-d8f9-4bff-bfcb-f6f74de7dc7c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "964c46907a3a329fdde56b32f4b5a0a22973e4b9190ca5b87db6bf5a1eb20462"}}, "hash": "19abd5c91ebc45d95080efbdbe59bb6de18c731bf62438f026b8a80240c1145d", "text": "pose estimation\n\nWe evaluate LightGlue for the tasks of homography esti- mation, relative pose estimation, and visual localization. We also analyze the impacts of our design decisions.\n\nWe evaluate LightGlue for pose estimation in outdoor scenes that exhibit strong occlusion and challenging lighting and structural changes.\n\nSetup: We use image pairs from the MegaDepth-1500 test set following the evaluation of [68]. The test set contains 1500 image pairs from two popular phototourism destina- tions: St. Peters Square and Reichstag. The data was col- lected in a way that the difficulty is balanced based on visual overlap. We evaluate our method on the downstream task of relative pose estimation.\n\n5.1. Homography estimation\n\nWe evaluate the quality of correspondences estimated by LightGlue on planar scenes of the HPatches [2] dataset. This dataset is composed of sequences of 5 image pairs, each under either illumination or viewpoint", "start_char_idx": 31320, "end_char_idx": 32263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dafcacc4-d8f9-4bff-bfcb-f6f74de7dc7c": {"__data__": {"id_": "dafcacc4-d8f9-4bff-bfcb-f6f74de7dc7c", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "b15d6d5e-0064-4818-9503-39754929969d", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "19abd5c91ebc45d95080efbdbe59bb6de18c731bf62438f026b8a80240c1145d"}, "3": {"node_id": "946c1d16-56bb-4d8c-ba0b-1555df851642", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8e1566a84f37c1b15434423a5df8882e61970e685bda3847a33377573bd597c6"}}, "hash": "964c46907a3a329fdde56b32f4b5a0a22973e4b9190ca5b87db6bf5a1eb20462", "text": "composed of sequences of 5 image pairs, each under either illumination or viewpoint changes.\n\nSetup: Following SuperGlue [56], we report the precision and recall compared to GT matches at a reprojection error of 3px. We also evaluate the accuracy of homographies esti- mated from the correspondences using robust and non-robust solvers: RANSAC [22] and the weighted DLT [24]. For each image pair, we compute the mean reprojection error of the four image corners and report the area under the cumula- tive error curve (AUC) up to values of 1px and 5px. Fol- lowing best practices in benchmarking [31] and unlike past works [56, 68], we use a state-of-the-art robust estimator [3] and extensively tune the inlier threshold for each method separately. We then report the highest scoring results.\n\nWe estimate an essential matrix both with vanilla RANSAC and LO-RANSAC [34],", "start_char_idx": 32256, "end_char_idx": 33126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "946c1d16-56bb-4d8c-ba0b-1555df851642": {"__data__": {"id_": "946c1d16-56bb-4d8c-ba0b-1555df851642", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "dafcacc4-d8f9-4bff-bfcb-f6f74de7dc7c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "964c46907a3a329fdde56b32f4b5a0a22973e4b9190ca5b87db6bf5a1eb20462"}, "3": {"node_id": "26ee11bc-2381-4340-9a52-20b53563588d", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b69adc0ac748e4f60284909ef5301879516ef5422eb7682e743880ff7d977174"}}, "hash": "8e1566a84f37c1b15434423a5df8882e61970e685bda3847a33377573bd597c6", "text": "an essential matrix both with vanilla RANSAC and LO-RANSAC [34], respectively, and decom- pose them into a rotation and a translation. The inlier thresh- old is tuned for each approach on the test data \u2013 we think that this makes the comparison more fair as we do not evaluate RANSAC itself. We compute the pose error as the maximum angular error in rotation and translation and we report its AUC at 5\u00b0, 10\u00b0, and 20\u00b0.\n\nBaselines: We extract 2048 local features per images, each resized such that its larger dimension is 1600 pixels. With SuperPoint [16] features, we compare LightGlue to nearest- neighbor matching with mutual check and to the official implementations of SuperGlue [56] and SGMNet [8]. With DISK [73] we only evaluate against its own strong baseline, as no other trained matcher with DISK is publicly available. We also evaluate the recent, dense deep matchers LoFTR [68], MatchFormer [78], and", "start_char_idx": 33143, "end_char_idx": 34053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26ee11bc-2381-4340-9a52-20b53563588d": {"__data__": {"id_": "26ee11bc-2381-4340-9a52-20b53563588d", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "946c1d16-56bb-4d8c-ba0b-1555df851642", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8e1566a84f37c1b15434423a5df8882e61970e685bda3847a33377573bd597c6"}, "3": {"node_id": "b494e4b9-ee9b-4794-a807-98e144fbea89", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "588f29a92f872a40ca4a93f430d670286164379c63785530e93ec0408436df4c"}}, "hash": "b69adc0ac748e4f60284909ef5301879516ef5422eb7682e743880ff7d977174", "text": "the recent, dense deep matchers LoFTR [68], MatchFormer [78], and ASpanFormer [9]. We carefully follow their respective evaluation setups and resize the input images such that their largest dimension is 840 pix- els (LoFTR, MatchFormer) or 1152 pixels (ASpanFormer).\n\nBaselines: We follow the setup of [68] and resize all images such that their smaller dimension is equal to 480 pixels. We evaluate sparse matchers with 1024 local features extracted by SuperPoint [16]. We compare LightGlue against nearest- neighbor matching with mutual check and the deep matchers SuperGlue [56] and SGMNet [8]. We use the official models trained on outdoor datasets [38, 64]. For reference, we also evaluate the dense matcher LoFTR [68], selecting only the top 1024 predicted matches for the sake of fairness.\n\n6\n\nRANSAC AUC\n\nLO-RANSAC AUC time (ms)\n\nDay\n\nNight\n\nfeatures +", "start_char_idx": 34053, "end_char_idx": 34912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b494e4b9-ee9b-4794-a807-98e144fbea89": {"__data__": {"id_": "b494e4b9-ee9b-4794-a807-98e144fbea89", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "26ee11bc-2381-4340-9a52-20b53563588d", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b69adc0ac748e4f60284909ef5301879516ef5422eb7682e743880ff7d977174"}, "3": {"node_id": "ff18a060-413a-4c56-a41a-faad54d5eecf", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "fee97cff4fcc7f1adbcc9775aa3f012689e5bddb91d25567587b8c4e5b5256a2"}}, "hash": "588f29a92f872a40ca4a93f430d670286164379c63785530e93ec0408436df4c", "text": "AUC time (ms)\n\nDay\n\nNight\n\nfeatures + matcher\n\nSuperPoint + matcher\n\npairs per second\n\n5\u00b0 / 10\u00b0 / 20\u00b0\n\n(0.25m,2\u00b0) / (0.5m,5\u00b0) / (1.0m,10\u00b0)\n\n88.2 / 95.5 / 98.7 SuperGlue SGMNet 86.8 / 94.2 / 97.7 ClusterGNN 89.4 / 95.5 / 98.5 LightGlue 89.2 / 95.4 / 98.5\n\n86.7 / 92.9 / 100 83.7 / 91.8 / 99.0 81.6 / 93.9 / 100 87.8 / 93.9 / 100\n\ne LoFTR s n e d\n\n52.8 / 69.2 / 81.2 53.3 / 69.7 / 81.8 55.3 / 71.5 / 83.1\n\n66.4 / 78.6 / 86.5 66.5 / 78.9 / 87.5 69.4", "start_char_idx": 34935, "end_char_idx": 35381, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ff18a060-413a-4c56-a41a-faad54d5eecf": {"__data__": {"id_": "ff18a060-413a-4c56-a41a-faad54d5eecf", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "b494e4b9-ee9b-4794-a807-98e144fbea89", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "588f29a92f872a40ca4a93f430d670286164379c63785530e93ec0408436df4c"}, "3": {"node_id": "28d4b0e1-a30c-44d2-b8c7-9ecd5f79254b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "99519410ebaa8a1c5eca4775eed0f689baaae49b75babc2ffc0e802db52c2f05"}}, "hash": "fee97cff4fcc7f1adbcc9775aa3f012689e5bddb91d25567587b8c4e5b5256a2", "text": "/ 86.5 66.5 / 78.9 / 87.5 69.4 / 81.1 / 88.9\n\n181 388 369\n\n6.5 10.2 13* 17.2 / 26.1\n\nMatchFormer ASpanFormer\n\nK NN+ratio S I D\n\n38.1 / 55.4 / 69.6 43.5 / 61.0 / 75.3\n\n57.2 / 69.5 / 78.6 61.3 / 74.3 / 83.8\n\n7.4 44.5\n\nLightGlue\n\nTable 3. Outdoor visual localization. On the Aachen Day-Night dataset, LightGlue performs on par with SuperGlue but runs 2.5\u00d7 faster, 4\u00d7 when optimized. SGMNet and ClusterGNN are both slower and less robust on night-time images (*approximation).\n\nt NN+mutual n i o P r e p u S\n\n31.7 / 46.8 / 60.1 49.7 / 67.1 / 80.6", "start_char_idx": 35393, "end_char_idx": 35935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "28d4b0e1-a30c-44d2-b8c7-9ecd5f79254b": {"__data__": {"id_": "28d4b0e1-a30c-44d2-b8c7-9ecd5f79254b", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "ff18a060-413a-4c56-a41a-faad54d5eecf", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "fee97cff4fcc7f1adbcc9775aa3f012689e5bddb91d25567587b8c4e5b5256a2"}, "3": {"node_id": "77424ee9-defc-4b65-9027-d7d6536a3e51", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9b606cd30099392a5374bacf6e870ae7fbc8306c201f3997885a9364a03b1f9b"}}, "hash": "99519410ebaa8a1c5eca4775eed0f689baaae49b75babc2ffc0e802db52c2f05", "text": "/ 46.8 / 60.1 49.7 / 67.1 / 80.6 43.2 / 61.6 / 75.6 49.9 / 67.0 / 80.1 49.4 / 67.2 / 80.1\n\n51.0 / 54.1 / 73.6 65.8 / 78.7 / 87.5 59.8 / 74.1 / 83.9 66.7 / 79.3 / 87.9 66.3 / 79.0 / 87.9\n\n5.7 70.0 73.8 44.2 31.4\n\nSuperGlue SGMNet LightGlue \u00eb adaptive\n\nBaselines: We extract up to 4096 features with Super- Point and match them with SuperGlue, SGMNet [8], Clus- terGNN [65], and LightGlue with adaptive depth and width. Since the implementation of ClusterGNN is not publicly available, we report the accuracy found in the original paper and the time estimates kindly provided by the authors.\n\nTable 2. Relative pose estimation. On the", "start_char_idx": 35935, "end_char_idx": 36567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "77424ee9-defc-4b65-9027-d7d6536a3e51": {"__data__": {"id_": "77424ee9-defc-4b65-9027-d7d6536a3e51", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "28d4b0e1-a30c-44d2-b8c7-9ecd5f79254b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "99519410ebaa8a1c5eca4775eed0f689baaae49b75babc2ffc0e802db52c2f05"}, "3": {"node_id": "8bed7003-1d02-43f9-8e87-77c5d10231c6", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "d7a704882b5df7e54b360b358ad58b6210d66e783bf1db1724e1f433326fa1c9"}}, "hash": "9b606cd30099392a5374bacf6e870ae7fbc8306c201f3997885a9364a03b1f9b", "text": "kindly provided by the authors.\n\nTable 2. Relative pose estimation. On the MegaDepth1500 dataset, LightGlue predicts more precise correspondences with higher pose accuracy (AUC), and speed than existing sparse matchers. It is competitive with dense matchers for a fraction of the inference time, and even outperforms LoFTR and MatchFormer with the superior LO-RANSAC estimator. The adaptive scheme greatly reduces the run time for only a minor loss of accuracy.\n\nResults: Table 3 shows that LightGlue reaches a similar accuracy as SuperGlue but at a 2.5\u00d7 higher throughput. The optimized variant, which leverages an efficient self- attention [14], increases the throughput by 4\u00d7. LightGlue thus matches up to 4096 keypoints in real time.\n\nLarger images would improve their accuracy, as with sparse features, but would incur prohibitive and unpractical run time and memory requirements.\n\nResults: Table 2 shows that LightGlue largely", "start_char_idx": 36528, "end_char_idx": 37460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8bed7003-1d02-43f9-8e87-77c5d10231c6": {"__data__": {"id_": "8bed7003-1d02-43f9-8e87-77c5d10231c6", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "77424ee9-defc-4b65-9027-d7d6536a3e51", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9b606cd30099392a5374bacf6e870ae7fbc8306c201f3997885a9364a03b1f9b"}, "3": {"node_id": "beb32c2b-70f0-4d38-8ce5-6b52d9f59e21", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5e2a2ea7fd43f2c58ae01994f591e5b72af077edd7b7acf3e37fa49e38b6ec3f"}}, "hash": "d7a704882b5df7e54b360b358ad58b6210d66e783bf1db1724e1f433326fa1c9", "text": "run time and memory requirements.\n\nResults: Table 2 shows that LightGlue largely outperforms the existing approaches SuperGlue and SGMNet on Super- Point features, and can greatly improve the matching accu- racy over DISK local features. It yields better correspon- dences and more accurate relative poses and reduces the inference time by 30%. LightGlue typically predicts slightly fewer matches than SuperGlue but those are more accu- rate. By detecting confident predictions early in the model, the adaptive variant is over 2\u00d7 faster than SuperGlue and SGMNet and still more accurate. With a carefully tuned LO- RANSAC [34], LightGlue can achieve higher accuracy than some popular dense matcher which are between 5 and 11 times slower. Among the evaluated dense matchers, ASPAN- Former is the most accurate. Considering trade-off between accuracy and speed, LightGlue outperforms all approaches by a large margin.\n\n5.4.", "start_char_idx": 37455, "end_char_idx": 38377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "beb32c2b-70f0-4d38-8ce5-6b52d9f59e21": {"__data__": {"id_": "beb32c2b-70f0-4d38-8ce5-6b52d9f59e21", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "8bed7003-1d02-43f9-8e87-77c5d10231c6", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "d7a704882b5df7e54b360b358ad58b6210d66e783bf1db1724e1f433326fa1c9"}, "3": {"node_id": "1a7b5a50-c21f-4b70-8cfc-e3a5d7b28483", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "59d28cccbe13a90de46fa51984c0805e19c59e98020e1d13bb2eca9aff36171f"}}, "hash": "5e2a2ea7fd43f2c58ae01994f591e5b72af077edd7b7acf3e37fa49e38b6ec3f", "text": "LightGlue outperforms all approaches by a large margin.\n\n5.4. Insights\n\nAblation study: We validate our design decisions by eval- uating LightGlue after its pre-training on the challenging synthetic homography dataset with extreme photometric augmentations. We train different variants with SuperPoint features and 5M samples, all within 4 GPU-days. We create a test set from the same augmentations applied to images unseen during training. We extract 512 keypoints from each. We also compare against SuperGlue, which we train with the same setup. More details are provided in the Appendix. We report the ablation results in Table 4. Compared to SuperGlue, LightGlue converges significantly faster, and achieves +4% recall and +12% precision. Note that Super- Glue can achieve similar accuracies as LightGlue with a long-enough training, but the improved convergence makes it much more practical to train on new data.\n\n5.3. Outdoor visual", "start_char_idx": 38394, "end_char_idx": 39332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1a7b5a50-c21f-4b70-8cfc-e3a5d7b28483": {"__data__": {"id_": "1a7b5a50-c21f-4b70-8cfc-e3a5d7b28483", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "beb32c2b-70f0-4d38-8ce5-6b52d9f59e21", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5e2a2ea7fd43f2c58ae01994f591e5b72af077edd7b7acf3e37fa49e38b6ec3f"}, "3": {"node_id": "5f0613c8-76e7-40f9-83a3-e676b05ba8be", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "42d7616ac3c9d2556fe401b2d26b83617e3b225f4c33e926ecfb191fc5cd6b0e"}}, "hash": "59d28cccbe13a90de46fa51984c0805e19c59e98020e1d13bb2eca9aff36171f", "text": "it much more practical to train on new data.\n\n5.3. Outdoor visual localization\n\nWithout the matchability classifier, the network loses its ability to discriminate between good and bad matches, as shown in Figure 6. Intuitively, the similarity matrix proposes many likely matches while the matchability filters incorrect proposals. Thus, our partial assignment can be viewed as an elegant fusion of mutual nearest neighbor search and a learned inlier classifier [44, 82]. This is significantly faster than solving the optimal transport problem of SuperGlue.\n\nSetup: We evaluate long-term visual localization in chal- lenging conditions using the large-scale Aachen Day-Night benchmark [59]. We follow the Hierarchical Localization framework with the hloc toolbox [55]. We first triangu- late a sparse 3D point cloud from the 4328 daytime ref- erence images, with known poses and calibration, using COLMAP [60]. For each of the 824 daytime and 98", "start_char_idx": 39332, "end_char_idx": 40276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f0613c8-76e7-40f9-83a3-e676b05ba8be": {"__data__": {"id_": "5f0613c8-76e7-40f9-83a3-e676b05ba8be", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "1a7b5a50-c21f-4b70-8cfc-e3a5d7b28483", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "59d28cccbe13a90de46fa51984c0805e19c59e98020e1d13bb2eca9aff36171f"}, "3": {"node_id": "58bb2cae-413d-4131-988e-33201961e8dd", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e5b3d8b9c8429a2de7b783ec8270f9184e267616e88e4775dfe95058fc13f58f"}}, "hash": "42d7616ac3c9d2556fe401b2d26b83617e3b225f4c33e926ecfb191fc5cd6b0e", "text": "calibration, using COLMAP [60]. For each of the 824 daytime and 98 night- time queries, we retrieve 50 images with NetVLAD [1], match each of them, and estimate a camera pose with RANSAC and a Perspective-n-Point solver. We report the pose recall at multiple thresholds and the average throughput of the matching step during both mapping and localization.\n\nReplacing learned absolute positional encoding with ro- tary embeddings improves the accuracy, with a minor penalty on run time from rotating queries and keys at each self- attention layer. Using relative positions, LightGlue learns to match geometric patterns across images. Reminding the net- work about positions at each layer improves the robustness\n\n7\n\ndifficulty\n\narchitecture\n\nprecision\n\nrecall\n\ntime (ms)\n\nmetric\n\naverage\n\neasy medium hard\n\nSuperGlue\n\n74.6\n\n90.5\n\n29.1\n\naverage index", "start_char_idx": 40277, "end_char_idx": 41125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "58bb2cae-413d-4131-988e-33201961e8dd": {"__data__": {"id_": "58bb2cae-413d-4131-988e-33201961e8dd", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "5f0613c8-76e7-40f9-83a3-e676b05ba8be", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "42d7616ac3c9d2556fe401b2d26b83617e3b225f4c33e926ecfb191fc5cd6b0e"}, "3": {"node_id": "cbd8b3af-0e1a-455c-b6d5-703c93ca6812", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5b3091ef6e6c6465346d77d07efd070a08d06fa9dafbcdd2fedc9968ecf528d4"}}, "hash": "e5b3d8b9c8429a2de7b783ec8270f9184e267616e88e4775dfe95058fc13f58f", "text": "index of stopping layer \u2193 4.7 ratio of unmatchable points (%) \u2191 19.8 speedup over non-adaptive \u2191 1.86\n\nLightGlue (full) \u00eb a) no matchability \u00eb b) absolute positions \u00eb c) full cross-attention \u00eb d) early layer (#5/9)\n\n86.8 67.4 84.2 86.6 78.1\n\n96.3 97.0 94.7 96.1 92.7\n\n19.4 18.9 18.7 22.8 11.9\n\n5.5 23.4 1.33\n\n6.9 27.9 1.16\n\n5.7 23.7 1.45\n\nTable 5. Impact of adaptive depth and width. Early stopping helps most on smaller scenes, where the network stops after just half the layers. On harder scenes, the network requires more layers to converge, but smaller view overlap between image pairs allows the network to more aggressively prune the width of the network. Overall, adaptive depth- and", "start_char_idx": 41176, "end_char_idx": 41866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cbd8b3af-0e1a-455c-b6d5-703c93ca6812": {"__data__": {"id_": "cbd8b3af-0e1a-455c-b6d5-703c93ca6812", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "58bb2cae-413d-4131-988e-33201961e8dd", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e5b3d8b9c8429a2de7b783ec8270f9184e267616e88e4775dfe95058fc13f58f"}, "3": {"node_id": "25bf4a5e-0bc6-4592-824f-5321d0f7d5b1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "738dc3fea6325fd16fae181514bb259f62a7b6c3aa067eb3e322dd27b7b9295b"}}, "hash": "5b3091ef6e6c6465346d77d07efd070a08d06fa9dafbcdd2fedc9968ecf528d4", "text": "to more aggressively prune the width of the network. Overall, adaptive depth- and width- pruning reduces the run time by 33% and is particularly effective on easy pairs.\n\nTable 4. Ablation study on synthetic homographies. a-b) Both matchability and positional encoding improve the accuracy without impact on the time. c) The bidirectional cross-attention is faster without drop of accuracy. d) Thanks to the deep supervision, early layers yield good predictions on pairs with low difficulty.\n\nNo matchability\n\nLightGlue-adaptive\n\nnumber of keypoints per imagerun time (ms)\n\nSGMNet\n\nLightGlue\n\n512102420484096102050100200\n\nSuperGlue\n\nWith matchability\n\nFigure 7. Run time vs number of keypoints. The full LightGlue model is 35% faster than SuperGlue and the adaptive depth and width make it even faster. SGMNet is comparably fast only for 4k keypoints and above but is much", "start_char_idx": 41803, "end_char_idx": 42675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "25bf4a5e-0bc6-4592-824f-5321d0f7d5b1": {"__data__": {"id_": "25bf4a5e-0bc6-4592-824f-5321d0f7d5b1", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "cbd8b3af-0e1a-455c-b6d5-703c93ca6812", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5b3091ef6e6c6465346d77d07efd070a08d06fa9dafbcdd2fedc9968ecf528d4"}, "3": {"node_id": "b9413688-685f-4087-a3e1-4d4d48ec9b74", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "27f9240336b78c5d2e16bfceaa3e3dcfb5cc09a130459ad83f9f5fb5909d39ec"}}, "hash": "738dc3fea6325fd16fae181514bb259f62a7b6c3aa067eb3e322dd27b7b9295b", "text": "SGMNet is comparably fast only for 4k keypoints and above but is much slower for standard input sizes.\n\nunmatchable points early and leaves them out of the inputs to subsequent layers, thus avoiding unnecessary computations.\n\nFigure 6. Benefit of the matchability. The matchability helps filter out outliers (red) that are visually similar, retaining only inlier correspondences (green).\n\nEfficiency: Figure 7 shows run times for different numbers of input keypoints. For up to 2K keypoints per image, which is a common setting for visual localization, LightGlue is faster than both SuperGlue [56] and SGMNet [8]. Adaptive pruning further reduces the run time for any input size.\n\nof the network, resulting in +2% precision.\n\nBidirectional cross-attention is equally accurate as stan- dard cross-attention, but saves 20% run time by only com- puting the similarity matrix once. Currently, the bottleneck is computing the", "start_char_idx": 42688, "end_char_idx": 43608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b9413688-685f-4087-a3e1-4d4d48ec9b74": {"__data__": {"id_": "b9413688-685f-4087-a3e1-4d4d48ec9b74", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "25bf4a5e-0bc6-4592-824f-5321d0f7d5b1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "738dc3fea6325fd16fae181514bb259f62a7b6c3aa067eb3e322dd27b7b9295b"}, "3": {"node_id": "d38b97a2-714a-4cd0-a997-d2a5b94622fa", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5d86bb6307e0c5352d38491f5ab10069182e72057b1c424dcffcf0fdf0e3ee98"}}, "hash": "27f9240336b78c5d2e16bfceaa3e3dcfb5cc09a130459ad83f9f5fb5909d39ec", "text": "only com- puting the similarity matrix once. Currently, the bottleneck is computing the softmax along two dimensions. With a dedicated bidirectional softmax kernel, plenty of redundant computations could be avoided.\n\n6. Conclusion\n\nThis paper introduces LightGlue, a deep neural network trained to match sparse local features across images. Build- ing on the success of SuperGlue, we combine the power of attention mechanisms with insights about the matching problem and with recent innovations in Transformer. We give this model the ability to introspect the confidence of its own predictions. This yields an elegant scheme that adapts the amount of computation to the difficulty of each image pair. Both its depth and width are adaptive: 1) the inference can stop at an early layer if all predictions are ready, and 2) points that are deemed not matchable are discarded early from further steps. The resulting model, LightGlue, is finally faster, more accurate, and easier to train than the", "start_char_idx": 43591, "end_char_idx": 44583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d38b97a2-714a-4cd0-a997-d2a5b94622fa": {"__data__": {"id_": "d38b97a2-714a-4cd0-a997-d2a5b94622fa", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "b9413688-685f-4087-a3e1-4d4d48ec9b74", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "27f9240336b78c5d2e16bfceaa3e3dcfb5cc09a130459ad83f9f5fb5909d39ec"}, "3": {"node_id": "30d68ce3-7d65-4e47-852a-fee9953253eb", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5b3c65f33719420cc9459d2040f16fda11a04c4a2ad83c2f32951a785e4a6419"}}, "hash": "5d86bb6307e0c5352d38491f5ab10069182e72057b1c424dcffcf0fdf0e3ee98", "text": "is finally faster, more accurate, and easier to train than the long-unrivaled SuperGlue. In summary, LightGlue is a drop-in replacement with only benefits. The code will be released publicly for the benefit of the community.\n\nUsing deep supervision, also intermediate layers have meaningful outputs. Already after 5 layers, the network can predict robust matches, achieving > 90% recall. In the fi- nal layers, the network focuses on rejecting outliers, thus improving the match precision.\n\nAdaptivity: By predicting matchability scores and confi- dences, we can adaptively reduce the computations during a forward-pass on a case-by-case basis. Table 5 studies the ef- fectiveness of the two pruning mechanisms \u2013 adaptive depth and width \u2013 on MegaDepth image pairs for different ranges of visual overlap. For easy samples, such as the successive frames of a video, the network quickly converges and exits after a few", "start_char_idx": 44607, "end_char_idx": 45523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30d68ce3-7d65-4e47-852a-fee9953253eb": {"__data__": {"id_": "30d68ce3-7d65-4e47-852a-fee9953253eb", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "d38b97a2-714a-4cd0-a997-d2a5b94622fa", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5d86bb6307e0c5352d38491f5ab10069182e72057b1c424dcffcf0fdf0e3ee98"}, "3": {"node_id": "1623a297-9d4f-42a8-9a55-693bc937f6a5", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "0bd2d587573ebe1d1e32b9c4bdfee943ec9bcd00fd278c8341fcde7d23c2a7bf"}}, "hash": "5b3c65f33719420cc9459d2040f16fda11a04c4a2ad83c2f32951a785e4a6419", "text": "successive frames of a video, the network quickly converges and exits after a few layers, resulting in a 1.86\u00d7 speedup. In cases of low visual overlap, e.g. loop closure, the network requires more layers to converge. It however rejects confident and\n\nAcknowledgments: We thank Mihai Dusmanu, R\u00b4emi Pau- trat, and Shaohui Liu for their helpful feedback.\n\n8\n\nPoint pruning\n\nMatchability\n\nMatches\n\nFigure 8. Visualization of adaptive depth and width. From top to bottom, we show three easy, medium and difficult image pairs. The left column shows how LightGlue reduces its width: it finds out early that some points (\u2022) are unmatchable (mostly by visual overlap) and discards non-repeatable points in later layers: \u2022 \u2192 \u2022 \u2192 \u2022. This is very effective on difficult pairs. LightGlue looks for matches only in the reduced search space (\u2022). The matchability scores", "start_char_idx": 45508, "end_char_idx": 46363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1623a297-9d4f-42a8-9a55-693bc937f6a5": {"__data__": {"id_": "1623a297-9d4f-42a8-9a55-693bc937f6a5", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "30d68ce3-7d65-4e47-852a-fee9953253eb", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5b3c65f33719420cc9459d2040f16fda11a04c4a2ad83c2f32951a785e4a6419"}, "3": {"node_id": "719fdc44-7038-4b3f-9522-2c8183d46807", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "dde661b775a21f74a1afa0c10942d57233ce1415e0fb35845c87cbb42b66fe2b"}}, "hash": "0bd2d587573ebe1d1e32b9c4bdfee943ec9bcd00fd278c8341fcde7d23c2a7bf", "text": "looks for matches only in the reduced search space (\u2022). The matchability scores (middle column, from non-matchable \u2022 to likely matchable \u2022), help find accurate correspondences and are almost binary. On the right we visualize predicted matches as epipolar in- or outliers. We report the run time and stopping layer for each pair. On easy samples, LightGlue stops after only 2-3 layers, running with close to 100 FPS.\n\n9\n\nSIFT+LightGlue\n\nSuperPoint+LightGlue\n\nDISK+LightGlue\n\nFigure 9. Comparison of features produced by LightGlue for different local features. We compare the outputs of SIFT+LightGlue (left), SuperPoint+LightGlue (middle) and DISK+LightGlue (right).\n\n10\n\nAppendix\n\nTask 1: Stereo Task 2: Multiview\n\nSfM features (2048 keypoints)\n\nPairs per", "start_char_idx": 46365, "end_char_idx": 47120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "719fdc44-7038-4b3f-9522-2c8183d46807": {"__data__": {"id_": "719fdc44-7038-4b3f-9522-2c8183d46807", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "1623a297-9d4f-42a8-9a55-693bc937f6a5", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "0bd2d587573ebe1d1e32b9c4bdfee943ec9bcd00fd278c8341fcde7d23c2a7bf"}, "3": {"node_id": "358581b8-6620-45e7-8042-8e52fe7a11a9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "d85c99f120be5e4bc762415c90515d897eefb77087962aae3806013ea211251e"}}, "hash": "dde661b775a21f74a1afa0c10942d57233ce1415e0fb35845c87cbb42b66fe2b", "text": "Multiview\n\nSfM features (2048 keypoints)\n\nPairs per second\n\nAUC@K\u25e6\n\nAUC@5\u25e6@N\n\nA. Image Matching Challenge\n\n5\u25e6\n\n10\u25e6\n\n5\n\n10\n\n25\n\nIn this section, we present results obtained on the Pho- toTourism dataset of the Image Matching Challenge 2020 (IMC) [26] in both stereo and multi-view tracks. The data is very similar to the MegaDepth [38] evaluation, exhibits similar statistics but different scenes. We follow the stan- dardized matching pipeline of IMC with the setup and hy- perparameters of SuperGlue [56]. We run the evaluation on the 3 validation scenes from the PhotoTourism dataset with LightGlue trained with two kinds of local features.\n\nSP+SuperGlue SP+LightGlue\n\n58.64 71.07 61.88 78.97 86.75 59.03 71.13 62.87", "start_char_idx": 47141, "end_char_idx": 47859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "358581b8-6620-45e7-8042-8e52fe7a11a9": {"__data__": {"id_": "358581b8-6620-45e7-8042-8e52fe7a11a9", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "719fdc44-7038-4b3f-9522-2c8183d46807", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "dde661b775a21f74a1afa0c10942d57233ce1415e0fb35845c87cbb42b66fe2b"}, "3": {"node_id": "a0e5b11e-e971-4d29-a7f0-f33cff4efa25", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "ecfe956906cccb7db92dd0b6508bc23df5d2f2a9e3aa52b2c8d4d07732e18392"}}, "hash": "d85c99f120be5e4bc762415c90515d897eefb77087962aae3806013ea211251e", "text": "61.88 78.97 86.75 59.03 71.13 62.87 79.36 86.98\n\n16.2 43.4\n\n196.7 44.5\n\nDISK+NN+ratio 57.76 68.73 59.91 78.95 87.54 DISK+LightGlue 67.02 77.82 67.91 80.58 88.35\n\nTable 6. Structure-from-Motion with the Image Matching Chal- lenge 2020. We evaluate the stereo track, at multiple error thresh- olds, and the multi-view track, for various numbers of images N . LightGlue yields better poses than SuperGlue on the multi-view track and significantly reduces the matching time. In combination with DISK, LightGlue improves over SuperPoint+SuperGlue and DISK+NN+ratio in both tracks by a large margin.\n\nSuperPoint: For SuperPoint+SuperGlue and Super- Point+LightGlue, we extract a maximum of 2048 keypoints and use DEGENSAC [11,", "start_char_idx": 47877, "end_char_idx": 48597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a0e5b11e-e971-4d29-a7f0-f33cff4efa25": {"__data__": {"id_": "a0e5b11e-e971-4d29-a7f0-f33cff4efa25", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "358581b8-6620-45e7-8042-8e52fe7a11a9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "d85c99f120be5e4bc762415c90515d897eefb77087962aae3806013ea211251e"}, "3": {"node_id": "637b6b93-f153-4944-a725-b29bbf9a662f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "78f9b140f997e9add640206a054d782080055d8a96ddfce301b7540000dc1edb"}}, "hash": "ecfe956906cccb7db92dd0b6508bc23df5d2f2a9e3aa52b2c8d4d07732e18392", "text": "we extract a maximum of 2048 keypoints and use DEGENSAC [11, 12, 43] with a threshold on the detection confidence of 1.1 in the stereo track (as suggested by SuperGlue). We do not perform any parameter tuning and reuse our model from the outdoor experiments with adaptive depth- and width, and use efficient self-attention [14] and mixed-precision during evaluation.\n\nTask 1: Stereo Task 2: Multiview\n\nAverage\n\nfeatures + matcher\n\nAUC 5\u25e6/ 10\u25e6\n\nAUC 5\u25e6/ 10\u25e6\n\nAUC 5\u25e6/ 10\u25e6\n\nSP+SGMNet SP+SuperGlue SP+LightGlue\n\n29.6 / 43.0 36.5 / 50.5 36.7 / 50.7\n\n60.2 / 71.6 63.3 / 73.8 63.6 / 74.4\n\n44.9 / 57.3 49.9 / 62.2 50.2 /", "start_char_idx": 48578, "end_char_idx": 49189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "637b6b93-f153-4944-a725-b29bbf9a662f": {"__data__": {"id_": "637b6b93-f153-4944-a725-b29bbf9a662f", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "a0e5b11e-e971-4d29-a7f0-f33cff4efa25", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "ecfe956906cccb7db92dd0b6508bc23df5d2f2a9e3aa52b2c8d4d07732e18392"}, "3": {"node_id": "c7416539-0cbd-42bc-9d7c-2fb129c412d9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "39828217e97230cb73d40116d5084ecb45e1c13831eb219e02ee8afead1c0411"}}, "hash": "78f9b140f997e9add640206a054d782080055d8a96ddfce301b7540000dc1edb", "text": "/ 57.3 49.9 / 62.2 50.2 / 62.6\n\nDISK+NN+ratio DISK+LightGlue\n\n36.3 / 48.5 43.1 / 56.6\n\n61.5 / 71.6 66.2 / 76.2\n\n48.9 / 60.1 54.7 / 66.4\n\nDISK: We also train LightGlue with DISK local fea- tures [73], a previous winner of the Image Matching Chal- lenge. We follow the same training setup as for SuperPoint. For evaluation, we follow the guidelines from the authors for the restricted keypoint scenario (max 2048 features per image) and use mutual nearest neighbor matching with a ratio test of 0.95 as a baseline. We again use DEGENSAC for relative pose estimation with a threshold of 0.75.\n\nDISK (8K) +NN+ratio* SP+SuperGlue* LoFTR-SfM DISK", "start_char_idx": 49221, "end_char_idx": 49861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7416539-0cbd-42bc-9d7c-2fb129c412d9": {"__data__": {"id_": "c7416539-0cbd-42bc-9d7c-2fb129c412d9", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "637b6b93-f153-4944-a725-b29bbf9a662f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "78f9b140f997e9add640206a054d782080055d8a96ddfce301b7540000dc1edb"}, "3": {"node_id": "1d347c22-2461-4a69-8245-aa2be7bf901a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7269c8b5e7de74ee14f0e54db1475ee057085c244f30038b5ad24e4605d37d28"}}, "hash": "39828217e97230cb73d40116d5084ecb45e1c13831eb219e02ee8afead1c0411", "text": "SP+SuperGlue* LoFTR-SfM DISK (8K)+LightGlue\n\n44.6 / 56.2 44.6 / 58.6 48.4 / 60.9 48.7 / 61.8\n\n65.0 / 74.4 66.8 / 77.1 66.4 / 76.1 68.9 / 78.2\n\n54.8 / 65.3 55.7 / 67.9 57.4 / 68.5 58.8 / 70.0\n\nTable 7. IMC 2021 \u2013 Phototourism. *DISK+NN and SP+SG use test-time augmentation while LightGlue does not. To compete with these tuned baselines, we just increase the number of keypoints, e.g. DISK (8K). LoFTR-SfM clusters dense matches with SuperPoint detections. LightGlue outperforms other sparse baselines both in the stereo and multiview task, and even surpasses tuned baselines from the public leaderboard by a large", "start_char_idx": 49855, "end_char_idx": 50468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1d347c22-2461-4a69-8245-aa2be7bf901a": {"__data__": {"id_": "1d347c22-2461-4a69-8245-aa2be7bf901a", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "c7416539-0cbd-42bc-9d7c-2fb129c412d9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "39828217e97230cb73d40116d5084ecb45e1c13831eb219e02ee8afead1c0411"}, "3": {"node_id": "61536b64-d213-4ede-90b6-128097fb380b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b548ac333243714a87d4609d837c529d8df0527ac615c5753f858ad7109d8aa5"}}, "hash": "7269c8b5e7de74ee14f0e54db1475ee057085c244f30038b5ad24e4605d37d28", "text": "task, and even surpasses tuned baselines from the public leaderboard by a large margin.\n\nResults: Table 6 reports the evaluation results. We also re- port the average matching speed over all 3 validation scenes. LightGlue is competitive with SuperGlue both in the stereo and multi-view track, while running 2.5\u00d7 faster. Most of these run time improvements are due to the adaptive-depth, which largely reduces the run time for easy image pairs.\n\napproaches with a fair margin.\n\nImage Matching Challenge 2023: We compete in the IMC 2023 [28], which evaluates end-to-end Structure- from-Motion in terms of camera pose accuracy, averaged over multiple thresholds, with a diverse set of scenes beyond phototourism. We use the default recontruction pipeline of hloc [55] and retrieve 50 pairs per image using NetVLAD [1]. We average the results over 3 runs to reduce the impact of randomness in the reconstruction pipeline. On the public /", "start_char_idx": 50428, "end_char_idx": 51361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "61536b64-d213-4ede-90b6-128097fb380b": {"__data__": {"id_": "61536b64-d213-4ede-90b6-128097fb380b", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "1d347c22-2461-4a69-8245-aa2be7bf901a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7269c8b5e7de74ee14f0e54db1475ee057085c244f30038b5ad24e4605d37d28"}, "3": {"node_id": "7d656dc9-e1cd-4dba-abdb-9bbd1862b21c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "c9fd8d6158b1e2d17ad2cc391eb756d5c7aee6025dfdb3e3181a7468cd52e16e"}}, "hash": "b548ac333243714a87d4609d837c529d8df0527ac615c5753f858ad7109d8aa5", "text": "reduce the impact of randomness in the reconstruction pipeline. On the public / private leaderboards, respectively, Super- Point+SuperGlue achieves a score of 36.1 / 43.8 (%), while SuperPoint+LightGlue reaches 38.4 / 46.1, which is a +2.3% improvement.\n\nLightGlue trained with DISK [73] largely outperforms both the nearest-neighbor matching baseline with ratio test but also SuperPoint+LightGlue. On the smaller thresholds, DISK+LightGlue achieves +8%/+5% AUC in the stereo and multi-view tasks compared to our SuperPoint equivalent. With DISK, our model predicts 30% more matches than SP+LightGlue with an even higher epipolar precision.\n\nImage Matching Challenge 2021: We evaluate the photo- tourism subset of the IMC 2021 [27] benchmark, both in the stereo- and multiview track. We compare our baseline on Su-", "start_char_idx": 51362, "end_char_idx": 52176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7d656dc9-e1cd-4dba-abdb-9bbd1862b21c": {"__data__": {"id_": "7d656dc9-e1cd-4dba-abdb-9bbd1862b21c", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "61536b64-d213-4ede-90b6-128097fb380b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b548ac333243714a87d4609d837c529d8df0527ac615c5753f858ad7109d8aa5"}, "3": {"node_id": "3d3e0ddf-acde-4444-aa24-511ce8050c22", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b9bfeeae72305f7d347b250f2d3fd3eb12ba9b1f001c4a1c2f13415f2bd3603a"}}, "hash": "c9fd8d6158b1e2d17ad2cc391eb756d5c7aee6025dfdb3e3181a7468cd52e16e", "text": "in the stereo- and multiview track. We compare our baseline on Su- perPoint [16] and DISK [73] with their respective baselines in a clean setting and in a restricted keypoint setting (max 2048 detections). Furthermore, we compare our best scoring method on IMC 2020, DISK+LightGlue, with tuned ver- sions of DISK [73], SuperPoint+SuperGlue [16, 56] as well as the SfM implementation of the dense matcher LoFTR [68]. Table 7 reports the experiment. LightGlue outperforms all\n\nB. Additional results\n\nRelative pose estimation:\n\nResults reported in Section 5.2 were computed with a sub- set of the MegaDepth dataset [38] as introduced by previous\n\n11\n\npose estimation AUC time (ms) @5\u25e6 @10\u25e6 @20\u25e6\n\nDay\n\nNight\n\nfeatures + matcher\n\npairs per second\n\nfeatures +", "start_char_idx": 52189, "end_char_idx": 52942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3d3e0ddf-acde-4444-aa24-511ce8050c22": {"__data__": {"id_": "3d3e0ddf-acde-4444-aa24-511ce8050c22", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "7d656dc9-e1cd-4dba-abdb-9bbd1862b21c", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "c9fd8d6158b1e2d17ad2cc391eb756d5c7aee6025dfdb3e3181a7468cd52e16e"}, "3": {"node_id": "8fd5a7d8-ea80-4b04-99fb-f39a48c7c9b3", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9fee71c674992af7f9825fb3e204ec09e3a27d3894ad92180102f1eee9962815"}}, "hash": "b9bfeeae72305f7d347b250f2d3fd3eb12ba9b1f001c4a1c2f13415f2bd3603a", "text": "+ matcher\n\npairs per second\n\nfeatures + matcher\n\n#matches\n\nP\n\n(0.25m,2\u00b0) / (0.5m,5\u00b0) / (1.0m,10\u00b0)\n\n78.5 / 90.6 / 99.0 77.5 / 91.6 / 99.5\n\ne LoFTR s n e d\n\n2231 2416 4299\n\n89.8 91.2 94.7\n\n66.4 65.2 68.0\n\n79.1 78.1 80.4\n\n87.6 87.4 88.7\n\n181 388 239\n\nLoFTR ASpanFormer\n\n88.7 / 95.6 / 99.0 89.4 / 95.6 / 99.0\n\n-\n\nMatchFormer ASPanFormer\n\n89.8 / 96.1 / 99.4 90.2 / 96.0 / 99.4\n\n77.0 / 90.6 / 100 77.0 / 91.1 / 100\n\nSP+SuperGlue", "start_char_idx": 52963, "end_char_idx": 53385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8fd5a7d8-ea80-4b04-99fb-f39a48c7c9b3": {"__data__": {"id_": "8fd5a7d8-ea80-4b04-99fb-f39a48c7c9b3", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "3d3e0ddf-acde-4444-aa24-511ce8050c22", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b9bfeeae72305f7d347b250f2d3fd3eb12ba9b1f001c4a1c2f13415f2bd3603a"}, "3": {"node_id": "c7f873bf-ddfb-4c71-bc1f-e7cf3026c452", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8a25bd54e04a39f86ee25b1d4370c65204c09266d400f9c29fa22af618c20f90"}}, "hash": "9fee71c674992af7f9825fb3e204ec09e3a27d3894ad92180102f1eee9962815", "text": "/ 100 77.0 / 91.1 / 100\n\nSP+SuperGlue SP+LightGlue\n\n6.4 17.3\n\nT NN+ratio SGMNet LightGlue\n\n160 405 383\n\n82.3 82.5 84.1\n\n48.3 50.7 57.0\n\n62.2 66.6 71.3\n\n73.2 76.5 81.8\n\n5.7 71.7 44.3\n\nF I S\n\nTable 9. Outdoor visual localization on Aachen v1.1. LightGlue achieves similar accuracy with higher throughput.\n\nt NN+mutual n i o SuperGlue P r SGMNet e p u LightGlue S\n\n697 712 725 709\n\n49.4 93.0 89.8 94.5\n\n37.7 64.8 61.7 65.5\n\n50.9 77.5 74.3 77.8\n\n62.3 86.6 83.4 86.9\n\n5.6 70.0 74.0", "start_char_idx": 53390, "end_char_idx": 53866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7f873bf-ddfb-4c71-bc1f-e7cf3026c452": {"__data__": {"id_": "c7f873bf-ddfb-4c71-bc1f-e7cf3026c452", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "8fd5a7d8-ea80-4b04-99fb-f39a48c7c9b3", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "9fee71c674992af7f9825fb3e204ec09e3a27d3894ad92180102f1eee9962815"}, "3": {"node_id": "0c71ba38-076d-4d2c-8a40-b50c3f85b5c1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "26295d881f81243e88e20586a4b76819eb036de60d3755c98c08a7ddf1d4be59"}}, "hash": "8a25bd54e04a39f86ee25b1d4370c65204c09266d400f9c29fa22af618c20f90", "text": "86.6 83.4 86.9\n\n5.6 70.0 74.0 44.2\n\nDUC1\n\nDUC2\n\nfeatures + matcher\n\n(0.25m,10\u00b0) / (0.5m,10\u00b0) / (1.0m,10\u00b0)\n\nTable 8. Relative pose estimation on Megadepth-1800. This split is different from Table 2. In contrast to the split used by previous works [38, 68], this set of test images avoids training overlap with SuperGlue [56]. LightGlue predicts a similar amount of correspon- dences but with higher precision (P), pose accuracy (AUC), and speed than existing sparse matchers. It is competitive with dense matchers for a fraction of the inference time.\n\n54.2 / 74.8 / 85.5 55.7 / 71.8 / 81.7 55.0 / 74.0 / 81.7\n\nLoFTR MatchFormer ASpanFormer\n\n47.5 / 72.2 /", "start_char_idx": 53873, "end_char_idx": 54527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c71ba38-076d-4d2c-8a40-b50c3f85b5c1": {"__data__": {"id_": "0c71ba38-076d-4d2c-8a40-b50c3f85b5c1", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "c7f873bf-ddfb-4c71-bc1f-e7cf3026c452", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8a25bd54e04a39f86ee25b1d4370c65204c09266d400f9c29fa22af618c20f90"}, "3": {"node_id": "cef44be1-ee48-4ff0-8872-08d74530e3c1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a0e7c05f36bb7bb53182228d50da0c145ae38a6e0ecf2c78d83a03467545a97b"}}, "hash": "26295d881f81243e88e20586a4b76819eb036de60d3755c98c08a7ddf1d4be59", "text": "MatchFormer ASpanFormer\n\n47.5 / 72.2 / 84.8 46.5 / 73.2 / 85.9 51.5 / 73.7 / 86.4\n\n53.4 / 77.1 / 80.9 55.0 / 74.8 / 79.4\n\nSP+SuperGlue SP+LightGlue\n\n47.0 / 69.2 / 79.8 49.0 / 68.2 / 79.3\n\nTable 10. Indoor visual localization on InLoc. LightGlue performs similarly to SuperGlue (within the variability of the dataset).\n\nworks [9, 68, 78]. However, the images therein overlap with the training set of SuperGlue [56], the state-of-the-art sparse feature matcher and thus our main competitor.\n\nFor a more fair evaluation, we perform an extensive out- door experiment on the test scenes of our MegaDepth [38] split, which covers 4 unique phototourism landmarks that SuperGlue was not trained with: Sagrada", "start_char_idx": 54519, "end_char_idx": 55219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cef44be1-ee48-4ff0-8872-08d74530e3c1": {"__data__": {"id_": "cef44be1-ee48-4ff0-8872-08d74530e3c1", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "0c71ba38-076d-4d2c-8a40-b50c3f85b5c1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "26295d881f81243e88e20586a4b76819eb036de60d3755c98c08a7ddf1d4be59"}, "3": {"node_id": "5e521b21-0ead-494e-8690-68b4cc7096b9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "59259d1f377ee1970baa30a18899d95743f3172b4de2665b78b365cd949befb0"}}, "hash": "a0e7c05f36bb7bb53182228d50da0c145ae38a6e0ecf2c78d83a03467545a97b", "text": "unique phototourism landmarks that SuperGlue was not trained with: Sagrada Familia, Lincoln Memorial Statue, London Castle, and the British Museum. To balance the difficulty of image pairs, we bin pairs into three categories based on their visual overlap score [19, 56], with intervals [10, 30]%, [30, 50]%, and [50, 70]%. We sam- ple 150 image pairs per bin per scene, totaling 1800 image pairs. We carefully rerun the experiment with the same setup that was used in Table 2. We report the precision as the ratio of matches with an epipolar error below 3px. With SIFT [41], we evaluate the ratio test and SGMNet [8] only, as the original SuperGlue model is not publicly available.\n\nFigure 10. Failure cases on InLoc [70]. LightGlue sometimes matches repeated objects in the scene with strong texture, instead of the geometric structure.\n\nTable 8 confirms that LightGlue predicts more accurate correspondences", "start_char_idx": 55189, "end_char_idx": 56098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5e521b21-0ead-494e-8690-68b4cc7096b9": {"__data__": {"id_": "5e521b21-0ead-494e-8690-68b4cc7096b9", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "cef44be1-ee48-4ff0-8872-08d74530e3c1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a0e7c05f36bb7bb53182228d50da0c145ae38a6e0ecf2c78d83a03467545a97b"}, "3": {"node_id": "9666c334-2245-4f6a-8a07-6c02c944702f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "23fe0526d86ffb37837c0087f8724b40f455cf774d5ff3dd05f88d6bfe693727"}}, "hash": "59259d1f377ee1970baa30a18899d95743f3172b4de2665b78b365cd949befb0", "text": "8 confirms that LightGlue predicts more accurate correspondences than existing sparse matchers, at a fraction of the time. Detector-free feature matchers like LoFTR re- main state-of-the-art on this task, although by a mere 2% AUC@5\u00b0 with LO-RANSAC.\n\ninsignificant because each split only has 205/151 queries (1.5% of difference \u2261 3 queries). Failures of LightGlue over SuperGlue (6/356 images @1m) are due to more matches on repeated objects (like trash cans), i.e. to better matching and weak retrieval \u2013 we show an example in Figure 10.\n\nOutdoor visual localization: For completeness, we also report results on the Aachen v1.1 dataset [59] and compare our method to recent sparse and dense baselines. Table 9 shows that all methods perform similarly on this dataset, which is largely saturated, with insignificant variations in the results.", "start_char_idx": 56107, "end_char_idx": 56950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9666c334-2245-4f6a-8a07-6c02c944702f": {"__data__": {"id_": "9666c334-2245-4f6a-8a07-6c02c944702f", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "5e521b21-0ead-494e-8690-68b4cc7096b9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "59259d1f377ee1970baa30a18899d95743f3172b4de2665b78b365cd949befb0"}, "3": {"node_id": "3ea99bc3-8f16-41ae-9ba1-18c87cbd849b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "158356b5ca7c5947eb6bcfbc0f953dd63372fe8dc279c6cb5da7d43faf6124d5"}}, "hash": "23fe0526d86ffb37837c0087f8724b40f455cf774d5ff3dd05f88d6bfe693727", "text": "which is largely saturated, with insignificant variations in the results. LightGlue is however far faster than all approaches.\n\nC. Implementation details\n\nC.1. Architecture\n\nIndoor visual localization: We report results for InLoc in Table 10. We use hloc and run SuperGlue again for fairness. For LoFTR and ASpanFormer, report existing results as no code is available. LightGlue is competitive with SuperGlue and more accurate at (0.25m,10\u00b0). Differences of <2% are\n\nPositional Encoding. 2D image coordinates are normalized to a range [-1, 1] while retaining the image aspect ratio. We then project 2D coordinates into frequencies with a linear projection Wp \u2208 R2d/2h, where h is the number of attention\n\n12\n\nheads. We cache the result for all layers. We follow the efficient scheme of Roformer [67] to apply the rotations to query and key embeddings during self-attention,", "start_char_idx": 56944, "end_char_idx": 57817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3ea99bc3-8f16-41ae-9ba1-18c87cbd849b": {"__data__": {"id_": "3ea99bc3-8f16-41ae-9ba1-18c87cbd849b", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "9666c334-2245-4f6a-8a07-6c02c944702f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "23fe0526d86ffb37837c0087f8724b40f455cf774d5ff3dd05f88d6bfe693727"}, "3": {"node_id": "cf094076-a52c-4980-83bb-f2d5a25b643f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "cd491caa5c65a5785e9f1499ae46afc114cc400297dbce4ab4d829b5b2b53fe3"}}, "hash": "158356b5ca7c5947eb6bcfbc0f953dd63372fe8dc279c6cb5da7d43faf6124d5", "text": "to apply the rotations to query and key embeddings during self-attention, avoiding quadratic complexity to compute relative positional bias. We do not apply any positional encoding during cross-attention, but let the network learn spatial patterns by aggregating context within each image.\n\npose estimation AUC\n\ntime (%)\n\nMethod\n\n#matches\n\nP\n\n@5\u25e6 @10\u25e6 @20\u25e6\n\nSP+LightGlue \u00eb layer 7/9 \u00eb layer 5/9 \u00eb layer 3/9\n\n613 705 702 687\n\n96.2 96.0 94.5 90.0\n\n66.7 66.2 65.0 64.0\n\n79.3 79.1 77.8 76.7\n\n87.9 88.0 87.0 85.8\n\n100.0 82.4 60.0 41.9\n\n\u00eb confidence 98% \u00eb confidence 95% \u00eb confidence 90% \u00eb confidence 80%\n\n610 608 607 605\n\n96.2 95.4 94.5", "start_char_idx": 57819, "end_char_idx": 58450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cf094076-a52c-4980-83bb-f2d5a25b643f": {"__data__": {"id_": "cf094076-a52c-4980-83bb-f2d5a25b643f", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "3ea99bc3-8f16-41ae-9ba1-18c87cbd849b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "158356b5ca7c5947eb6bcfbc0f953dd63372fe8dc279c6cb5da7d43faf6124d5"}, "3": {"node_id": "f925dced-f69c-449f-9c37-629d941e110b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a1d5e7d715eb8c17afdedb745c79d3063e00bf7ce107582c709dce51d093c218"}}, "hash": "cd491caa5c65a5785e9f1499ae46afc114cc400297dbce4ab4d829b5b2b53fe3", "text": "confidence 80%\n\n610 608 607 605\n\n96.2 95.4 94.5 92.6\n\n66.6 66.3 65.9 65.2\n\n79.3 79.0 78.5 77.8\n\n88.0 87.9 87.2 86.7\n\n80.5 70.6 61.5 48.4\n\nGraph Neural Network: The graph neural network consists of 9 transformer layers with both a self- and cross-attention unit. The update MLP (Eq. 1) has a single hidden layer of di- mension dh = 2d followed by LayerNorm, GeLU activation and a linear projection (2d, d) with bias.\n\nTable 11. Evaluation of early-stopping on MegaDepth. Matches predicted by deeper layers are more accurate but require more computations with a higher inference time. Modeling confidences adaptively selects the model depth that yields a sufficient accuracy. A more conservative stopping, with a higher threshold \u03b1, yields a higher accuracy at", "start_char_idx": 58473, "end_char_idx": 59231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f925dced-f69c-449f-9c37-629d941e110b": {"__data__": {"id_": "f925dced-f69c-449f-9c37-629d941e110b", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "cf094076-a52c-4980-83bb-f2d5a25b643f", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "cd491caa5c65a5785e9f1499ae46afc114cc400297dbce4ab4d829b5b2b53fe3"}, "3": {"node_id": "d0adb495-064a-4f39-86fa-1e904b74dd2e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5c28b2161a65271d41d382a48ba14b16e4e51146523d1c5435ae4d5cc35cf000"}}, "hash": "a1d5e7d715eb8c17afdedb745c79d3063e00bf7ce107582c709dce51d093c218", "text": "more conservative stopping, with a higher threshold \u03b1, yields a higher accuracy at the cost of higher inference time. \u03b1=95% yields the best trade-off.\n\nEach attention unit has three projection matrices for query, key and value, plus an additional linear projection that merges the multi-head output. In bidirectional cross atten- tion, the projections for query and key are shared. In practice we use an efficient self-attention [14] which optimizes IO complexity of the attention aggregation. This could also be extended for bidirectional cross attention. While training we use gradient checkpointing to significantly reduce the required VRAM.\n\n1234567801020304050\n\nDetected negatives (%) after n layerslayer\n\nCorrespondences: The linear layers (Eq. 6) map from d to d and are not shared across layers. For all experiments we use the mutual check and a filter threshold \u03c4 = 0.1.\n\nConfidence classifier: The classifier predicts the confidence with a", "start_char_idx": 59203, "end_char_idx": 60152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d0adb495-064a-4f39-86fa-1e904b74dd2e": {"__data__": {"id_": "d0adb495-064a-4f39-86fa-1e904b74dd2e", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "f925dced-f69c-449f-9c37-629d941e110b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a1d5e7d715eb8c17afdedb745c79d3063e00bf7ce107582c709dce51d093c218"}, "3": {"node_id": "c1188a9c-8ab9-4c44-81cc-916ccd3342d8", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a82161531733c56f40e33a58238084fab0a9932de05a32a3fcd5e28b9b48ec21"}}, "hash": "5c28b2161a65271d41d382a48ba14b16e4e51146523d1c5435ae4d5cc35cf000", "text": "0.1.\n\nConfidence classifier: The classifier predicts the confidence with a linear layer followed by a sigmoid activation. Con- fidences are predicted for each keypoint and only at layers 1, .., L \u2212 1, since, by definition, the confidences of the final layer L are 1. Each prediction is supervised with a binary cross-entropy loss and its gradients are not propagated into the states to avoid impacting the matching accuracy. The state already encodes sufficient information since it is also supervised for matchability prediction.\n\nFigure 11. Continuous detection of unmatchable points. After just a few layers the network detects many points which are un- matchable, and we exclude them from context aggregation.\n\naccuracy tradeoff than trimming the model to fewer layers. Stopping the network early mainly sacrifices precision. For our experiments we chose 95% confidence, which yields on average 25% run time reduction with hardly any loss of accuracy on downstream", "start_char_idx": 60157, "end_char_idx": 61125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1188a9c-8ab9-4c44-81cc-916ccd3342d8": {"__data__": {"id_": "c1188a9c-8ab9-4c44-81cc-916ccd3342d8", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "d0adb495-064a-4f39-86fa-1e904b74dd2e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5c28b2161a65271d41d382a48ba14b16e4e51146523d1c5435ae4d5cc35cf000"}, "3": {"node_id": "1d792d69-87d6-424a-a1e4-df9b63ec7fe1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6d8b0961bdf928b53d020a5a2a3e41692e8cb91dac5414c235f69ce1048593f9"}}, "hash": "a82161531733c56f40e33a58238084fab0a9932de05a32a3fcd5e28b9b48ec21", "text": "on average 25% run time reduction with hardly any loss of accuracy on downstream tasks.\n\nHere, \u03b2 = 0.01 is a threshold on how matchable a point is. If Eq. 13 holds, we exclude the point from context ag- gregation in the following layers. This adds an overhead of gather and scatter per layer, but pruning becomes increas- ingly effective with more keypoints.\n\nExit criterion and point pruning: During training we ob- served that the confidence predictions are less accurate in earlier layers. We therefore exponentially decay the confi- dence threshold:\n\n\u03bbl = 0.8 + 0.1e\u22124\u2113/L .\n\n(12)\n\nIn Figure 11 we report the fraction of keypoints excluded in each layer. After just a few layers of context aggrega- tion, LightGlue is confident to exclude > 30% of keypoints early on. Since the number of keypoints have a quadratic impact on run time, as shown in", "start_char_idx": 61125, "end_char_idx": 61974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1d792d69-87d6-424a-a1e4-df9b63ec7fe1": {"__data__": {"id_": "1d792d69-87d6-424a-a1e4-df9b63ec7fe1", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "c1188a9c-8ab9-4c44-81cc-916ccd3342d8", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a82161531733c56f40e33a58238084fab0a9932de05a32a3fcd5e28b9b48ec21"}, "3": {"node_id": "51ef6659-d4b2-4344-ba0f-5fd65487a9b4", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "3d3715f3577528a2c5a7cb57e9e98b1f6accfd7dd2fec9cbe76a8fa1d29d6b18"}}, "hash": "6d8b0961bdf928b53d020a5a2a3e41692e8cb91dac5414c235f69ce1048593f9", "text": "Since the number of keypoints have a quadratic impact on run time, as shown in Fig. 7, this can largely re- duce the number of computations in a forward pass and thus significantly reduce inference time.\n\nA state is deemed confident if c\u2113 i > \u03bb\u2113. During inference, we halt the network if \u03b1=95% of states are deemed confident. For point pruning, a point is deemed unmatchable when its predicted confidence is high and its matchability is low:\n\nunmatchable(i) = cl\n\ni > \u03bb\u2113 & \u03c3\u2113\n\n(13)\n\ni < \u03b2\n\nC.2. Local features\n\nWe report an ablation on the exit confidence \u03b1 in Table 11 for relative pose estimation on MegaDepth. Lowering \u03b1 to 80% reduces the inference time by almost 50% compared to our full model, while maintaining competitive accuracy compared to SuperGlue on this task. Reducing the confi- dence threshold is far more effective in terms of", "start_char_idx": 61978, "end_char_idx": 62822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "51ef6659-d4b2-4344-ba0f-5fd65487a9b4": {"__data__": {"id_": "51ef6659-d4b2-4344-ba0f-5fd65487a9b4", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "1d792d69-87d6-424a-a1e4-df9b63ec7fe1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6d8b0961bdf928b53d020a5a2a3e41692e8cb91dac5414c235f69ce1048593f9"}, "3": {"node_id": "9abbd0e8-de67-45fa-a73c-f7e31dbe2ef9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "d050e091f08993235db786f71052994c72c8418c632adc8c483f3b6bed1f1c2c"}}, "hash": "3d3715f3577528a2c5a7cb57e9e98b1f6accfd7dd2fec9cbe76a8fa1d29d6b18", "text": "this task. Reducing the confi- dence threshold is far more effective in terms of run time -\n\nWe train LightGlue with three popular local feature de- tectors and descriptors: SuperPoint [16], SIFT [41] and DISK [73]. During training and evaluation, we discard the detection threshold for all methods and use the top-k key- points according to the detection score. During training, if\n\n13\n\nthere are less than k detections available, we append random detections and descriptors. For SIFT [41] and DISK [73], we add a linear layer to project descriptors to d=256 before feeding them to the Transformer backbone.\n\nSuperPoint: SuperPoint is a popular feature detector which produces highly repeatable points located at distinctive re- gions. We use the official, open-sourced version of Su- perPoint from MagicLeap [16]. The detections are pixel- accurate, i.e. the keypoint localization accuracy depends on the image", "start_char_idx": 62820, "end_char_idx": 63732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9abbd0e8-de67-45fa-a73c-f7e31dbe2ef9": {"__data__": {"id_": "9abbd0e8-de67-45fa-a73c-f7e31dbe2ef9", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "51ef6659-d4b2-4344-ba0f-5fd65487a9b4", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "3d3715f3577528a2c5a7cb57e9e98b1f6accfd7dd2fec9cbe76a8fa1d29d6b18"}, "3": {"node_id": "ff3bb99f-c4f1-4a41-b5d1-30dc85973069", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "09b522b6bde4537a4f23b4549d69cbe20acf293b65a5bcc88a90564665dd89a5"}}, "hash": "d050e091f08993235db786f71052994c72c8418c632adc8c483f3b6bed1f1c2c", "text": "accurate, i.e. the keypoint localization accuracy depends on the image resolution.\n\nFigure 12. Examples of synthetic homographies. We show the original images (left) and two augmented examples (center and right) resulting from strong perspective transformations and ex- treme photometric augmentations.\n\nSIFT: We use the excellent implementation of SIFT from vlfeat [75] when training on MegaDepth, and SIFTGPU from COLMAP [60] for fast feature extraction when pre- training on homographies. We observed that these imple- mentations are largely equivalent during training and can be exchanged freely. Also, SIFT features from OpenCV can be used without retraining. Orientation and scale are not used in positional encoding.\n\nTraining details: We extract 512/1024/1024 keypoints for SuperPoint/SIFT/DISK, and a batch size of 64. The initial learning rate is 0.0001, and we multiply the learning rate by 0.8 each epoch after 20 epochs. We stop", "start_char_idx": 63739, "end_char_idx": 64680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ff3bb99f-c4f1-4a41-b5d1-30dc85973069": {"__data__": {"id_": "ff3bb99f-c4f1-4a41-b5d1-30dc85973069", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "9abbd0e8-de67-45fa-a73c-f7e31dbe2ef9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "d050e091f08993235db786f71052994c72c8418c632adc8c483f3b6bed1f1c2c"}, "3": {"node_id": "7e2bcfc5-a822-45f2-905a-26ddcf11f482", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e1b9887b2fb75cad8c3da7a2abde6550cb10d291fed8fa65f097d4fafbe3daa3"}}, "hash": "09b522b6bde4537a4f23b4549d69cbe20acf293b65a5bcc88a90564665dd89a5", "text": "the learning rate by 0.8 each epoch after 20 epochs. We stop the training after 40 epochs (6M image pairs), or 2 days with 2 Nvidia RTX 3090 (for SuperPoint). Our network achieves > 99% recall and > 90% precision on the validation and test set. We also observed that, for fine-tuning, one can stop the pre-training after just one day with only minor losses.\n\nDISK: DISK learns detection and description with a rein- forcement learning objective. Its descriptors are more pow- erful than SIFT and SuperPoint and its detections are more repeatable, especially under large viewpoint and illumination changes.\n\nWe also experimented with sampling images from MegaDepth [38] for homography pre-training, and could not observe major differences. Strong photometric augmen- tations and perspective changes are crucial for training a robust model.\n\nC.3. Homography pre-training\n\nFollowing Sarlin et al. [56], we", "start_char_idx": 64693, "end_char_idx": 65595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7e2bcfc5-a822-45f2-905a-26ddcf11f482": {"__data__": {"id_": "7e2bcfc5-a822-45f2-905a-26ddcf11f482", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "ff3bb99f-c4f1-4a41-b5d1-30dc85973069", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "09b522b6bde4537a4f23b4549d69cbe20acf293b65a5bcc88a90564665dd89a5"}, "3": {"node_id": "9058e6b6-d1e1-46d5-9105-02ce96404880", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5bc1d4386451113e4b96b323fd1b7e78eaddafcfed6dd39bf7a319d47b8c9631"}}, "hash": "e1b9887b2fb75cad8c3da7a2abde6550cb10d291fed8fa65f097d4fafbe3daa3", "text": "Homography pre-training\n\nFollowing Sarlin et al. [56], we first pre-train LightGlue\n\nC.4. Finetuning on MegaDepth\n\non synthetic homographies of real-images.\n\nWe fine-tune our model on phototourism images with\n\nDataset: We use 170k images from the Oxford-Paris 1M distractors dataset [50], and split them into 150k/10k/10k images for training/validation/test.\n\npseudo ground-truth camera poses and depth images.\n\nDataset: We use the MegaDepth dataset [38], which contains dense reconstructions of a large variety of pop- ular landmarks all around the globe, obtained through COLMAP+MVS [60, 61]. Following Sun et al. [68], we bin each pair by its covisibility score [19], into ranges [0.1, 0.3], [0.3, 0.5] and [0.5, 0.7]. Scenes which are part of the", "start_char_idx": 65594, "end_char_idx": 66344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9058e6b6-d1e1-46d5-9105-02ce96404880": {"__data__": {"id_": "9058e6b6-d1e1-46d5-9105-02ce96404880", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "7e2bcfc5-a822-45f2-905a-26ddcf11f482", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e1b9887b2fb75cad8c3da7a2abde6550cb10d291fed8fa65f097d4fafbe3daa3"}, "3": {"node_id": "1693798f-391b-4324-a5bb-8869f6e40ec9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "3cca8815b82ae6bec7d5fe34472aa773f3052d07a14adf63c0d84f7f054009b0"}}, "hash": "5bc1d4386451113e4b96b323fd1b7e78eaddafcfed6dd39bf7a319d47b8c9631", "text": "and [0.5, 0.7]. Scenes which are part of the valida- tion and test set in the image matching challenge [26] are also excluded from training, resulting in 368/5/24 scenes for training/validation/test. At the beginning of each epoch, we sample 100 image pairs per scene.\n\nHomography sampling: We generate homographies by randomly sampling four image corners. We split the image into four quarters, and sample a random point in each quarter. To avoid degenerates, we enforce that the enclosed area is convex. After, we apply random rotations and translations to the corners s.t. the corners remain inside the image. With this process, we can generate extreme perspective changes while avoiding border artifacts. This process is repeated twice, resulting in two largely skewed homographies. In interpolation, we then enforce the extracted images to be of size 640x480.\n\nImages are resized s.t. their larger edge is of size 1024,\n\nand", "start_char_idx": 66360, "end_char_idx": 67289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1693798f-391b-4324-a5bb-8869f6e40ec9": {"__data__": {"id_": "1693798f-391b-4324-a5bb-8869f6e40ec9", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "9058e6b6-d1e1-46d5-9105-02ce96404880", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5bc1d4386451113e4b96b323fd1b7e78eaddafcfed6dd39bf7a319d47b8c9631"}, "3": {"node_id": "8d618149-8d33-496e-9607-dba2df1cae92", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "52c2b428f6e54adc38eccb5f2236e459c4f4dcf835b2bb4451841c30442ead2b"}}, "hash": "3cca8815b82ae6bec7d5fe34472aa773f3052d07a14adf63c0d84f7f054009b0", "text": "are resized s.t. their larger edge is of size 1024,\n\nand zero-pad images to 1024\u00d71024 resolution.\n\nSupervision: Following SuperGlue [56], we reproject points using camera poses and depth to the other image. Correspon- dences with a maximum reprojection error of 3 pixels and which are mutually closest are labelled as inliers. A point where the closest correspondence has a reprojection error larger than 5px are is labelled as outlier. Furthermore, we also declare points without depth and no correspondence with a Sampson Error smaller than 3 px outliers.\n\nPhotometric augmentation: The color images are then forwarded through a sequence of strong photometric aug- mentations, including blur, hue, saturation, sharpness, illu- mination, gamma and noise. Furthermore, we add random additive shades into the image to simulate occlusions and non-uniform illumination changes.\n\nSupervision: Correspondences with 3px symmetric repro- jection error", "start_char_idx": 67279, "end_char_idx": 68223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8d618149-8d33-496e-9607-dba2df1cae92": {"__data__": {"id_": "8d618149-8d33-496e-9607-dba2df1cae92", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "1693798f-391b-4324-a5bb-8869f6e40ec9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "3cca8815b82ae6bec7d5fe34472aa773f3052d07a14adf63c0d84f7f054009b0"}, "3": {"node_id": "8b904a93-070e-4a67-b9e1-9c8890ec0ccc", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e3853e94a816560d6dfc43a2427bb8b9e5af913514c9510ab882da043fe9fbf0"}}, "hash": "52c2b428f6e54adc38eccb5f2236e459c4f4dcf835b2bb4451841c30442ead2b", "text": "Correspondences with 3px symmetric repro- jection error are deemed inliers, and points without any cor- respondence under this threshold are outliers.\n\nTraining details: Weights are initialized from the pre- trained model on homographies, Training starts with a learn-\n\n14\n\nRun time (ms)\n\nselfcrossassignment05101520\n\nLightGlue\n\nSuperGlue\n\ning rate of 1e-5 and we exponentially decay it by 0.95 in each epoch after 10 epochs, and stop training after 50 epochs (2 days on 2 RTX 3090). The top 2048 keypoints are extracted per image, and we use a batch size of 32. To speed-up train- ing, we cache detections and descriptors per image, requiring around 200 GB of disk space.\n\nFigure 13. Run time breakdown. We evluate the runtime of self-, cross- and partial assignment layers on 1024 keypoints for Super- Glue and LightGlue. Most of LightGlue\u2019s default", "start_char_idx": 68222, "end_char_idx": 69073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8b904a93-070e-4a67-b9e1-9c8890ec0ccc": {"__data__": {"id_": "8b904a93-070e-4a67-b9e1-9c8890ec0ccc", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "8d618149-8d33-496e-9607-dba2df1cae92", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "52c2b428f6e54adc38eccb5f2236e459c4f4dcf835b2bb4451841c30442ead2b"}, "3": {"node_id": "3cbac886-bcaa-41a2-8e3a-88ecdf72785a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "fec7cfa610d57909305e101ea354cb4085f110f0fb56ed456c9fdcee16bdcb13"}}, "hash": "e3853e94a816560d6dfc43a2427bb8b9e5af913514c9510ab882da043fe9fbf0", "text": "for Super- Glue and LightGlue. Most of LightGlue\u2019s default inference time improvements stem from a significantly faster partial assignment layer and reuse of computations in bidirectional cross-attention.\n\nC.5. Homography estimation\n\nWe validate the models capabilities on real homographies on the Hpatches dataset [2]. We follow the setup introduced in LoFTR [68] and resize images to a maximum edge length of 480.\n\nOur cheap double-softmax and the unary matchability predictions are significantly faster than solving it using op- timal transport [66, 48], where 100 iterations are required during training to maintain stability.\n\nFor SuperPoint we extract the top 1024 keypoints with the highest detection score, and report precision (fraction of matches within 3px homography error) and recall (frac- tion of recovered mutual nearest-neighbour matches within 3px homography error). For LoFTR we only report epipolar precision. Furthermore, we evaluate the", "start_char_idx": 69073, "end_char_idx": 70031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3cbac886-bcaa-41a2-8e3a-88ecdf72785a": {"__data__": {"id_": "3cbac886-bcaa-41a2-8e3a-88ecdf72785a", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "8b904a93-070e-4a67-b9e1-9c8890ec0ccc", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e3853e94a816560d6dfc43a2427bb8b9e5af913514c9510ab882da043fe9fbf0"}, "3": {"node_id": "588afb2f-817f-4a6a-b61c-abb4a30e7712", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "35328a0b741f83e2029738c8ea8a0102a95ed1def557adb58f105dcedea9951d"}}, "hash": "fec7cfa610d57909305e101ea354cb4085f110f0fb56ed456c9fdcee16bdcb13", "text": "error). For LoFTR we only report epipolar precision. Furthermore, we evaluate the models in the down- stream task of homography matrix estimation. Following SuperGlue [56], we report pose estimation results from ro- bust estimation using RANSAC/MAGSAC [3] and the least squares solution with the weighted DLT algorithm. We eval- uate the accuracy of estimated homography by their mean absolute corner distance towards the ground-truth homogra- phy.\n\nIn practice, we also use efficient self-attention [14] and mixed-precision to significantly reduce run time and memory requirements. However, for a fair comparison, we exclude these performance improvements from all experiments ex- cept where explicitly stated otherwise.\n\nE. Qualitative Results\n\nFigure 8 shows how LightGlue discards unmatched points and its early stopping mechanism on easy/medium/hard pairs. Figure 9 illustrates the matching output for LightGlue with SIFT [41], SuperPoint [16] and", "start_char_idx": 70012, "end_char_idx": 70964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "588afb2f-817f-4a6a-b61c-abb4a30e7712": {"__data__": {"id_": "588afb2f-817f-4a6a-b61c-abb4a30e7712", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "3cbac886-bcaa-41a2-8e3a-88ecdf72785a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "fec7cfa610d57909305e101ea354cb4085f110f0fb56ed456c9fdcee16bdcb13"}, "3": {"node_id": "09eeb4db-79c8-40b0-b222-85a06d9b8f07", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "70af66c509fdf843628412519017fa0862cd63957006be42ed1009b578f2c071"}}, "hash": "35328a0b741f83e2029738c8ea8a0102a95ed1def557adb58f105dcedea9951d", "text": "the matching output for LightGlue with SIFT [41], SuperPoint [16] and DISK [73] on some qualita- tive examples.\n\nWe use OpenCV with USAC MAGSAC for robust ho- mography estimation, and tune the threshold for each method separately. Our reasoning behind this decision, which is in contrast to previous works in feature matching [56, 68] which fix the RANSAC parameters, is that we mainly use RANSAC as a tool to evaluate the low-level matches on a downstream task, and we want to minimize the varia- tions introduced by its hyperparameters in order to obtain fair and representative evaluations. Different matches typi- cally require different RANSAC thresholds, and thus a fixed threshold is suboptimal for comparison. For example on out- door relative pose estimation, tuning the RANSAC threshold yields +7% AUC@5\u25e6on SuperGlue, skewing the reported", "start_char_idx": 70976, "end_char_idx": 71824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "09eeb4db-79c8-40b0-b222-85a06d9b8f07": {"__data__": {"id_": "09eeb4db-79c8-40b0-b222-85a06d9b8f07", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "588afb2f-817f-4a6a-b61c-abb4a30e7712", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "35328a0b741f83e2029738c8ea8a0102a95ed1def557adb58f105dcedea9951d"}, "3": {"node_id": "4dc467e5-ed33-4e3a-bfb9-8376a49133b7", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "dcb41f90159fb16b5c899854d73c1307a060db46bc254ba2927ea93b9f0cfc25"}}, "hash": "70af66c509fdf843628412519017fa0862cd63957006be42ed1009b578f2c071", "text": "+7% AUC@5\u25e6on SuperGlue, skewing the reported numbers.\n\nReferences\n\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. NetVLAD: CNN architecture for weakly supervised place recognition. In CVPR, 2016. 7, 11\n\n[2] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys- tian Mikolajczyk. Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors. In CVPR, 2017. 6, 15\n\n[3] Daniel Barath, Jiri Matas, and Jana Noskova. Magsac: marginalizing sample consensus. In CVPR, 2019. 6, 15 [4] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. SURF:\n\nSpeeded up robust features. In ECCV, 2006.", "start_char_idx": 71845, "end_char_idx": 72481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4dc467e5-ed33-4e3a-bfb9-8376a49133b7": {"__data__": {"id_": "4dc467e5-ed33-4e3a-bfb9-8376a49133b7", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "09eeb4db-79c8-40b0-b222-85a06d9b8f07", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "70af66c509fdf843628412519017fa0862cd63957006be42ed1009b578f2c071"}, "3": {"node_id": "68a17f27-2e13-4c99-93c0-16bb371d914e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b87a6046d2bd7b5177e33d44475501f34fd9f75d6b5d45c119d05fcacda9380e"}}, "hash": "dcb41f90159fb16b5c899854d73c1307a060db46bc254ba2927ea93b9f0cfc25", "text": "SURF:\n\nSpeeded up robust features. In ECCV, 2006. 2\n\n[5] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jos\u00b4e Neira, Ian Reid, and John J Leonard. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. TRO, 32(6):1309\u2013 1332, 2016. 2\n\nD. Timings\n\nAll experiments were conducted on a single RTX 3080 with 10GB VRAM. We report the timings of the matching process only, excluding sparse feature extraction (which is linear in the number of images) and robust pose estimation. We report the average over the respective datasets.\n\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing", "start_char_idx": 72478, "end_char_idx": 73203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "68a17f27-2e13-4c99-93c0-16bb371d914e": {"__data__": {"id_": "68a17f27-2e13-4c99-93c0-16bb371d914e", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "4dc467e5-ed33-4e3a-bfb9-8376a49133b7", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "dcb41f90159fb16b5c899854d73c1307a060db46bc254ba2927ea93b9f0cfc25"}, "3": {"node_id": "366863b6-673d-4f6d-9dec-abff2d07ed26", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e9103660e8e1667ffdac2aaa9e1d5245cc80a896d37b736d09c77cfc0d7a7bef"}}, "hash": "b87a6046d2bd7b5177e33d44475501f34fd9f75d6b5d45c119d05fcacda9380e", "text": "Piotr Bojanowski, and Armand Joulin. Emerg- ing Properties in Self-Supervised Vision Transformers. In ICCV, 2021. 1\n\nIn Figure 13 we benchmark self-/cross-attention and solv- ing the partial assignment problem against the respective counterparts in SuperGlue [56]. Bidirectional cross-attention reduces the run-time by 33% by only computing the simi- larity matrix once. However, the main bottleneck remains computing the softmax over both directions.\n\n[7] Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten Sattler, and Marc Pollefeys. Handcrafted outlier detection revisited. In ECCV, 2020. 2\n\n[8] Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to\n\n15\n\nmatch features with seeded graph", "start_char_idx": 73206, "end_char_idx": 73962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "366863b6-673d-4f6d-9dec-abff2d07ed26": {"__data__": {"id_": "366863b6-673d-4f6d-9dec-abff2d07ed26", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "68a17f27-2e13-4c99-93c0-16bb371d914e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b87a6046d2bd7b5177e33d44475501f34fd9f75d6b5d45c119d05fcacda9380e"}, "3": {"node_id": "62da66d8-fa5a-4e6c-811f-82f66cc9adb1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5d9025ef55cc10e0638ccc6017ff0da1662309592e0f0ce65058a7098a87be4a"}}, "hash": "e9103660e8e1667ffdac2aaa9e1d5245cc80a896d37b736d09c77cfc0d7a7bef", "text": "Tai, and Long Quan. Learning to\n\n15\n\nmatch features with seeded graph matching network. ICCV, 2021. 1, 2, 5, 6, 7, 8, 12\n\n[20] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.\n\nDepth-Adaptive Transformer. In ICLR, 2020. 2, 4\n\n[9] Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, Mingmin Zhen, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer. In ECCV, 2022. 2, 6, 12 [10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training Deep Nets with Sublinear Memory Cost. arXiv:1604.06174, 2016. 6\n\n[21] Michael Figurnov, Maxwell D Collins, Yukun Zhu,", "start_char_idx": 73944, "end_char_idx": 74587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "62da66d8-fa5a-4e6c-811f-82f66cc9adb1": {"__data__": {"id_": "62da66d8-fa5a-4e6c-811f-82f66cc9adb1", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "366863b6-673d-4f6d-9dec-abff2d07ed26", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e9103660e8e1667ffdac2aaa9e1d5245cc80a896d37b736d09c77cfc0d7a7bef"}, "3": {"node_id": "d421e349-dd36-413e-b132-703b86887d7b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a81aba0e5c3079c0a3132cda85e1e5e2433519b734cefe02c11b6004fdb733ed"}}, "hash": "5d9025ef55cc10e0638ccc6017ff0da1662309592e0f0ce65058a7098a87be4a", "text": "Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, and Ruslan Salakhutdinov. Spatially Adaptive Computation Time for Residual Networks. In CVPR, 2017. 3\n\n[22] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381\u2013395, 1981. 2, 6\n\n[11] Ond\u02c7rej Chum, Ji\u02c7r\u00b4\u0131 Matas, and Josef Kittler. Locally optimized RANSAC. In Joint Pattern Recognition Symposium, pages 236\u2013243. Springer, 2003. 11\n\n[23] Christopher G Harris, Mike Stephens, et al. A combined corner and edge detector. In Alvey vision conference, 1988. 2 [24] Richard Hartley and Andrew Zisserman. Multiple view geom- etry in computer vision.", "start_char_idx": 74607, "end_char_idx": 75376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d421e349-dd36-413e-b132-703b86887d7b": {"__data__": {"id_": "d421e349-dd36-413e-b132-703b86887d7b", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "62da66d8-fa5a-4e6c-811f-82f66cc9adb1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "5d9025ef55cc10e0638ccc6017ff0da1662309592e0f0ce65058a7098a87be4a"}, "3": {"node_id": "00b870a3-a4d2-4d20-a770-ff54321633e6", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "474dc0aec1a069022d6a63bdfc8117054721bcd265db9e9fc55e114608437522"}}, "hash": "a81aba0e5c3079c0a3132cda85e1e5e2433519b734cefe02c11b6004fdb733ed", "text": "Richard Hartley and Andrew Zisserman. Multiple view geom- etry in computer vision. Cambridge university press, 2003. 6\n\n[12] Ondrej Chum, Tomas Werner, and Jiri Matas. Two-view In geometry estimation unaffected by a dominant plane. CVPR, 2005. 11\n\n[13] Aaron Daniel Cohen, Adam Roberts, Alejandra Molina, Alena Butryna, Alicia Jin, Apoorv Kulshreshtha, Ben Hutchin- son, Ben Zevenbergen, Blaise Hilary Aguera-Arcas, Chung ching Chang, Claire Cui, Cosmo Du, Daniel De Freitas Adi- wardana, Dehao Chen, Dmitry (Dima) Lepikhin, Ed H. Chi, Erin Hoffman-John, Heng-Tze Cheng, Hongrae Lee, Igor Kri- vokon, James Qin, Jamie Hall, Joe Fenton, Johnny Soraker, Kathy Meier-Hellstern,", "start_char_idx": 75347, "end_char_idx": 76021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "00b870a3-a4d2-4d20-a770-ff54321633e6": {"__data__": {"id_": "00b870a3-a4d2-4d20-a770-ff54321633e6", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "d421e349-dd36-413e-b132-703b86887d7b", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a81aba0e5c3079c0a3132cda85e1e5e2433519b734cefe02c11b6004fdb733ed"}, "3": {"node_id": "6537c040-bd70-464d-8c4f-1138f1000fa9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a49275ba8dfc3d7d48af153dcdb732d7d09c9e90c50ccabdd8ead025bf580c0c"}}, "hash": "474dc0aec1a069022d6a63bdfc8117054721bcd265db9e9fc55e114608437522", "text": "Hall, Joe Fenton, Johnny Soraker, Kathy Meier-Hellstern, Kristen Olson, Lora Mois Aroyo, Maarten Paul Bosma, Marc Joseph Pickett, Marcelo Amorim Menegali, Marian Croak, Mark D\u00b4\u0131az, Matthew Lamm, Maxim Krikun, Meredith Ringel Morris, Noam Shazeer, Quoc V. Le, Rachel Bernstein, Ravi Rajakumar, Ray Kurzweil, Romal Thoppilan, Steven Zheng, Taylor Bos, Toju Duke, Tulsee Doshi, Vincent Y. Zhao, Vinodkumar Prabhakaran, Will Rusch, YaGuang Li, Yanping Huang, Yanqi Zhou, Yuanzhong Xu, and Zhifeng Chen. LaMDA: Language Models for Dialog Applications. arXiv:2201.08239, 2022. 1\n\n[25] Jared Heinly, Johannes L Schonberger, Enrique Dunn, and", "start_char_idx": 76043, "end_char_idx": 76677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6537c040-bd70-464d-8c4f-1138f1000fa9": {"__data__": {"id_": "6537c040-bd70-464d-8c4f-1138f1000fa9", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "00b870a3-a4d2-4d20-a770-ff54321633e6", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "474dc0aec1a069022d6a63bdfc8117054721bcd265db9e9fc55e114608437522"}, "3": {"node_id": "7fa02926-68cd-40fe-9152-781d08ab1183", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e0c9035255c5d9c86842790b63175863d8dac33bd112eec6e656069bd0f48d96"}}, "hash": "a49275ba8dfc3d7d48af153dcdb732d7d09c9e90c50ccabdd8ead025bf580c0c", "text": "Heinly, Johannes L Schonberger, Enrique Dunn, and Jan-Michael Frahm. Reconstructing the World* in Six Days *(as Captured by the Yahoo 100 Million Image Dataset). In CVPR, 2015. 2\n\n[26] CVPR\n\n2020\n\nImage\n\nMatching\n\nChallenge.\n\nhttps://www.cs.ubc.ca/research/ image-matching-challenge/2020/. June 15, 2023. 11, 14\n\nAccessed\n\n[27] CVPR\n\n2021\n\nImage\n\nMatching\n\nChallenge.\n\nhttps://www.cs.ubc.ca/research/ image-matching-challenge/. 15, 2023. 11\n\nAccessed June\n\n[28] CVPR\n\nImage https://www.kaggle.com/competitions/ image-matching-challenge-2023/overview. Accessed June 15, 2023. 11\n\n2023\n\nMatching\n\nChallenge.\n\n[29] Andrew", "start_char_idx": 76685, "end_char_idx": 77303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7fa02926-68cd-40fe-9152-781d08ab1183": {"__data__": {"id_": "7fa02926-68cd-40fe-9152-781d08ab1183", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "6537c040-bd70-464d-8c4f-1138f1000fa9", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "a49275ba8dfc3d7d48af153dcdb732d7d09c9e90c50ccabdd8ead025bf580c0c"}, "3": {"node_id": "405158b9-9b8d-4624-9de0-5e1c55bf07d3", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6c97927c6b6fd8604f8ad6b21008474046c0dd8b146febe58943ae96463091fb"}}, "hash": "e0c9035255c5d9c86842790b63175863d8dac33bd112eec6e656069bd0f48d96", "text": "11\n\n2023\n\nMatching\n\nChallenge.\n\n[29] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: A general architec- ture for structured inputs & outputs. In ICLR, 2022. 1 [30] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis- serman, Oriol Vinyals, and Jo\u02dcao Carreira. Perceiver: General Perception with Iterative Attention. In ICML, 2021. 2 [31] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, and Eduard", "start_char_idx": 77305, "end_char_idx": 77957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "405158b9-9b8d-4624-9de0-5e1c55bf07d3": {"__data__": {"id_": "405158b9-9b8d-4624-9de0-5e1c55bf07d3", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "7fa02926-68cd-40fe-9152-781d08ab1183", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e0c9035255c5d9c86842790b63175863d8dac33bd112eec6e656069bd0f48d96"}, "3": {"node_id": "79736362-4040-4c34-8c9a-a73211271402", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "f36d945f1b9a9502b68c115974c5bfaecea9abae0c9dff6f3da5907124caf473"}}, "hash": "6c97927c6b6fd8604f8ad6b21008474046c0dd8b146febe58943ae96463091fb", "text": "Matas, Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image Match- ing across Wide Baselines: From Paper to Practice. IJCV, 2020. 6\n\n[14] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. 2, 6, 7, 11, 13, 15\n\n[15] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal Transformers. In ICLR, 2019. 2\n\n[16] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. SuperPoint: Self-supervised interest point detection and description. In CVPR Workshop on Deep Learning for Visual SLAM, 2018. 2, 6, 11, 13, 14, 15\n\n[17] Jacob Devlin,", "start_char_idx": 77964, "end_char_idx": 78650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "79736362-4040-4c34-8c9a-a73211271402": {"__data__": {"id_": "79736362-4040-4c34-8c9a-a73211271402", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "405158b9-9b8d-4624-9de0-5e1c55bf07d3", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "6c97927c6b6fd8604f8ad6b21008474046c0dd8b146febe58943ae96463091fb"}, "3": {"node_id": "87ec85ba-f93f-4b3e-a58b-19bded79bf60", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "19b5d2f26b1565725d5f9d0dfb13e0fa1abce0b27bfee83da28a69f3de13d1a9"}}, "hash": "f36d945f1b9a9502b68c115974c5bfaecea9abae0c9dff6f3da5907124caf473", "text": "2, 6, 11, 13, 14, 15\n\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, 2019. 1, 3\n\n[32] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Trans- formers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings of the International Confer- ence on Machine Learning (ICML), 2020. 2\n\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image", "start_char_idx": 78654, "end_char_idx": 79352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "87ec85ba-f93f-4b3e-a58b-19bded79bf60": {"__data__": {"id_": "87ec85ba-f93f-4b3e-a58b-19bded79bf60", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "79736362-4040-4c34-8c9a-a73211271402", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "f36d945f1b9a9502b68c115974c5bfaecea9abae0c9dff6f3da5907124caf473"}, "3": {"node_id": "958656c5-73d4-4f7e-890c-74aeb1e141f8", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "4a225b9fbdaf4e850e6f1ffe03044a56399b7c17336c012b22e2051ac4962f0f"}}, "hash": "19b5d2f26b1565725d5f9d0dfb13e0fa1abce0b27bfee83da28a69f3de13d1a9", "text": "and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR, 2021. 1\n\n[33] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re- former: The Efficient Transformer. In ICLR, 2020. 2 [34] Viktor Larsson. PoseLib - Minimal Solvers for Camera Pose\n\nEstimation, 2020. 6, 7\n\n[19] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle- feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net: A trainable CNN for joint detection and description of local features. In CVPR, 2019. 2, 12, 14\n\n[35] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se- ungjin Choi, and Yee Whye Teh. Set Transformer: A Frame- work for", "start_char_idx": 79326, "end_char_idx": 79984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "958656c5-73d4-4f7e-890c-74aeb1e141f8": {"__data__": {"id_": "958656c5-73d4-4f7e-890c-74aeb1e141f8", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "87ec85ba-f93f-4b3e-a58b-19bded79bf60", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "19b5d2f26b1565725d5f9d0dfb13e0fa1abce0b27bfee83da28a69f3de13d1a9"}, "3": {"node_id": "045b94f1-1612-4002-9276-c6236c6f87d7", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "c6e6b3191a5e174e6d56bb5018c8775efed5c0c7ca5363494354b38655af3ee7"}}, "hash": "4a225b9fbdaf4e850e6f1ffe03044a56399b7c17336c012b22e2051ac4962f0f", "text": "Choi, and Yee Whye Teh. Set Transformer: A Frame- work for Attention-based Permutation-Invariant Neural Net- works. In ICML, 2019. 2\n\n16\n\n[36] Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, and Xiaoou Tang. Not all pixels are equal: Difficulty-aware se- mantic segmentation via deep layer cascade. In CVPR, 2017. 3\n\n[55] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 2019. 1, 7, 11\n\n[56] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning feature match- ing with graph neural networks. In CVPR, 2020. 1, 2, 6, 8, 11, 12, 14,", "start_char_idx": 79997, "end_char_idx": 80678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "045b94f1-1612-4002-9276-c6236c6f87d7": {"__data__": {"id_": "045b94f1-1612-4002-9276-c6236c6f87d7", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "958656c5-73d4-4f7e-890c-74aeb1e141f8", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "4a225b9fbdaf4e850e6f1ffe03044a56399b7c17336c012b22e2051ac4962f0f"}, "3": {"node_id": "8db43dbe-6391-4294-a2ed-c6cfe42236b1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "c2c0b567353976c3ba97c21c9237978e4852b4408d27b197f57ecc9a423b83c9"}}, "hash": "c6e6b3191a5e174e6d56bb5018c8775efed5c0c7ca5363494354b38655af3ee7", "text": "In CVPR, 2020. 1, 2, 6, 8, 11, 12, 14, 15\n\n[37] Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio. Learnable Fourier Features for Multi-dimensional Spatial Positional Encoding. In NeurIPS, 2021. 3\n\n[38] Zhengqi Li and Noah Snavely. MegaDepth: Learning single- view depth prediction from internet photos. In CVPR, 2018. 5, 6, 11, 12, 14\n\n[57] Paul-Edouard Sarlin, Mihai Dusmanu,\n\nJohannes L. Sch\u00a8onberger, Pablo Speciale, Lukas Gruber, Viktor Larsson, Ondrej Miksik, and Marc Pollefeys. LaMAR: Benchmarking Localization and Mapping for Augmented Reality. In ECCV, 2022. 1, 2\n\n[39] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor", "start_char_idx": 80698, "end_char_idx": 81333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8db43dbe-6391-4294-a2ed-c6cfe42236b1": {"__data__": {"id_": "8db43dbe-6391-4294-a2ed-c6cfe42236b1", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "045b94f1-1612-4002-9276-c6236c6f87d7", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "c6e6b3191a5e174e6d56bb5018c8775efed5c0c7ca5363494354b38655af3ee7"}, "3": {"node_id": "92d5c780-a559-437e-aefe-2fb3702c45d1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "28a5c308d3fa2011bb5e8039532c21ccd5ee9f3ac0824eb2677987f57c6d940d"}}, "hash": "c2c0b567353976c3ba97c21c9237978e4852b4408d27b197f57ecc9a423b83c9", "text": "Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-Perfect Structure-from-Motion with Featuremetric Refinement. In ICCV, 2021. 2\n\n[58] Paul-Edouard Sarlin, Ajaykumar Unagar, M\u02daans Larsson, Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, and Torsten Sattler. Back to the Feature: Learning robust camera localization from pixels to pose. In CVPR, 2021. 1\n\n[40] Zhuang Liu, Zhiqiu Xu, Hung-Ju Wang, Trevor Darrell, and Evan Shelhamer. Anytime Dense Prediction with Confidence Adaptivity. In ICLR, 2022. 3, 4\n\n[41] David G Lowe. Distinctive image features from scale- IJCV,", "start_char_idx": 81318, "end_char_idx": 81980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "92d5c780-a559-437e-aefe-2fb3702c45d1": {"__data__": {"id_": "92d5c780-a559-437e-aefe-2fb3702c45d1", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "8db43dbe-6391-4294-a2ed-c6cfe42236b1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "c2c0b567353976c3ba97c21c9237978e4852b4408d27b197f57ecc9a423b83c9"}, "3": {"node_id": "556dbab9-5a90-48c0-926d-20eeffb941f3", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b4b2668affc8d6f4d077da9c65611fb970aa9917567f7e45bcc45dbf633c731b"}}, "hash": "28a5c308d3fa2011bb5e8039532c21ccd5ee9f3ac0824eb2677987f57c6d940d", "text": "David G Lowe. Distinctive image features from scale- IJCV, 60(2):91\u2013110, 2004. 2, 6, 12,\n\ninvariant keypoints. 13, 14, 15\n\n[59] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Oku- tomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas Pajdla. Benchmarking 6DOF outdoor visual localization in changing conditions. In CVPR, 2018. 1, 7, 12\n\n[42] Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Working hard to know your neighbor\u2019s margins: Local descriptor learning loss. In NeurIPS, 2017. 2\n\n[43] Dmytro Mishkin, Jiri Matas, and Michal Perdoch. Mods: Fast and robust method for", "start_char_idx": 81976, "end_char_idx": 82646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "556dbab9-5a90-48c0-926d-20eeffb941f3": {"__data__": {"id_": "556dbab9-5a90-48c0-926d-20eeffb941f3", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "92d5c780-a559-437e-aefe-2fb3702c45d1", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "28a5c308d3fa2011bb5e8039532c21ccd5ee9f3ac0824eb2677987f57c6d940d"}, "3": {"node_id": "42ee3e86-a475-4eaf-b526-55f7920b4013", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "17fc789e6fcb50b92e0887ff207c15280a1531f759789268f4a89f05d11236c8"}}, "hash": "b4b2668affc8d6f4d077da9c65611fb970aa9917567f7e45bcc45dbf633c731b", "text": "Jiri Matas, and Michal Perdoch. Mods: Fast and robust method for two-view matching. Computer Vision and Image Understanding, 2015. 11\n\n[60] Johannes Lutz Sch\u00a8onberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 2, 7, 14\n\n[61] Johannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstruc- tured multi-view stereo. In ECCV, 2016. 14\n\n[44] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu Salzmann, and Pascal Fua. Learning to find good correspondences. In CVPR, 2018. 2, 7\n\n[62] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay,", "start_char_idx": 82643, "end_char_idx": 83308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "42ee3e86-a475-4eaf-b526-55f7920b4013": {"__data__": {"id_": "42ee3e86-a475-4eaf-b526-55f7920b4013", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "556dbab9-5a90-48c0-926d-20eeffb941f3", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b4b2668affc8d6f4d077da9c65611fb970aa9917567f7e45bcc45dbf633c731b"}, "3": {"node_id": "a7b678be-36d6-4517-be75-b47a6ea5615e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "79bba5e82f34c2a90cceee35c0c08e0b82882ea05bec12e6c94aea4af66a4631"}}, "hash": "17fc789e6fcb50b92e0887ff207c15280a1531f759789268f4a89f05d11236c8", "text": "Dara Bahri, Vinh Q. Tran, Yi Tay, and Donald Metzler. Confident Adaptive Language Modeling. In NeurIPS, 2022. 2, 4 [63] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self- Attention with Relative Position Representations. In NAACL- HTL, 2018. 3\n\n[45] Ra\u00b4ul Mur-Artal, J. M. M. Montiel, and Juan D. Tard\u00b4os. ORB- SLAM: a versatile and accurate monocular SLAM system. TRO, 31(5):1147\u20131163, 2015. 2\n\n[46] R\u00b4emi Pautrat, Viktor Larsson, Martin R. Oswald, and Marc Pollefeys. Online invariance selection for local feature de- scriptors. In ECCV, 2020. 2\n\n[64] Tianwei Shen, Zixin Luo, Lei Zhou, Runze Zhang, Siyu Zhu, Tian", "start_char_idx": 83336, "end_char_idx": 83956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a7b678be-36d6-4517-be75-b47a6ea5615e": {"__data__": {"id_": "a7b678be-36d6-4517-be75-b47a6ea5615e", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "42ee3e86-a475-4eaf-b526-55f7920b4013", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "17fc789e6fcb50b92e0887ff207c15280a1531f759789268f4a89f05d11236c8"}, "3": {"node_id": "fb4c9479-22d2-47eb-b6a3-6bd0d76d7702", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8bdc263cf7e2bfb0a9302373a521dd100347ab6d87f13a4c1de2c4bcbe3cb610"}}, "hash": "79bba5e82f34c2a90cceee35c0c08e0b82882ea05bec12e6c94aea4af66a4631", "text": "Lei Zhou, Runze Zhang, Siyu Zhu, Tian Fang, and Long Quan. Matchable image retrieval by learning from surface reconstruction. In ACCV, 2018. 6 [65] Yan Shi, Jun-Xiong Cai, Yoli Shavit, Tai-Jiang Mu, Wensen Feng, and Kai Zhang. ClusterGNN: Cluster-based coarse-to- fine graph neural network for efficient feature matching. In CVPR, 2022. 1, 2, 5, 7\n\n[47] Malte Pedersen, Joakim Bruslund Haurum, Thomas B Moes- lund, and Marianne Nyegaard. Re-identification of giant sunfish using keypoint matching. In Northern Lights Deep Learning Workshop, 2022. 1\n\n[48] Gabriel Peyr\u00b4e and Marco Cuturi. Computational optimal transport. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019. 2, 5,", "start_char_idx": 83953, "end_char_idx": 84647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fb4c9479-22d2-47eb-b6a3-6bd0d76d7702": {"__data__": {"id_": "fb4c9479-22d2-47eb-b6a3-6bd0d76d7702", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "a7b678be-36d6-4517-be75-b47a6ea5615e", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "79bba5e82f34c2a90cceee35c0c08e0b82882ea05bec12e6c94aea4af66a4631"}, "3": {"node_id": "383d7d6b-7777-4601-8256-c0eacbc2c8ad", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7fc3bfbc4be2dcb02368ebb53687b07a3c02cc34f0029e9e0c286ca369b439bb"}}, "hash": "8bdc263cf7e2bfb0a9302373a521dd100347ab6d87f13a4c1de2c4bcbe3cb610", "text": "in Machine Learning, 11(5-6):355\u2013607, 2019. 2, 5, 15\n\n[66] Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 1967. 5, 15\n\n[49] Markus N. Rabe and Charles Staats. Self-attention Does Not\n\nNeed O(n2) Memory. arXiv:2112.05682, 2021. 2\n\n[50] Filip Radenovi\u00b4c, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ond\u02c7rej Chum. Revisiting Oxford and Paris: Large-scale image retrieval benchmarking. In CVPR, 2018. 5, 14\n\n[67] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864, 2021. 3,", "start_char_idx": 84636, "end_char_idx": 85281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "383d7d6b-7777-4601-8256-c0eacbc2c8ad": {"__data__": {"id_": "383d7d6b-7777-4601-8256-c0eacbc2c8ad", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "fb4c9479-22d2-47eb-b6a3-6bd0d76d7702", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "8bdc263cf7e2bfb0a9302373a521dd100347ab6d87f13a4c1de2c4bcbe3cb610"}, "3": {"node_id": "86f88c87-3188-492d-945d-5d1d161276e4", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "22eedbc49257f6fd966f56c0c8a4c70b3666d832aea2280fe0b53d69092f6fc1"}}, "hash": "7fc3bfbc4be2dcb02368ebb53687b07a3c02cc34f0029e9e0c286ca369b439bb", "text": "Position Embedding. arXiv:2104.09864, 2021. 3, 4, 13\n\n[68] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with Transformers. CVPR, 2021. 2, 6, 11, 12, 14, 15 [69] Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou. OnePose: One-shot object pose estimation without CAD mod- els. In CVPR, 2022. 1\n\n[51] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. 1, 3 [52] Jerome Revaud, Philippe Weinzaepfel, C\u00b4esar De Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, and Martin", "start_char_idx": 85283, "end_char_idx": 85912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "86f88c87-3188-492d-945d-5d1d161276e4": {"__data__": {"id_": "86f88c87-3188-492d-945d-5d1d161276e4", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "383d7d6b-7777-4601-8256-c0eacbc2c8ad", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7fc3bfbc4be2dcb02368ebb53687b07a3c02cc34f0029e9e0c286ca369b439bb"}, "3": {"node_id": "076a55c4-0d46-4d69-bab7-1b8e89a02590", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e831f6a99e62036d5207c372b5f5b5d343be8f05d70416f4b638076f13e07388"}}, "hash": "22eedbc49257f6fd966f56c0c8a4c70b3666d832aea2280fe0b53d69092f6fc1", "text": "Pion, Gabriela Csurka, Yohann Cabon, and Martin Humen- berger. R2D2: Repeatable and reliable detector and descriptor. In NeurIPS, 2019. 2\n\n[53] Edward Rosten and Tom Drummond. Machine learning for\n\nhigh-speed corner detection. In ECCV, 2006. 2\n\n[70] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak- ihiko Torii. InLoc: Indoor Visual Localization with Dense Matching and View Synthesis. TPAMI, 2019. 12\n\n[54] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R Bradski. ORB: An efficient alternative to SIFT or SURF. In ICCV, 2011. 2\n\n17\n\n[71] Surat Teerapittayanon, Bradley McDanel,", "start_char_idx": 85914, "end_char_idx": 86567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "076a55c4-0d46-4d69-bab7-1b8e89a02590": {"__data__": {"id_": "076a55c4-0d46-4d69-bab7-1b8e89a02590", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "86f88c87-3188-492d-945d-5d1d161276e4", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "22eedbc49257f6fd966f56c0c8a4c70b3666d832aea2280fe0b53d69092f6fc1"}, "3": {"node_id": "8ce5d6e6-002d-4d5f-8f05-55b993161338", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7f87e676e537030dd93c8ef04403374cc81b66bb3966a14e8b66d27a905493ed"}}, "hash": "e831f6a99e62036d5207c372b5f5b5d343be8f05d70416f4b638076f13e07388", "text": "Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. BranchyNet: Fast inference via early exiting from deep neural networks. ICPR, 2016. 3, 4\n\n[72] Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, and Vassileios Balntas. SOSNet: Second Order Similarity Regularization for Local Descriptor Learning. In CVPR, 2019. 2\n\n[73] Micha\u0142 J Tyszkiewicz, Pascal Fua, and Eduard Trulls. DISK: In NeurIPS,\n\nLearning local features with policy gradient. 2020. 2, 6, 11, 13, 14, 15\n\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In", "start_char_idx": 86573, "end_char_idx": 87210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8ce5d6e6-002d-4d5f-8f05-55b993161338": {"__data__": {"id_": "8ce5d6e6-002d-4d5f-8f05-55b993161338", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "076a55c4-0d46-4d69-bab7-1b8e89a02590", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e831f6a99e62036d5207c372b5f5b5d343be8f05d70416f4b638076f13e07388"}, "3": {"node_id": "c7103c14-4077-4a6a-8434-3d1972431f37", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b24416d7a5dbc2e590ce2141ac7c409403f8e5ed2ba8876b03bb7b391870e20e"}}, "hash": "7f87e676e537030dd93c8ef04403374cc81b66bb3966a14e8b66d27a905493ed", "text": "Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. 1, 2, 3 [75] Andrea Vedaldi and Brian Fulkerson. VLFeat: An open and portable library of computer vision algorithms. In ACM inter- national conference on Multimedia, 2010. 14\n\n[76] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: In CVPR,\n\nExploiting spatial sparsity for faster inference. 2020. 3 [77] Phil Wang.\n\nBidirectional\n\ncross\n\nattention.\n\nhttps://github.com/lucidrains/ bidirectional-cross-attention. 4\n\n[78] Qing Wang, Jiaming Zhang, Kailun Yang, Kunyu Peng, and Rainer Stiefelhagen. MatchFormer: Interleaving Attention in Transformers for Feature Matching. In ACCV, 2022. 2, 6, 12 [79] Sinong Wang, Belinda Z.", "start_char_idx": 87197, "end_char_idx": 87900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7103c14-4077-4a6a-8434-3d1972431f37": {"__data__": {"id_": "c7103c14-4077-4a6a-8434-3d1972431f37", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "8ce5d6e6-002d-4d5f-8f05-55b993161338", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "7f87e676e537030dd93c8ef04403374cc81b66bb3966a14e8b66d27a905493ed"}, "3": {"node_id": "5db410a5-82e3-4d1f-ba5c-674f825be945", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "4be946aa293da6464502be76ba4f55cbfdd10007803d741d09ca7e271241c2ad"}}, "hash": "b24416d7a5dbc2e590ce2141ac7c409403f8e5ed2ba8876b03bb7b391870e20e", "text": "2022. 2, 6, 12 [79] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-Attention with Linear Complexity. arXiv:2006.04768, 2020. 2\n\n[80] Yan Wang, Zihang Lai, Gao Huang, Brian H. Wang, Laurens van der Maaten, Mark E. Campbell, and Kilian Q. Weinberger. Anytime Stereo Image Depth Estimation on Mobile Devices. ICRA, 2018. 3, 4\n\n[81] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned invariant feature transform. In ECCV, 2016. 2\n\n[82] Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao. Learning two-view", "start_char_idx": 87916, "end_char_idx": 88537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5db410a5-82e3-4d1f-ba5c-674f825be945": {"__data__": {"id_": "5db410a5-82e3-4d1f-ba5c-674f825be945", "embedding": null, "metadata": {"file": "2306.13643v1.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c6114a4-afbf-45a4-83f6-cd2d70a83c3a", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "e51bc9bd4b3b30df6d35ce01b2768b43a3990c26d0bb4372b0ed69d48f17f198"}, "2": {"node_id": "c7103c14-4077-4a6a-8434-3d1972431f37", "node_type": null, "metadata": {"file": "2306.13643v1.pdf"}, "hash": "b24416d7a5dbc2e590ce2141ac7c409403f8e5ed2ba8876b03bb7b391870e20e"}}, "hash": "4be946aa293da6464502be76ba4f55cbfdd10007803d741d09ca7e271241c2ad", "text": "Yurong Chen, Long Quan, and Hongen Liao. Learning two-view correspondences and geometry using order-aware network. In ICCV, 2019. 2, 7\n\n[83] Lulin Zhang, Ewelina Rupnik, and Marc Pierrot-Deseilligny. Feature matching for multi-epoch historical aerial images. ISPRS Journal of Photogrammetry and Remote Sensing, 182:176\u2013189, 2021. 1\n\n18", "start_char_idx": 88514, "end_char_idx": 88849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"1c6114a4-afbf-45a4-83f6-cd2d70a83c3a": {"node_ids": ["a573b002-0d92-428d-a48d-2cce0a286c36", "0803fb57-4e48-4e17-ab80-ce1ba08f94ac", "4a9a202f-f246-4466-ae3a-aba01475afc5", "2bca0dad-497b-4a01-9d6f-a90ee16f6a0e", "04c3ce37-44a5-4a97-8a43-c303ff8f28c2", "b15b964a-7605-4b6e-a088-f0b21b60908c", "c5583c66-ac98-4a56-a9e7-ab57435872a5", "9c4c5d61-ad46-4022-b9c7-51f0721020bf", "af0618cd-8378-476a-9f0b-0fac1971e397", "50c65e40-4f4e-4c8b-9ac0-72c47fd724c7", "421707ef-f3a5-4b5e-a7e5-238e600906f8", "b01d032b-fe92-4a01-be44-488dac655c47", "761861c5-296f-439a-a225-b986fdd74fc1", "46e6d9bb-340f-4ee3-8d21-edbaa5348e1e", "dc2e61d6-cb40-4f23-b2f6-dc8d41aef858", "c7c3abef-9148-4b08-badd-b31eb06ce1fe", "65dd82f8-de16-480f-ab50-9b75a01a6314", "e4bf0c50-cd16-4a42-80d1-508a739cd3b2", "6107db41-ff01-42eb-a3f6-f486a16a4f2c", "57e65d6e-31c8-4ff2-b498-157573256f9f", "ef4c8c43-2e1d-45a8-b380-b47b75395f42", "640a9b0b-a083-4364-90e2-2f5543b56315", "34b32204-2ef5-4f6b-9793-44bb1dd96711", "872c14ae-0416-476d-97fa-38a37827f66b", "37bed96a-f95a-47dd-a1d9-a2ed0e58b845", "de020396-b2cb-41d4-ba1f-4108e5a557b3", "caea057f-c4ca-4b03-acde-d84deb48413f", "e48db89d-394f-4da1-9e88-5c1eab40747c", "5b414108-ce4d-40f6-a572-2975e369348c", "27232c73-14cd-4802-ae69-4433455e8b32", "0c9c177e-0989-4c36-b6e1-ed23a7276819", "a02df946-1b52-436a-87ea-58340691d278", "4f102465-74bd-4e58-a28a-46526f522bea", "39cbc489-014e-406a-82a9-b31b11f722ea", "98242268-27d0-4a33-809e-b4dc476681ab", "c9ba681f-d8b7-4f8b-be47-d369fa644e67", "bb426f0a-bd9f-46cd-bfc2-4717564d1170", "b4c4cd88-7cea-4fd2-a854-f1187d6f3c6a", "cadb9d45-9843-4531-a2cd-2567a287210a", "232c6788-a8b7-437d-84ad-fa0273d1901c", "f848e9ac-4c21-4afa-8fd7-ca37e3c61c3e", "277bc300-2c2e-4b92-9eea-fa96e785ae98", "f60c039b-347d-41d8-8334-99c8389902a7", "a7a1ac86-b86e-4f9a-a2fc-2ecc2d885007", "d54f85af-d0fc-4636-a72f-ad4613bcf6e6", "be8c7dcb-1b14-4850-a82b-5f9e9de74c4d", "b15d6d5e-0064-4818-9503-39754929969d", "dafcacc4-d8f9-4bff-bfcb-f6f74de7dc7c", "946c1d16-56bb-4d8c-ba0b-1555df851642", "26ee11bc-2381-4340-9a52-20b53563588d", "b494e4b9-ee9b-4794-a807-98e144fbea89", "ff18a060-413a-4c56-a41a-faad54d5eecf", "28d4b0e1-a30c-44d2-b8c7-9ecd5f79254b", "77424ee9-defc-4b65-9027-d7d6536a3e51", "8bed7003-1d02-43f9-8e87-77c5d10231c6", "beb32c2b-70f0-4d38-8ce5-6b52d9f59e21", "1a7b5a50-c21f-4b70-8cfc-e3a5d7b28483", "5f0613c8-76e7-40f9-83a3-e676b05ba8be", "58bb2cae-413d-4131-988e-33201961e8dd", "cbd8b3af-0e1a-455c-b6d5-703c93ca6812", "25bf4a5e-0bc6-4592-824f-5321d0f7d5b1", "b9413688-685f-4087-a3e1-4d4d48ec9b74", "d38b97a2-714a-4cd0-a997-d2a5b94622fa", "30d68ce3-7d65-4e47-852a-fee9953253eb", "1623a297-9d4f-42a8-9a55-693bc937f6a5", "719fdc44-7038-4b3f-9522-2c8183d46807", "358581b8-6620-45e7-8042-8e52fe7a11a9", "a0e5b11e-e971-4d29-a7f0-f33cff4efa25", "637b6b93-f153-4944-a725-b29bbf9a662f", "c7416539-0cbd-42bc-9d7c-2fb129c412d9", "1d347c22-2461-4a69-8245-aa2be7bf901a", "61536b64-d213-4ede-90b6-128097fb380b", "7d656dc9-e1cd-4dba-abdb-9bbd1862b21c", "3d3e0ddf-acde-4444-aa24-511ce8050c22", "8fd5a7d8-ea80-4b04-99fb-f39a48c7c9b3", "c7f873bf-ddfb-4c71-bc1f-e7cf3026c452", "0c71ba38-076d-4d2c-8a40-b50c3f85b5c1", "cef44be1-ee48-4ff0-8872-08d74530e3c1", "5e521b21-0ead-494e-8690-68b4cc7096b9", "9666c334-2245-4f6a-8a07-6c02c944702f", "3ea99bc3-8f16-41ae-9ba1-18c87cbd849b", "cf094076-a52c-4980-83bb-f2d5a25b643f", "f925dced-f69c-449f-9c37-629d941e110b", "d0adb495-064a-4f39-86fa-1e904b74dd2e", "c1188a9c-8ab9-4c44-81cc-916ccd3342d8", "1d792d69-87d6-424a-a1e4-df9b63ec7fe1", "51ef6659-d4b2-4344-ba0f-5fd65487a9b4", "9abbd0e8-de67-45fa-a73c-f7e31dbe2ef9", "ff3bb99f-c4f1-4a41-b5d1-30dc85973069", "7e2bcfc5-a822-45f2-905a-26ddcf11f482", "9058e6b6-d1e1-46d5-9105-02ce96404880", "1693798f-391b-4324-a5bb-8869f6e40ec9", "8d618149-8d33-496e-9607-dba2df1cae92", "8b904a93-070e-4a67-b9e1-9c8890ec0ccc", "3cbac886-bcaa-41a2-8e3a-88ecdf72785a", "588afb2f-817f-4a6a-b61c-abb4a30e7712", "09eeb4db-79c8-40b0-b222-85a06d9b8f07", "4dc467e5-ed33-4e3a-bfb9-8376a49133b7", "68a17f27-2e13-4c99-93c0-16bb371d914e", "366863b6-673d-4f6d-9dec-abff2d07ed26", "62da66d8-fa5a-4e6c-811f-82f66cc9adb1", "d421e349-dd36-413e-b132-703b86887d7b", "00b870a3-a4d2-4d20-a770-ff54321633e6", "6537c040-bd70-464d-8c4f-1138f1000fa9", "7fa02926-68cd-40fe-9152-781d08ab1183", "405158b9-9b8d-4624-9de0-5e1c55bf07d3", "79736362-4040-4c34-8c9a-a73211271402", "87ec85ba-f93f-4b3e-a58b-19bded79bf60", "958656c5-73d4-4f7e-890c-74aeb1e141f8", "045b94f1-1612-4002-9276-c6236c6f87d7", "8db43dbe-6391-4294-a2ed-c6cfe42236b1", "92d5c780-a559-437e-aefe-2fb3702c45d1", "556dbab9-5a90-48c0-926d-20eeffb941f3", "42ee3e86-a475-4eaf-b526-55f7920b4013", "a7b678be-36d6-4517-be75-b47a6ea5615e", "fb4c9479-22d2-47eb-b6a3-6bd0d76d7702", "383d7d6b-7777-4601-8256-c0eacbc2c8ad", "86f88c87-3188-492d-945d-5d1d161276e4", "076a55c4-0d46-4d69-bab7-1b8e89a02590", "8ce5d6e6-002d-4d5f-8f05-55b993161338", "c7103c14-4077-4a6a-8434-3d1972431f37", "5db410a5-82e3-4d1f-ba5c-674f825be945"], "metadata": {"file": "2306.13643v1.pdf"}}}}